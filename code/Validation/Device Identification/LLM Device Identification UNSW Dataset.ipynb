{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import functools\n",
    "import glob\n",
    "import base64\n",
    "# import common\n",
    "import multiprocessing.pool\n",
    "import tldextract\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import threading\n",
    "import itertools\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "\n",
    "# load meta data \n",
    "file_dhcp = '../../../Validation Data/UNSW/scrape-results/dhcp_responses.csv'\n",
    "file_endpoints = '../../../Validation Data/UNSW/scrape-results/endpoints.csv'\n",
    "file_mdns= '../../../Validation Data/UNSW/scrape-results/mdns_responses.csv'\n",
    "file_oui = '../../../Validation Data/UNSW/scrape-results/oui_friendly.csv'\n",
    "file_ssdp = '../../../Validation Data/UNSW/scrape-results/ssdp_response.csv'\n",
    "file_upnp = '../../../Validation Data/UNSW/scrape-results/upnp_responses.csv'\n",
    "file_user_agent = '../../../Validation Data/UNSW/scrape-results/user_agent.csv'\n",
    "\n",
    "\n",
    "file_dhcp = pd.read_csv(file_dhcp).drop(columns='Unnamed: 0')\n",
    "file_endpoints = pd.read_csv(file_endpoints).drop(columns='Unnamed: 0').dropna().groupby('mac')['domain'].agg(lambda x: \" + \".join(set(', '.join(x).split(', ')))).reset_index()\n",
    "file_mdns = pd.read_csv(file_mdns).drop(columns='Unnamed: 0')\n",
    "file_oui = pd.read_csv(file_oui).drop(columns='Unnamed: 0')\n",
    "file_ssdp = pd.read_csv(file_ssdp).drop(columns='Unnamed: 0')\n",
    "file_upnp = pd.read_csv(file_upnp).drop(columns='Unnamed: 0')\n",
    "file_user_agent = pd.read_csv(file_user_agent).drop(columns='Unnamed: 0')\n",
    "# Merge the dataframes one by one\n",
    "merged_df = file_oui.merge(file_dhcp, on='mac', how='outer')\n",
    "merged_df = merged_df.merge(file_mdns, on='mac', how='outer')\n",
    "merged_df = merged_df.merge(file_ssdp, on='mac', how='outer')\n",
    "merged_df = merged_df.merge(file_upnp, on='mac', how='outer')\n",
    "merged_df = merged_df.merge(file_user_agent, on='mac', how='outer')\n",
    "merged_df = merged_df.merge(file_endpoints, on='mac', how='outer')\n",
    "\n",
    "print(merged_df.sample(5))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af00a5f694d58934"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "OPENAI_API_KEY = 'sk-PLACE-YOUR-KEY-HERE'\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "models = openai.Model.list()\n",
    "\n",
    "\n",
    "def chat_completion(messages, max_tokens=20):\n",
    "     return openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt= messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "\n",
    "def ask_gpt_raw(messages, max_tokens=20):\n",
    "    response = chat_completion(messages, max_tokens)\n",
    "    return response.choices[0].text.translate(str.maketrans('', '', '\\n\\r\\t'))\n",
    "\n",
    "# get device identification from dhcp metadata \n",
    "get_dhcp_vendor = lambda s: ask_gpt_raw(f'I have an IoT device named \"{s}\". What is the company that makes this IoT device? Output the company\\'s name only.', max_tokens=20)\n",
    "get_dhcp_type = lambda s: ask_gpt_raw(f'I have an IoT device named \"{s}\". What type of IoT device is this? Output the name of the device type only.', max_tokens=20)\n",
    "\n",
    "with multiprocessing.pool.ThreadPool(processes=10) as pool:\n",
    "    merged_df['gpt_dhcp_vendor'] = pool.map(get_dhcp_vendor, merged_df['dhcp_response'].fillna(''))\n",
    "    merged_df['gpt_dhcp_type'] = pool.map(get_dhcp_type, merged_df['dhcp_response'].fillna(''))\n",
    "\n",
    "\n",
    "# get device identification from netdisco metadata \n",
    "get_netdisco_vendor = lambda s: ask_gpt_raw(f'I have an IoT device named \"{s}\". What is the company that makes this IoT device? Output the company\\'s name only.', max_tokens=20)\n",
    "get_netdisco_type = lambda s: ask_gpt_raw(f'I have an IoT device named \"{s}\". What type of IoT device is this? Output the name of the device type only.', max_tokens=20)\n",
    "\n",
    "with multiprocessing.pool.ThreadPool(processes=5) as pool:\n",
    "    merged_df['gpt_netdisco_vendor'] = pool.map(get_netdisco_vendor,\n",
    "                                                merged_df['mdns_response'].fillna('') + ', ' +\n",
    "                                                merged_df['ssdp_response'].fillna('') + ', ' +\n",
    "                                                merged_df['upnp_response'].fillna(''))\n",
    "    merged_df['gpt_netdisco_type'] = pool.map(get_netdisco_type,\n",
    "                                              merged_df['mdns_response'].fillna('') + ', ' +\n",
    "                                              merged_df['ssdp_response'].fillna('') + ', ' +\n",
    "                                              merged_df['upnp_response'].fillna(''))\n",
    "\n",
    "# get device identification from OUI and useragent  metadata \n",
    "get_oui_agent_vendor = lambda s: ask_gpt_raw(f'I have an IoT device with OUI name: \"{s.split(\"::\")[0]}\", '\n",
    "                                             f'and user agent: \"{s.split(\"::\")[1:]}\"' \n",
    "                                             f'. What is the company that makes this IoT device? Output the company\\'s name only.',\n",
    "                                             max_tokens=20)\n",
    "get_oui_agent_type = lambda s: ask_gpt_raw(f'I have an IoT device with OUI name: \"{s.split(\"::\")[0]}\", '\n",
    "                                           f'and user agent: \"{s.split(\"::\")[1:]}\".'\n",
    "                                           f' What type of IoT device is this? Output the name of the device type only.',\n",
    "                                           max_tokens=20)\n",
    "\n",
    "with multiprocessing.pool.ThreadPool(processes=5) as pool:\n",
    "    merged_df['gpt_oui_agent_vendor'] = pool.map(get_oui_agent_vendor, \n",
    "                                                 merged_df['oui_friendly'].fillna('') + '::' +\n",
    "                                                 merged_df['user_agent'].fillna(''))\n",
    "    merged_df['gpt_oui_agent_type'] = pool.map(get_oui_agent_type,\n",
    "                                               merged_df['oui_friendly'].fillna('') + '::' +\n",
    "                                               merged_df['user_agent'].fillna(''))\n",
    "\n",
    "# get device identification from only useragent  metadata \n",
    "get_user_agent_vendor = lambda s: ask_gpt_raw(f'I have an IoT device with  user agent: \"{s}\"'\n",
    "                                              f'. What is the company that makes this IoT device? Output the company\\'s name only.',\n",
    "                                              max_tokens=10)\n",
    "get_user_agent_type = lambda s: ask_gpt_raw(f'I have an IoT device with user agent: \"{s}\".'\n",
    "                                            f' What type of IoT device is this? Output the name of the device type only.',\n",
    "                                            max_tokens=10)\n",
    "\n",
    "\n",
    "with multiprocessing.pool.ThreadPool(processes=5) as pool:\n",
    "    merged_df['gpt_user_agent_vendor'] = pool.map(get_user_agent_vendor, merged_df['user_agent'].fillna(''))\n",
    "    merged_df['gpt_user_agent_type'] = pool.map(get_user_agent_type, merged_df['user_agent'].fillna(''))\n",
    "    \n",
    "    \n",
    "# merge all information into a single dataframe \n",
    "merged_df = merged_df[['mac', 'oui', 'oui_friendly', 'dhcp_response', 'mdns_response',\n",
    "           'ssdp_response', 'upnp_response', 'user_agent', 'domain',\n",
    "           'gpt_dhcp_vendor', 'gpt_netdisco_vendor', 'gpt_netdisco_type',\n",
    "           'gpt_dhcp_type', 'gpt_oui_agent_vendor', 'gpt_oui_agent_type', \n",
    "           'gpt_user_agent_vendor', 'gpt_user_agent_type']]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fcc3c53aaed94df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPT Clean responses "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fad2b0c2e8c96cb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpt_two_feature_df = merged_df.copy(deep=True)\n",
    "\n",
    "# Filter out crappy answers\n",
    "def remove_bad_answer(s):\n",
    "    if '___' in s:\n",
    "        return ''\n",
    "\n",
    "    if 'N/A' in s:\n",
    "        return ''\n",
    "\n",
    "    if s.endswith('.'):\n",
    "        return s[0:-1]\n",
    "\n",
    "    if 'unknown' in s.lower() or 'espressif' in s.lower() or 'ESP' in s:\n",
    "        return ''\n",
    "\n",
    "    if s == 'DLNA_DMR':\n",
    "        return 'Streaming Device'\n",
    "\n",
    "    return s\n",
    "\n",
    "for col in gpt_two_feature_df.columns:\n",
    "    if col.startswith('gpt_'):\n",
    "        gpt_two_feature_df[col] = gpt_two_feature_df[col].fillna('')\n",
    "        gpt_two_feature_df[col] = gpt_two_feature_df[col].apply(remove_bad_answer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc57dfc5c58dcdd8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stop_word_set = {\n",
    "    'electronics', 'smart', 'inc', 'technology', 'espressif', 'assistant', 'technologies', 'corporation', 'ltd'\n",
    "}\n",
    "\n",
    "equivalent_companies = [\n",
    "    {'lg', 'lg electronics'},\n",
    "    {'nest', 'google'},\n",
    "    {'myq', 'chamberlain'},\n",
    "    {'xiaomi', 'yeelight'},\n",
    "    {'dropcam', 'nest'}\n",
    "]\n",
    "\n",
    "def is_same_vendor(v1, v2):\n",
    "\n",
    "    v1 = v1.lower().replace('.', '').replace('-', '')\n",
    "    v2 = v2.lower().replace('.', '').replace('-', '')\n",
    "\n",
    "    if v1 == '' or v2 == '':\n",
    "        return ''\n",
    "\n",
    "    # Same vendor if one is a substring of another\n",
    "    if len(v1) >= 3 and v1 in v2 and v1 not in stop_word_set:\n",
    "        return 'substring:' + v1\n",
    "    if len(v2) >= 3 and v2 in v1 and v2 not in stop_word_set:\n",
    "        return 'substring:' + v2\n",
    "\n",
    "    # Same vendor if sharing at least one token that is not a stop word\n",
    "    v1_tokens = set(v1.split())\n",
    "    v2_tokens = set(v2.split())\n",
    "    common_tokens = (v1_tokens & v2_tokens) - stop_word_set\n",
    "    if common_tokens:\n",
    "        return 'common_tokens:' + '+'.join(common_tokens)\n",
    "\n",
    "    # Some of these tokens are substrings of each other\n",
    "    for (t1, t2) in itertools.product(v1_tokens, v2_tokens):\n",
    "        if len(t1) >= 3 and t1 in t2 and t1 not in stop_word_set:\n",
    "            return 'common_token_substring:' + t1\n",
    "        if len(t2) >= 3 and t2 in t1 and t2 not in stop_word_set:\n",
    "            return 'common_token_substring:' + t2\n",
    "        \n",
    "    # TODO: [Added by Jakaria] Didn't consider the equivalent companies\n",
    "    \n",
    "    if {v1, v2} in equivalent_companies:\n",
    "        return 'equivalent:'+v1\n",
    "\n",
    "    return 'different'\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94f18df757f7d739"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpt_two_feature_consistency_df = gpt_two_feature_df.copy().fillna('')\n",
    "\n",
    "columns_to_check = ['gpt_user_agent_vendor', 'gpt_dhcp_vendor', 'gpt_netdisco_vendor', 'oui_friendly', 'domains_friendly']\n",
    "\n",
    "gpt_two_feature_consistency_df['domains_friendly'] = gpt_two_feature_consistency_df['domain'].fillna('').apply(\n",
    "    lambda s: ' '.join(set([tldextract.extract(reg_domain).domain for reg_domain in s.split('+')]))\n",
    ")\n",
    "\n",
    "# Check across the columns for consistency\n",
    "for (col1, col2) in itertools.combinations(columns_to_check, 2):\n",
    "    new_col = f'consistency:{col1}:{col2}'\n",
    "    gpt_two_feature_consistency_df[new_col] = gpt_two_feature_consistency_df.apply(\n",
    "        lambda r: is_same_vendor(r[col1], r[col2]), axis=1\n",
    "    )\n",
    "\n",
    "# Consolidate\n",
    "def consolidate_consistency(r):\n",
    "    r = r.to_dict()\n",
    "\n",
    "    common_term_set = set()\n",
    "    for (k, v) in r.items():\n",
    "        if k.startswith('consistency:') and ':' in v:\n",
    "            common_term_set.add(v.split(':', 1)[1])\n",
    "\n",
    "    # Some of the common terms are substrings of each other; merge these terms\n",
    "    terms_to_remove = set()\n",
    "    for (t1, t2) in itertools.permutations(common_term_set, 2):\n",
    "        if len(t1) >=3 and t1 in t2:\n",
    "            terms_to_remove.add(t2)\n",
    "\n",
    "    common_term_set -= terms_to_remove\n",
    "\n",
    "    # Merge equiv companies\n",
    "    if common_term_set in equivalent_companies:\n",
    "        common_term_set.pop()\n",
    "\n",
    "    # If there are multiple terms and one of them is X (e.g. Spotify), remove X.\n",
    "    if len(common_term_set) > 1:\n",
    "        common_term_set -= {'spotify', 'google'}\n",
    "\n",
    "    return '+'.join(common_term_set)\n",
    "\n",
    "gpt_two_feature_consistency_df['consolidated_vendor'] = gpt_two_feature_consistency_df.apply(lambda r: consolidate_consistency(r), axis=1)\n",
    "\n",
    "# Remove stop words and irrelevant results\n",
    "def clean_consolidated_vendor(s):\n",
    "\n",
    "    if 'hewlett' in s or 'packard' in s:\n",
    "        return 'hp'\n",
    "\n",
    "    if 'raspberry' in s:\n",
    "        return 'raspberry-pi'\n",
    "\n",
    "    if 'wemo' in s:\n",
    "        return 'belkin'\n",
    "\n",
    "    if s in ('ind', 'one', 'hon', 'shenzhen', 'electric', 'media', 'hom', 'ltd', 'digital', 'things', 'the', 'night', 'security'):\n",
    "        return ''\n",
    "\n",
    "    if s == 'free':\n",
    "        return 'freebox'\n",
    "\n",
    "    s = s.replace('electronics', '') \\\n",
    "        .replace('international inc', '') \\\n",
    "        .replace('inc', '') \\\n",
    "        .replace('corporation', '') \\\n",
    "        .replace('networks', '') \\\n",
    "        .replace('labs', '') \\\n",
    "        .replace('group', '') \\\n",
    "        .replace('llc', '') \\\n",
    "        .replace('foundation', '') \\\n",
    "        .replace('technology', '') \\\n",
    "        .replace('technologies', '') \\\n",
    "        .replace('ltd', '')\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "gpt_two_feature_consistency_df['consolidated_vendor'] = gpt_two_feature_consistency_df['consolidated_vendor'].apply(clean_consolidated_vendor)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c4083971ee14ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_clean_type(r):\n",
    "    if r['gpt_user_agent_type']:\n",
    "        if (r['gpt_dhcp_type'] == r['gpt_user_agent_type']) or (r['gpt_netdisco_type'] == r['gpt_user_agent_type']):\n",
    "            return r['gpt_user_agent_type']\n",
    "\n",
    "    if r['gpt_dhcp_type']:\n",
    "        if (r['gpt_dhcp_type'] == r['gpt_netdisco_type']) or (r['gpt_dhcp_type'] == r['gpt_user_agent_vendor']):\n",
    "            return r['gpt_dhcp_type']\n",
    "\n",
    "    if r['gpt_netdisco_type']:\n",
    "        if (r['gpt_dhcp_type'] == r['gpt_netdisco_type']) or (r['gpt_netdisco_type'] == r['gpt_user_agent_vendor']):\n",
    "            return r['gpt_netdisco_type']\n",
    "\n",
    "    if r['gpt_user_agent_type'] and r['gpt_user_agent_vendor'] != '':\n",
    "        return r['gpt_user_agent_type']\n",
    "    if r['gpt_dhcp_type'] and r['gpt_dhcp_vendor'] != '':\n",
    "        return r['gpt_dhcp_type']\n",
    "    if r['gpt_netdisco_type'] and r['gpt_netdisco_vendor'] != '':\n",
    "        return r['gpt_netdisco_type']\n",
    "    return ''\n",
    "\n",
    "\n",
    "gpt_clean_df = gpt_two_feature_consistency_df.copy()\n",
    "gpt_clean_df['gpt_clean_vendor'] = gpt_clean_df['consolidated_vendor']\n",
    "gpt_clean_df['gpt_clean_type'] = gpt_clean_df.apply(get_clean_type, axis=1)\n",
    "\n",
    "\n",
    "file_ground_truth = '../../../Validation Data/UNSW/List_of_Devices.csv'\n",
    "ground_truth = pd.read_csv(file_ground_truth)\n",
    "\n",
    "gpt_clean_df = gpt_clean_df.merge(ground_truth, on='mac', how='outer')\n",
    "\n",
    "# todo save result in file \n",
    "# file_to_save = '../../../Validation Data/UNSW/gpt-clean-updated.csv'\n",
    "# gpt_clean_df.to_csv(file_to_save, index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98e1d0e522c94835"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
