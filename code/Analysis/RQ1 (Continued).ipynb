{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todo NOTE: this script does not reproduce original result since we removed device IDs \n",
    "# todo NOTE: also this script will encounter runtime/space error since we used device IDs as a key while merge dataframes \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_cleaned_flow = '../../Endpoint Mapping Data/Cleaned Flow/cleaned_flow_stat.csv'\n",
    "cleaned_flow = pd.read_csv(file_cleaned_flow)\n",
    "\n",
    "cleaned_flow['super_vendor'] = cleaned_flow.apply(\n",
    "    lambda row: row.vendor_name.lower() if row.vendor_name==row.vendor_name\n",
    "    else row.gpt_clean_vendor,\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "cleaned_flow['generic_category'] = cleaned_flow.apply(\n",
    "    lambda row: row.man_generic_category if row.man_generic_category==row.man_generic_category\n",
    "    else row.gpt_generic_category,\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "# read all party mapping files\n",
    "file_all_party_mapping = '../../Endpoint Mapping Data/Domain Data/all_party_mapping.csv'\n",
    "all_party_mapping = pd.read_csv(file_all_party_mapping)\n",
    "# drop extra columns \n",
    "all_party_mapping = all_party_mapping[['super_vendor', 'domain', 'party_labels']].drop_duplicates()\n",
    "\n",
    "# marge with clean flow\n",
    "clean_flow_party_label = pd.merge(cleaned_flow,\n",
    "                                  all_party_mapping,\n",
    "                                  on=['super_vendor', 'domain'],\n",
    "                                  how='left'\n",
    "                                  )\n",
    "\n",
    "\n",
    "# read user information file \n",
    "file_user_device_timezone = '../../Inspector Dataset/New data/user_device_timezone.csv'\n",
    "user_device_timezone = pd.read_csv(file_user_device_timezone)\n",
    "user_device_timezone = user_device_timezone[['device_id', 'user_key', 'user_country', 'timezone']]\n",
    "\n",
    "# merge with timezone file \n",
    "clean_flow_party_label = pd.merge(clean_flow_party_label,\n",
    "                                  user_device_timezone,\n",
    "                                  on=['device_id'],\n",
    "                                  how='left'\n",
    "                                  )\n",
    "\n",
    "print(len(clean_flow_party_label['device_id'].unique()))\n",
    "unique_categories = clean_flow_party_label['generic_category'].unique()\n",
    "print(list(unique_categories))\n",
    "\n",
    "\n",
    "unique_categories = ['Media/TV',\n",
    "                     'Voice Assistant',\n",
    "                     'Surveillance',\n",
    "                     'Home Automation',\n",
    "                     'Home Appliance ',\n",
    "                     'Game Console',\n",
    "                     'Generic IoT',\n",
    "                     'Work Appliance',\n",
    "                     'Vehicle']\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "265e8cc6"
  },
  {
   "cell_type": "markdown",
   "id": "37db4123",
   "metadata": {},
   "source": [
    "### RQ 4\n",
    "\n",
    "#### Who are the dominant support parties within the IoT ecosystem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# finding top support parties\n",
    "\n",
    "from cleanco import basename\n",
    "\n",
    "devices_df = clean_flow_party_label\n",
    "support_flows = devices_df[devices_df['party_labels']==2]\n",
    "support_parties = support_flows['domain'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# load combined information\n",
    "file_combined_domain2org = '../../Endpoint Mapping Data/Domain Data/First Party Mapping/domain2org_all_possible_sources.csv'\n",
    "domain2org = pd.read_csv(file_combined_domain2org).rename(columns={'remote_hostname':'domain'})\n",
    "# merge \n",
    "support_parties = pd.merge(support_parties,\n",
    "                           domain2org,\n",
    "                           on=['domain'],\n",
    "                           how='left'\n",
    "                           )\n",
    "\n",
    "# find support organization\n",
    "support_parties['organization'] = support_parties.apply(lambda row: row.netify_org if row.netify_org==row.netify_org\n",
    "else basename(basename(row.python_whois_org).capitalize() if (row.python_whois_org==row.python_whois_org) & (len(str(row.python_whois_org).strip())>0)\n",
    "              else basename(basename(row.bash_whois_org).capitalize() if (row.bash_whois_org==row.bash_whois_org) & (len(str(row.bash_whois_org).strip())>0)\n",
    "                            else (row.xclusive_org.capitalize() if row.xclusive_org==row.xclusive_org\n",
    "                                  else row.domain))), axis=1)\n",
    "\n",
    "support_parties = support_parties[['domain', 'organization']]\n",
    "\n",
    "# add support organization in the flow table\n",
    "support_flows = pd.merge(support_flows, support_parties, on='domain', how='left')\n",
    "\n",
    "# get support organization and count of supported devices \n",
    "support_organization = support_flows.groupby(['organization'])[['device_id']].nunique().sort_values(by='device_id', ascending=False).reset_index().rename(columns={'device_id':'device_count'}).set_index('organization')\n",
    "\n",
    "\n",
    "\n",
    "## lIST TOP SUPPORT PARTIES\n",
    "\n",
    "for org in support_organization.index:\n",
    "    device_count = support_organization['device_count'].loc[org]\n",
    "\n",
    "    if device_count < 20:\n",
    "        continue\n",
    "    print(org.ljust(15), end=' & \\t')\n",
    "    # print(org ,'&', device_count, end=' & ')\n",
    "\n",
    "    vendor_device_count = support_flows[(support_flows['organization']==org)].groupby(['super_vendor'])[['device_id']].nunique().sort_values(by='device_id', ascending=False)\n",
    "    for vendor in vendor_device_count.index:\n",
    "        if vendor_device_count['device_id'].loc[vendor] < 2:\n",
    "            continue\n",
    "        print('{0}({1}), '.format(vendor, vendor_device_count['device_id'].loc[vendor]), end='')\n",
    "\n",
    "    print('\\\\\\\\[3pt]')\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "835dfb39f0b88c3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## lIST OF VENDORS WITH MULTIPLE SUPPORT PARTIES\n",
    "vendor_name_device_count = support_flows.groupby(['super_vendor'])[['device_id']].nunique().sort_values(by='device_id', ascending=False)\n",
    "\n",
    "for vendor in vendor_name_device_count.index:\n",
    "    device_count = vendor_name_device_count['device_id'].loc[vendor]\n",
    "\n",
    "    ## skip if device count is less than 3\n",
    "    if device_count < 20:\n",
    "        continue\n",
    "\n",
    "    print(vendor.ljust(15), end=' & \\t')\n",
    "\n",
    "    support_device_count = support_flows[(support_flows['super_vendor']==vendor)].groupby(['organization'])[['device_id']].nunique().sort_values(by='device_id', ascending=False)\n",
    "    \n",
    "    for support in support_device_count.index:\n",
    "        if support_device_count['device_id'].loc[support] < 5:\n",
    "            continue\n",
    "        print('{0}({1}), '.format(support, support_device_count['device_id'].loc[support]), end='')\n",
    "\n",
    "    print('\\\\\\\\[2pt]')\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a1956be64a83657"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
