{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todo NOTE: this script does not reproduce original result since we removed device IDs \n",
    "# todo NOTE: also this script will encounter runtime/space error since we used device IDs as a key while merge dataframes \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_cleaned_flow = '../../../Endpoint Mapping Data/Cleaned Flow/cleaned_flow_stat.csv'\n",
    "cleaned_flow = pd.read_csv(file_cleaned_flow)\n",
    "\n",
    "cleaned_flow['super_vendor'] = cleaned_flow.apply(\n",
    "    lambda row: row.vendor_name.lower() if row.vendor_name==row.vendor_name\n",
    "    else row.gpt_clean_vendor,\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "cleaned_flow['generic_category'] = cleaned_flow.apply(\n",
    "    lambda row: row.man_generic_category if row.man_generic_category==row.man_generic_category\n",
    "    else row.gpt_generic_category,\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "# read all party mapping files\n",
    "file_all_party_mapping = '../../../Endpoint Mapping Data/Domain Data/all_party_mapping.csv'\n",
    "all_party_mapping = pd.read_csv(file_all_party_mapping)\n",
    "# drop extra columns \n",
    "all_party_mapping = all_party_mapping[['super_vendor', 'domain', 'party_labels']].drop_duplicates()\n",
    "\n",
    "# marge with clean flow\n",
    "clean_flow_party_label = pd.merge(cleaned_flow,\n",
    "                                  all_party_mapping,\n",
    "                                  on=['super_vendor', 'domain'],\n",
    "                                  how='left'\n",
    "                                  )\n",
    "\n",
    "\n",
    "# read user information file \n",
    "file_user_device_timezone = '../../../Inspector Dataset/New data/user_device_timezone.csv'\n",
    "user_device_timezone = pd.read_csv(file_user_device_timezone)\n",
    "user_device_timezone = user_device_timezone[['device_id', 'user_key', 'user_country', 'timezone']]\n",
    "\n",
    "# merge with timezone file \n",
    "clean_flow_party_label = pd.merge(clean_flow_party_label,\n",
    "                                  user_device_timezone,\n",
    "                                  on=['device_id'],\n",
    "                                  how='left'\n",
    "                                  )\n",
    "\n",
    "unique_categories = ['Media/TV',\n",
    "                     'Home Automation',\n",
    "                     'Voice Assistant',\n",
    "                     'Surveillance',\n",
    "                     'Game Console',\n",
    "                     'Work Appliance',\n",
    "                     'Home Appliance ',\n",
    "                     'Generic IoT',]\n",
    "print(len(clean_flow_party_label['device_id'].unique()))\n",
    "\n",
    "clean_flow_party_label['average_out_byte_per_sec'] = clean_flow_party_label['total_out_byte']/clean_flow_party_label['flow_duration']\n",
    "threshold = 1e6\n",
    "# Replace values greater than the threshold with NaN\n",
    "clean_flow_party_label['average_out_byte_per_sec'] = np.where(clean_flow_party_label['average_out_byte_per_sec'] > threshold,\n",
    "                                                              np.nan, clean_flow_party_label['average_out_byte_per_sec'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "265e8cc6"
  },
  {
   "cell_type": "markdown",
   "id": "612bbcf6",
   "metadata": {},
   "source": [
    "## RQ 4 (Longitudinal Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "file_location = '../../../Statistical Data/RQ3/country-list.json'\n",
    "\n",
    "with open(file_location, encoding='utf-8') as f:\n",
    "    country_2_continent = json.load(f)\n",
    "\n",
    "# Create a list of tuples containing key-value pairs\n",
    "country_2_continent = [(key, value) for key, value in country_2_continent.items()]\n",
    "country_2_continent = pd.DataFrame(country_2_continent, columns=['remote_ip_country', 'domain_loc'])\n",
    "\n",
    "# find continent to remote ip country \n",
    "clean_flow_party_label  = pd.merge(clean_flow_party_label, country_2_continent, on=['remote_ip_country'], how='left')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5787ba172f641432"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function to find the timezone of the users\n",
    "def timezone_continent(_timezone):\n",
    "    try:\n",
    "        _timezone = float(_timezone)\n",
    "        if (_timezone >= -11) and (_timezone < -1):\n",
    "            return 'AMERICAS'\n",
    "        if (_timezone >= 0) and (_timezone <= 3):\n",
    "            return 'EA'\n",
    "        if _timezone >= 4:\n",
    "            return 'APA'\n",
    "    except:\n",
    "        pass\n",
    "    return 'UN'\n",
    "\n",
    "\n",
    "timezone = clean_flow_party_label[['device_id', 'timezone']].drop_duplicates()\n",
    "timezone['user_loc'] =  timezone.apply(lambda row: timezone_continent(row.timezone), axis=1)\n",
    "\n",
    "# find user location based on timezone\n",
    "clean_flow_party_label = pd.merge(clean_flow_party_label, timezone, on=['device_id', 'timezone'], how='left')\n",
    "print(len(clean_flow_party_label['device_id'].unique()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7476dac1da88db15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "ts = clean_flow_party_label['min_ts'].min()\n",
    "dt = datetime.fromtimestamp(ts)\n",
    "print(\"The date and time is:\", dt)\n",
    "\n",
    "ts = clean_flow_party_label['min_ts'].max()\n",
    "dt = datetime.fromtimestamp(ts)\n",
    "print(\"The date and time is:\", dt)\n",
    "\n",
    "threshold = datetime.strptime(\"2020-12-31 23:59:59\", '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "print(threshold)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7b3b5c320b6f034"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "months_timestamp = [\"2019-03-31 23:59:59\", \"2019-04-30 23:59:59\", \"2019-05-31 23:59:59\", \"2019-06-30 23:59:59\", \"2019-07-31 23:59:59\", \"2019-08-31 23:59:59\",\n",
    "                    \"2019-09-30 23:59:59\", \"2019-10-31 23:59:59\", \"2019-11-30 23:59:59\", \"2019-12-31 23:59:59\", \"2020-01-31 23:59:59\", \"2020-02-29 23:59:59\",\n",
    "                    \"2020-03-31 23:59:59\", \"2020-04-30 23:59:59\", \"2020-05-31 23:59:59\", \"2020-06-30 23:59:59\", \"2020-07-31 23:59:59\", \"2020-08-31 23:59:59\",\n",
    "                    \"2020-09-30 23:59:59\", \"2020-10-31 23:59:59\", \"2020-11-30 23:59:59\", \"2020-12-31 23:59:59\", \"2021-01-31 23:59:59\", \"2021-02-28 23:59:59\",\n",
    "                    \"2021-03-31 23:59:59\", \"2021-04-30 23:59:59\", \"2021-05-31 23:59:59\", \"2021-06-30 23:59:59\", \"2021-07-31 23:59:59\", \"2021-08-31 23:59:59\",\n",
    "                    \"2021-09-30 23:59:59\", \"2021-10-31 23:59:59\", \"2021-11-30 23:59:59\", \"2021-12-31 23:59:59\", \"2022-01-31 23:59:59\", \"2022-02-28 23:59:59\",\n",
    "                    \"2022-03-31 23:59:59\", \"2022-04-30 23:59:59\", \"2022-05-31 23:59:59\", \"2022-06-30 23:59:59\", \"2022-07-31 23:59:59\"]\n",
    "\n",
    "\n",
    "month_map = {\"2019-03-31 23:59:59\" : \"Mar'19\",\n",
    "             \"2019-04-30 23:59:59\" : \"Apr'19\",\n",
    "             \"2019-05-31 23:59:59\" : \"May'19\",\n",
    "             \"2019-06-30 23:59:59\" : \"Jun'19\",\n",
    "             \"2019-07-31 23:59:59\" : \"Jul'19\",\n",
    "             \"2019-08-31 23:59:59\" : \"Aug'19\",\n",
    "             \"2019-09-30 23:59:59\" : \"Sep'19\",\n",
    "             \"2019-10-31 23:59:59\" : \"Oct'19\",\n",
    "             \"2019-11-30 23:59:59\" : \"Nov'19\",\n",
    "             \"2019-12-31 23:59:59\" : \"Dec'19\",\n",
    "             \"2020-01-31 23:59:59\" : \"Jan'20\",\n",
    "             \"2020-02-29 23:59:59\" : \"Feb'20\",\n",
    "             \"2020-03-31 23:59:59\" : \"Mar'20\",\n",
    "             \"2020-04-30 23:59:59\" : \"Apr'20\",\n",
    "             \"2020-05-31 23:59:59\" : \"May'20\",\n",
    "             \"2020-06-30 23:59:59\" : \"Jun'20\",\n",
    "             \"2020-07-31 23:59:59\" : \"Jul'20\",\n",
    "             \"2020-08-31 23:59:59\" : \"Aug'20\",\n",
    "             \"2020-09-30 23:59:59\" : \"Sep'20\",\n",
    "             \"2020-10-31 23:59:59\" : \"Oct'20\",\n",
    "             \"2020-11-30 23:59:59\" : \"Nov'20\",\n",
    "             \"2020-12-31 23:59:59\" : \"Dec'20\",\n",
    "             \"2021-01-31 23:59:59\" : \"Jan'21\",\n",
    "             \"2021-02-28 23:59:59\" : \"Feb'21\",\n",
    "             \"2021-03-31 23:59:59\" : \"Mar'21\",\n",
    "             \"2021-04-30 23:59:59\" : \"Apr'21\",\n",
    "             \"2021-05-31 23:59:59\" : \"May'21\",\n",
    "             \"2021-06-30 23:59:59\" : \"Jun'21\",\n",
    "             \"2021-07-31 23:59:59\" : \"Jul'21\",\n",
    "             \"2021-08-31 23:59:59\" : \"Aug'21\",\n",
    "             \"2021-09-30 23:59:59\" : \"Sep'21\",\n",
    "             \"2021-10-31 23:59:59\" : \"Oct'21\",\n",
    "             \"2021-11-30 23:59:59\" : \"Nov'21\",\n",
    "             \"2021-12-31 23:59:59\" : \"Dec'21\",\n",
    "             \"2022-01-31 23:59:59\" : \"Jan'22\",\n",
    "             \"2022-02-28 23:59:59\" : \"Feb'22\",\n",
    "             \"2022-03-31 23:59:59\" : \"Mar'22\",\n",
    "             \"2022-04-30 23:59:59\" : \"Apr'22\",\n",
    "             \"2022-05-31 23:59:59\" : \"May'22\",\n",
    "             \"2022-06-30 23:59:59\" : \"Jun'22\",\n",
    "             \"2022-07-31 23:59:59\" : \"Jul'22\", }\n",
    "\n",
    "print(len(months_timestamp))\n",
    "\n",
    "# for date in months_timestamp:\n",
    "#     print(datetime.strptime(date, '%Y-%m-%d %H:%M:%S').timestamp())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52a56484ac27d3ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Temporal analysis  of RQ1 fine-grained\n",
    "sub_categories = ['Media/TV',\n",
    "                  'Home Automation',\n",
    "                  'Voice Assistant',\n",
    "                  'Surveillance',]\n",
    "                  # 'Game Console',]\n",
    "\n",
    "\n",
    "devices_df = clean_flow_party_label.copy(deep=True)\n",
    "\n",
    "aggregated_data = []\n",
    "\n",
    "\n",
    "for category in sub_categories:\n",
    "    for i in range(2,len(months_timestamp),3):\n",
    "        # print(months_timestamp[i-1], months_timestamp[i])\n",
    "        min_ts = datetime.strptime(months_timestamp[i-2], '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "        max_ts = datetime.strptime(months_timestamp[i+1], '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "    \n",
    "        df = devices_df[(devices_df['generic_category']==category) &\n",
    "                        (devices_df['min_ts']>min_ts) & (devices_df['max_ts']<=max_ts)]\n",
    "\n",
    "\n",
    "        first = df[df['party_labels']==1].groupby('device_id')['domain'].nunique().reset_index().rename(columns={\"domain\": \"1\"})\n",
    "        support = df[df['party_labels']==2].groupby('device_id')['domain'].nunique().reset_index().rename(columns={\"domain\": \"2\"})\n",
    "        Third = df[df['party_labels']==3].groupby('device_id')['domain'].nunique().reset_index().rename(columns={\"domain\": \"3\"})\n",
    "\n",
    "        devices = df[['device_id']].drop_duplicates()\n",
    "        devices = devices.merge(first,how ='left').merge(support,how ='left').merge(Third,how='left').fillna(0)\n",
    "\n",
    "        first_party_mean, support_party_mean, third_party_mean = devices[['1','2','3']].mean()\n",
    "        \n",
    "        first_party_up_stream = df[df['party_labels']==1]['average_out_byte_per_sec'].mean()\n",
    "        support_party_up_stream = df[df['party_labels']==2]['average_out_byte_per_sec'].mean()\n",
    "        third_party_up_stream = df[df['party_labels']==3]['average_out_byte_per_sec'].mean()\n",
    "        \n",
    "\n",
    "        data = {'Category': category,\n",
    "                'timestamp': max_ts,\n",
    "                'month': month_map[months_timestamp[i]],\n",
    "                'first_party_mean': first_party_mean,\n",
    "                'support_party_mean': support_party_mean,\n",
    "                'third_party_mean': third_party_mean,\n",
    "                'first_party_up_stream': first_party_up_stream,\n",
    "                'support_party_up_stream': support_party_up_stream, \n",
    "                'third_party_up_stream': third_party_up_stream\n",
    "        }\n",
    "        aggregated_data.append(data)\n",
    "\n",
    "\n",
    "# Create a DataFrame from the list of aggregated data\n",
    "rq_1_df = pd.DataFrame(aggregated_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98a7a1f4838337e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('QtAgg')  # Choose the appropriate backend\n",
    "plt.style.use('default')  # Set the default style\n",
    "\n",
    "# Convert 'Timestamp' column to datetime format\n",
    "# df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "# rq_1_df['month-year'] = rq_1_df['timestamp'].dt.strftime('%b-%Y')\n",
    "# Create a figure and axis for the plot\n",
    "# fig, ax = plt.subplots(figsize=(12, 6))\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 2.5), sharex=False)\n",
    "# plt.yscale('log')\n",
    "# Iterate through unique categories\n",
    "markers = ['*', 'd', 'o', 'v']\n",
    "i=0\n",
    "for category in rq_1_df['Category'].unique():\n",
    "    # Filter the DataFrame for the current category\n",
    "    category_data = rq_1_df[rq_1_df['Category'] == category]\n",
    "    # Plot the values for the current category\n",
    "    axes[0].plot(category_data['month'], category_data['first_party_mean'], marker=markers[i], label=category)\n",
    "    axes[1].plot(category_data['month'], category_data['support_party_mean'], marker=markers[i],label=category)\n",
    "    axes[2].plot(category_data['month'], category_data['third_party_mean'], marker=markers[i], label=category)\n",
    "    i += 1\n",
    "\n",
    "# Set labels and title\n",
    "# axes[0].set_xlabel('Timestamp')\n",
    "axes[0].set_ylabel('# of Endpoints')\n",
    "axes[0].set_title('First party endpoints')\n",
    "axes[1].set_title('Support party endpoints')\n",
    "axes[2].set_title('Third party endpoints')\n",
    "# axes[3].legend()\n",
    "axes[2].legend(loc='center left', bbox_to_anchor=(0, 0.65), prop={'size': 9}, framealpha=0.4)\n",
    "\n",
    "\n",
    "# Rotate category names vertically\n",
    "for ax in axes:\n",
    "    ax.grid(True, axis='y')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='center')\n",
    "# \n",
    "\n",
    "\n",
    "# todo Display and save the plot\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"../../Statistical Data/RQ4/Temporal-RQ-1-a-marked.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15ce541d696f091"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('QtAgg')  # Choose the appropriate backend\n",
    "plt.style.use('default')  # Set the default style\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 2.5), sharex=False)\n",
    "plt.yscale('log')\n",
    "markers = ['*', 'd', 'o', 'v']\n",
    "i=0\n",
    "# Iterate through unique categories\n",
    "for category in rq_1_df['Category'].unique():\n",
    "    # Filter the DataFrame for the current category\n",
    "    category_data = rq_1_df[rq_1_df['Category'] == category]\n",
    "    # Plot the values for the current category\n",
    "    axes[0].plot(category_data['month'], category_data['first_party_up_stream'], marker=markers[i],  label=category)\n",
    "    axes[1].plot(category_data['month'], category_data['support_party_up_stream'],  marker=markers[i], label=category)\n",
    "    axes[2].plot(category_data['month'], category_data['third_party_up_stream'], marker=markers[i],  label=category)\n",
    "    i += 1\n",
    "\n",
    "axes[0].set_yscale('log')\n",
    "axes[1].set_yscale('log')\n",
    "axes[2].set_yscale('log')\n",
    "\n",
    "axes[0].set_ylabel('Up-stream Data')\n",
    "\n",
    "# Rotate category names vertically\n",
    "for ax in axes:\n",
    "    ax.grid(True, axis='y')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='center')\n",
    "# \n",
    "\n",
    "\n",
    "# todo Display and save  the plot\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"../../Statistical Data/RQ4/Temporal-RQ-1-b-marked.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd02e65adca4b5e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Temporal analysis of RQ2\n",
    "\n",
    "devices_df = clean_flow_party_label.copy(deep=True)\n",
    "\n",
    "aggregated_data = []\n",
    "\n",
    "locations = ['AMERICAS', 'EA', 'APA']\n",
    "\n",
    "for category in sub_categories:\n",
    "    for i in range(2,len(months_timestamp),3):\n",
    "        # print(months_timestamp[i-1], months_timestamp[i])\n",
    "        min_ts = datetime.strptime(months_timestamp[i-2], '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "        max_ts = datetime.strptime(months_timestamp[i+1], '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "\n",
    "        df = devices_df[(devices_df['generic_category']==category) &\n",
    "                        (devices_df['min_ts']>min_ts) & (devices_df['max_ts']<=max_ts)]\n",
    "\n",
    "    \n",
    "        for loc in locations:\n",
    "            first = df[(df['user_loc']==loc) & (df['party_labels']==1)].groupby('device_id')['domain'].nunique().reset_index().rename(columns={\"domain\": \"1\"})\n",
    "            support = df[(df['user_loc']==loc) & (df['party_labels']==2)].groupby('device_id')['domain'].nunique().reset_index().rename(columns={\"domain\": \"2\"})\n",
    "            Third = df[(df['user_loc']==loc) & (df['party_labels']==3)].groupby('device_id')['domain'].nunique().reset_index().rename(columns={\"domain\": \"3\"})\n",
    "    \n",
    "            devices = df[(df['user_loc']==loc)][['device_id']].drop_duplicates()\n",
    "            devices = devices.merge(first,how ='left').merge(support,how ='left').merge(Third,how='left').fillna(0)\n",
    "    \n",
    "            first_party_mean, support_party_mean, third_party_mean = devices[['1','2','3']].mean()\n",
    "            first_party_up_stream = df[df['party_labels']==1]['average_out_byte_per_sec'].mean()\n",
    "            support_party_up_stream = df[df['party_labels']==2]['average_out_byte_per_sec'].mean()\n",
    "            third_party_up_stream = df[df['party_labels']==3]['average_out_byte_per_sec'].mean()\n",
    "\n",
    "\n",
    "            data = {'Category': category,\n",
    "                    'timestamp': max_ts,\n",
    "                    'month': month_map[months_timestamp[i]],\n",
    "                    'location': loc,\n",
    "                    'first_party_mean': first_party_mean,\n",
    "                    'support_party_mean': support_party_mean,\n",
    "                    'third_party_mean': third_party_mean,\n",
    "                    'first_party_up_stream': first_party_up_stream,\n",
    "                    'support_party_up_stream': support_party_up_stream,\n",
    "                    'third_party_up_stream': third_party_up_stream\n",
    "                    }\n",
    "            \n",
    "            aggregated_data.append(data)\n",
    "\n",
    "\n",
    "# Create a DataFrame from the list of aggregated data\n",
    "rq_2_df = pd.DataFrame(aggregated_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dc1a5d1a379323e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get unique categories and subcategories\n",
    "# categories = rq_2_df['Category'].unique()\n",
    "categories =['Media/TV', 'Home Automation', 'Voice Assistant']\n",
    "# subcategories = rq_2_df['location'].unique()\n",
    "subcategories = ['AMERICAS', 'EA', 'APA']\n",
    "\n",
    "# Create subplots for each category\n",
    "fig, axes = plt.subplots(len(categories), 3, figsize=(12,2.5*len(categories)), sharex=False)\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    category_data = rq_2_df[rq_2_df['Category'] == category]\n",
    "\n",
    "    # Create a separate subplot for each category\n",
    "    ax = axes[i, 0]\n",
    "\n",
    "    # Plot time series data for subcategories within the category\n",
    "    for subcategory in subcategories:\n",
    "        subset = category_data[category_data['location'] == subcategory]\n",
    "        ax.plot(subset['month'], subset['first_party_mean'], label=subcategory)\n",
    "\n",
    "        # Set labels and title for the subplot\n",
    "        # ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('# of First Party Domain')\n",
    "        ax.set_title(f'Category: {category}')\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Create a separate subplot for each category\n",
    "    ax = axes[i, 1]\n",
    "\n",
    "    # Plot time series data for subcategories within the category\n",
    "    for subcategory in subcategories:\n",
    "        subset = category_data[category_data['location'] == subcategory]\n",
    "        ax.plot(subset['month'], subset['support_party_mean'], label=subcategory)\n",
    "        ax.set_ylabel('# of Support Party Domain')\n",
    "        ax.set_title(f'Category: {category}')\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Create a separate subplot for each category\n",
    "    ax = axes[i, 2]\n",
    "\n",
    "    # Plot time series data for subcategories within the category\n",
    "    for subcategory in subcategories:\n",
    "        subset = category_data[category_data['location'] == subcategory]\n",
    "        ax.plot(subset['month'], subset['third_party_mean'], label=subcategory)\n",
    "        ax.set_ylabel('# of Third Party Domain')\n",
    "        ax.set_title(f'Category: {category}')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "for ax in axes[0]:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='center')\n",
    "for ax in axes[1]:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='center')\n",
    "for ax in axes[2]:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='center')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../Statistical Data/RQ4/Temporal-RQ-2.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Display the subplots\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d80b90b42f58c5d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Temporal analysis of RQ3\n",
    "location = ['AMERICAS', 'EA', 'APA']\n",
    "def flow_over_boarder_updated(dataframe):\n",
    "    for user_loc in location:\n",
    "        user_df = dataframe[dataframe['user_loc']==user_loc]\n",
    "        total_out_byte = user_df[user_df['domain_loc']=='AMERICAS']['total_out_byte'].sum() + user_df[user_df['domain_loc']=='EA']['total_out_byte'].sum() + user_df[user_df['domain_loc']=='APA']['total_out_byte'].sum()\n",
    "\n",
    "        print('{', end='')\n",
    "        for domain_loc in location:\n",
    "            out_byte = user_df[user_df['domain_loc']==domain_loc]['total_out_byte'].sum()\n",
    "            flow_percentage = out_byte*100/total_out_byte\n",
    "\n",
    "            if domain_loc != 'APA': print('%.1f' % flow_percentage, end=', ')\n",
    "            else: print('%.1f' % flow_percentage, end='},\\n')\n",
    "            \n",
    "            \n",
    "\n",
    "for category in unique_categories:\n",
    "    df = devices_df[(devices_df['generic_category']==category) & (devices_df['max_ts']<=threshold)]\n",
    "    flow_over_boarder_updated(df)\n",
    "    print(category, '< 2021 \\n\\n')\n",
    "    print()\n",
    "\n",
    "for category in unique_categories:\n",
    "    df = devices_df[(devices_df['generic_category']==category) & (devices_df['max_ts']>threshold)]\n",
    "    flow_over_boarder_updated(df)\n",
    "    print(category, '>= 2021 \\n\\n')\n",
    "    print()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c52af909578adc3d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Temporal RQ1 (appendix)\n",
    "# finding top support parties\n",
    "\n",
    "from cleanco import basename\n",
    "\n",
    "support_flows = devices_df[devices_df['party_labels']==2]\n",
    "support_parties = support_flows['domain'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# load combined information\n",
    "file_combined_domain2org = '../../Endpoint Mapping Data/Domain Data/First Party Mapping/domain2org_all_possible_sources.csv'\n",
    "domain2org = pd.read_csv(file_combined_domain2org).rename(columns={'remote_hostname':'domain'})\n",
    "# merge \n",
    "support_parties = pd.merge(support_parties,\n",
    "                           domain2org,\n",
    "                           on=['domain'],\n",
    "                           how='left'\n",
    "                           )\n",
    "\n",
    "# find support organization\n",
    "support_parties['organization'] = support_parties.apply(lambda row: row.netify_org if row.netify_org==row.netify_org\n",
    "else basename(basename(row.python_whois_org).capitalize() if (row.python_whois_org==row.python_whois_org) & (len(str(row.python_whois_org).strip())>0)\n",
    "              else basename(basename(row.bash_whois_org).capitalize() if (row.bash_whois_org==row.bash_whois_org) & (len(str(row.bash_whois_org).strip())>0)\n",
    "                            else (row.xclusive_org.capitalize() if row.xclusive_org==row.xclusive_org\n",
    "                                  else row.domain))), axis=1)\n",
    "\n",
    "support_parties = support_parties[['domain', 'organization']]\n",
    "\n",
    "# add support organization in the flow table\n",
    "support_flows = pd.merge(support_flows, support_parties, on='domain', how='left')\n",
    "\n",
    "# get support organization and count of supported devices \n",
    "support_organization = support_flows.groupby(['organization'])[['device_id']].nunique().sort_values(by='device_id', ascending=False).reset_index().rename(columns={'device_id':'device_count'}).set_index('organization')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7e61e55bf641bee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## lIST OF VENDORS WITH MULTIPLE SUPPORT PARTIES\n",
    "\n",
    "vendor_name_device_count = support_flows.groupby(['super_vendor'])[['device_id']].nunique().sort_values(by='device_id', ascending=False)\n",
    "\n",
    "for vendor in vendor_name_device_count.index:\n",
    "    device_count = vendor_name_device_count['device_id'].loc[vendor]\n",
    "\n",
    "    ## skip if device count is less than 3\n",
    "    if device_count < 10:\n",
    "        continue\n",
    "\n",
    "    print(vendor.ljust(15), end=' & \\t')\n",
    "\n",
    "    support_device_count = support_flows[(support_flows['super_vendor']==vendor)  & (support_flows['max_ts']<=threshold)].groupby(['organization'])[['device_id']].nunique().sort_values(by='device_id', ascending=False)\n",
    "    \n",
    "    for support in support_device_count.index:\n",
    "        if support_device_count['device_id'].loc[support] < 3:\n",
    "            continue\n",
    "        print('{0}({1}), '.format(support, support_device_count['device_id'].loc[support]), end='')\n",
    "\n",
    "    print('\\\\\\\\')\n",
    "    print()\n",
    "\n",
    "    print(\"\", end=' & ')\n",
    "\n",
    "    support_device_count = support_flows[(support_flows['super_vendor']==vendor)  & (support_flows['max_ts']>threshold)].groupby(['organization'])[['device_id']].nunique().sort_values(by='device_id', ascending=False)\n",
    "    for support in support_device_count.index:\n",
    "        if support_device_count['device_id'].loc[support] < 3:\n",
    "            continue\n",
    "        print('{0}({1}), '.format(support, support_device_count['device_id'].loc[support]), end='')\n",
    "\n",
    "    print('\\\\\\\\[2pt]')\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9be6d8135cd81d5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## lIST TOP SUPPORT PARTIES\n",
    "\n",
    "for org in support_organization.index:\n",
    "    device_count = support_organization['device_count'].loc[org]\n",
    "\n",
    "    if device_count < 10:\n",
    "        continue\n",
    "    print(org.ljust(15), end=' & \\t')\n",
    "\n",
    "    vendor_device_count = support_flows[(support_flows['organization']==org)  &\n",
    "                                        (support_flows['max_ts']<=threshold)].groupby(['super_vendor'])[['device_id']].nunique().sort_values(by='device_id', ascending=False)\n",
    "    for vendor in vendor_device_count.index:\n",
    "        if vendor_device_count['device_id'].loc[vendor] < 3:\n",
    "            continue\n",
    "        print('{0}({1}), '.format(vendor, vendor_device_count['device_id'].loc[vendor]), end='')\n",
    "\n",
    "    print('\\\\\\\\')\n",
    "    print()\n",
    "\n",
    "    print(\"\", end=' & ')\n",
    "    \n",
    "    vendor_device_count = support_flows[(support_flows['organization']==org)  &\n",
    "                                        (support_flows['max_ts']>threshold)].groupby(['super_vendor'])[['device_id']].nunique().sort_values(by='device_id', ascending=False)\n",
    "    \n",
    "    for vendor in vendor_device_count.index:\n",
    "        if vendor_device_count['device_id'].loc[vendor] < 3:\n",
    "            continue\n",
    "        print('{0}({1}), '.format(vendor, vendor_device_count['device_id'].loc[vendor]), end='')\n",
    "\n",
    "    print('\\\\\\\\[3pt]')\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fb7d25816e38ad3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
