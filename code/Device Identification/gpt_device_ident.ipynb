{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying devices with ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import functools\n",
    "import glob\n",
    "import base64\n",
    "import multiprocessing.pool\n",
    "import tldextract\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import threading\n",
    "import itertools\n",
    "import requests\n",
    "import pickle\n",
    "import whois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "OPENAI_API_KEY = 'sk-PLACE-YOUR-KEY-HERE'\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "models = openai.Model.list()\n",
    "\n",
    "\n",
    "def chat_completion(messages, max_tokens=20):\n",
    "    return openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt= messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0,\n",
    "        top_p=1.0,\n",
    "        frequency_penalty=0.0,\n",
    "        presence_penalty=0.0\n",
    "    )\n",
    "\n",
    "def ask_gpt_raw(messages, max_tokens=20):\n",
    "    response = chat_completion(messages, max_tokens)\n",
    "    return response.choices[0].text.translate(str.maketrans('', '', '\\n\\r\\t'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "V2_DIR = 'DEVICE FILE'\n",
    "V3_DIR = 'DEVICE FILE'\n",
    "TMP_DIR = 'DEVICE FILE'\n",
    "\n",
    "T_LOCK = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ask_gpt_raw('What is the capital of Tibet?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load device info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for filename in glob.glob(V2_DIR + 'devices.*'):\n",
    "    df = pd.read_csv(filename)\n",
    "    df_list.append(df)\n",
    "    \n",
    "v2_device_df = pd.concat(df_list).fillna('')\n",
    "v2_device_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v3_device_df = []\n",
    "\n",
    "with open(V3_DIR + 'devices.json') as fp:\n",
    "    for line in fp:\n",
    "        r = json.loads(line)\n",
    "        del r['_id']\n",
    "        v3_device_df.append(r)\n",
    "\n",
    "v3_device_df = pd.DataFrame(v3_device_df).fillna('')\n",
    "v3_device_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_netdisco(v):\n",
    "    v = str(v)    \n",
    "    if v.startswith('b64:'):\n",
    "        return base64.b64decode(v[4:])\n",
    "    return v\n",
    "\n",
    "# oui_parser = common.OUIParser()\n",
    "\n",
    "try:\n",
    "    raw_device_df = pd.read_csv(TMP_DIR + 'combined_device_raw_table.csv')\n",
    "\n",
    "except IOError:\n",
    "    raw_device_df = pd.concat([v2_device_df, v3_device_df])\n",
    "    raw_device_df['user_key'] = raw_device_df['user_key'].str.replace('-', '')\n",
    "    raw_device_df['suspected_pc'] = raw_device_df['suspected_pc'].apply(lambda v: True if v == True else False)\n",
    "    raw_device_df['user_agent_info'] = raw_device_df['ua_list'].apply(str)\n",
    "    # raw_device_df['oui_friendly'] = raw_device_df['device_oui'].apply(lambda s: oui_parser.get_vendor(s))\n",
    "    raw_device_df['oui_raw'] = raw_device_df['device_oui']\n",
    "    raw_device_df['netdisco_info'] = raw_device_df['netdisco_device_info_list'].apply(get_netdisco)\n",
    "\n",
    "    raw_device_df = raw_device_df[[\n",
    "        'user_key', 'device_id', 'device_ip', 'suspected_pc',\n",
    "        'device_vendor', 'device_name', 'device_type',   \n",
    "        'user_agent_info', 'oui_friendly', 'oui_raw', 'dhcp_hostname', 'netdisco_info']]\n",
    "\n",
    "    raw_device_df.set_index('user_key').to_csv(TMP_DIR + 'combined_device_raw_table.csv')\n",
    "\n",
    "raw_device_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_device_df['device_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "device_domain_df = pd.read_csv(TMP_DIR + 'device_hostname_df.csv')\n",
    "device_domain_df['domain'] = device_domain_df['remote_hostname'].apply(\n",
    "    lambda s: tldextract.extract(str(s).replace('?', '')).registered_domain.lower()\n",
    ")\n",
    "\n",
    "del device_domain_df['remote_hostname']\n",
    "device_domain_df = device_domain_df[device_domain_df['domain'] != '']\n",
    "device_domain_df = device_domain_df.drop_duplicates()\n",
    "\n",
    "# Get ad/trackers\n",
    "blacklist_url = 'https://raw.githubusercontent.com/notracking/hosts-blocklists/master/domains.txt'\n",
    "blacklisted_domain_set = set()\n",
    "for token in requests.get(blacklist_url).text.split():\n",
    "    components = token.split('/', 2)\n",
    "    if len(components) == 3:\n",
    "        blacklisted_domain_set.add(components[1])\n",
    "        \n",
    "# Remove ad/tracking\n",
    "device_domain_df = device_domain_df[\n",
    "    device_domain_df['domain'].apply(lambda s: s not in blacklisted_domain_set)\n",
    "]\n",
    "\n",
    "# Add user key\n",
    "device_domain_df = pd.merge(\n",
    "    device_domain_df,\n",
    "    raw_device_df[['device_id', 'user_key']].drop_duplicates(),\n",
    "    on='device_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Find domains contacted by at least 3 users\n",
    "domain_count_df = device_domain_df.groupby('domain')['user_key'].nunique().to_frame('user_count').reset_index()\n",
    "domain_count_df = domain_count_df[domain_count_df['user_count'] >= 3]\n",
    "del domain_count_df['user_count']\n",
    "\n",
    "device_domain_df = pd.merge(\n",
    "    device_domain_df,\n",
    "    domain_count_df,\n",
    "    on='domain',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "device_domain_df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device_domain_list_df = device_domain_df.groupby('device_id')['domain'].apply(lambda ss: '+'.join(set(ss))).to_frame('domains').reset_index()\n",
    "device_domain_list_df.to_parquet(TMP_DIR + 'device_domain_list_no_ad_tracking.parquet')\n",
    "device_domain_list_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ask_gpt_raw('which company operates \"mystrom.ch\"? output the company name only', max_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devices with names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "named_device_df = raw_device_df[raw_device_df['device_name'] != ''] \\\n",
    "    [['device_id', 'suspected_pc', 'device_vendor', 'device_name', 'device_type']].copy()\n",
    "print(len(named_device_df))\n",
    "named_device_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = named_device_df.sample(300)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df_copy = sample_df.copy()\n",
    "del sample_df_copy['suspected_pc']\n",
    "del sample_df_copy['device_id']\n",
    "sample_df_copy.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def contains(target_str, filter_list):\n",
    "    target_str = target_str.lower()\n",
    "    for s in filter_list:\n",
    "        if s.lower() in target_str:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vendor(t):\n",
    "\n",
    "    (_, r) = t\n",
    "    vendor = r['device_vendor'].lower()\n",
    "    \n",
    "    if contains(vendor, ['ieee', 'espressif', 'hon hai']):\n",
    "        device_label = f'{r[\"device_name\"]} - {r[\"device_type\"]}'\n",
    "    else:\n",
    "        device_label = f'{r[\"device_vendor\"]} - {r[\"device_name\"]} - {r[\"device_type\"]}'\n",
    "\n",
    "    prompt = f'I have an IoT device named \"{device_label}\". What is the company that makes this IoT device? Output the company\\'s name only.',\n",
    "    return ask_gpt_raw(prompt, max_tokens=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_gpt_raw('I have an IoT device named \"apple iphone\". What type of IoT device is this? Output the name of the device type only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_type(t):\n",
    "\n",
    "    (_, r) = t\n",
    "    device_label = f'{r[\"device_vendor\"]} - {r[\"device_name\"]} - {r[\"device_type\"]}'\n",
    "\n",
    "    prompt = f'I have an IoT device named \"{device_label}\". What type of IoT device is this? Output the name of the device type only.'\n",
    "    return ask_gpt_raw(prompt, max_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing.pool\n",
    "\n",
    "with multiprocessing.pool.ThreadPool(processes=10) as pool:\n",
    "    sample_df_copy['gpt_vendor'] = pool.map(get_vendor, sample_df_copy.iterrows())\n",
    "    sample_df_copy['gpt_type'] = pool.map(get_type, sample_df_copy.iterrows())\n",
    "\n",
    "sample_df_copy.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DHCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhcp_device_df = raw_device_df[raw_device_df['dhcp_hostname'] != ''] \\\n",
    "    [['dhcp_hostname', 'device_vendor', 'device_name', 'device_type']].copy()\n",
    "print(len(dhcp_device_df))\n",
    "dhcp_device_df = dhcp_device_df.sample(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhcp_sample_df = dhcp_device_df.copy()\n",
    "dhcp_sample_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_dhcp_vendor = lambda s: ask_gpt_raw(f'I have an IoT device named \"{s}\". What is the company that makes this IoT device? Output the company\\'s name only.', max_tokens=20)\n",
    "\n",
    "get_dhcp_type = lambda s: ask_gpt_raw(f'I have an IoT device named \"{s}\". What type of IoT device is this? Output the name of the device type only.', max_tokens=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_dhcp_vendor('Suhas-iPhone'))\n",
    "print(get_dhcp_type('Suhas-iPhone'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.pool.ThreadPool(processes=10) as pool:\n",
    "    dhcp_sample_df['gpt_dhcp_vendor'] = pool.map(get_dhcp_vendor, dhcp_sample_df['dhcp_hostname'])\n",
    "    dhcp_sample_df['gpt_dhcp_type'] = pool.map(get_dhcp_type, dhcp_sample_df['dhcp_hostname'])\n",
    "\n",
    "dhcp_sample_df.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netdisco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netdisco_device_df = raw_device_df[\n",
    "    (raw_device_df['netdisco_info'] != '[]') &\n",
    "    (raw_device_df['netdisco_info'] != '') \n",
    "][['netdisco_info', 'device_vendor', 'device_name', 'device_type']].copy()\n",
    "print(len(netdisco_device_df))\n",
    "netdisco_device_df = netdisco_device_df.sample(300).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netdisco_sample_df = netdisco_device_df.copy()\n",
    "netdisco_sample_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = netdisco_sample_df[netdisco_sample_df['netdisco_info'].apply(lambda s: '+' in str(s))]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = netdisco_sample_df.iloc[131]['netdisco_info'].decode('utf-8', 'replace')\n",
    "print(s)\n",
    "print(ask_gpt_raw('I have an IoT device with the following name. What is the company that makes this IoT device? Output the company\\'s name only.\\n\\n' + s))\n",
    "print(ask_gpt_raw('I have an IoT device with the following name. What type of IoT device is this? Output the name of the device type only.\\n\\n' + s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_netdisco_vendor = lambda s: ask_gpt_raw(f'I have an IoT device named \"{s}\". What is the company that makes this IoT device? Output the company\\'s name only.', max_tokens=20)\n",
    "get_netdisco_type = lambda s: ask_gpt_raw(f'I have an IoT device named \"{s}\". What type of IoT device is this? Output the name of the device type only.', max_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with multiprocessing.pool.ThreadPool(processes=10) as pool:\n",
    "    netdisco_sample_df['gpt_netdisco_vendor'] = pool.map(get_netdisco_vendor, netdisco_sample_df['netdisco_info'])\n",
    "    netdisco_sample_df['gpt_netdisco_type'] = pool.map(get_netdisco_type, netdisco_sample_df['netdisco_info'])\n",
    "\n",
    "netdisco_sample_df.sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNS\n",
    "\n",
    "Doesn't work well it seems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dns_device_df = raw_device_df[raw_device_df['device_name'] != ''] \\\n",
    "    [['device_id', 'device_vendor', 'device_name', 'device_type']]\n",
    "\n",
    "dns_device_df = pd.merge(dns_device_df, device_domain_df, on='device_id', how='inner').sample(300)\n",
    "\n",
    "dns_device_df.sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining multiple features\n",
    "\n",
    "... with at least two features available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df = raw_device_df.fillna('').copy()\n",
    "\n",
    "combined_df = pd.merge(combined_df, device_domain_list_df, on='device_id', how='left').fillna('')\n",
    "\n",
    "combined_df['has_user_label'] = combined_df['device_name'].apply(lambda s: 1 if s != '' else 0)\n",
    "\n",
    "combined_df['has_dhcp'] = combined_df['dhcp_hostname'].apply(lambda s: 1 if s != '' else 0)\n",
    "\n",
    "combined_df['has_netdisco'] = combined_df['netdisco_info'].apply(lambda s: 1 if s != '[]' and s != '' else 0)\n",
    "\n",
    "combined_df['has_domains'] = combined_df['domains'].apply(lambda s: 1 if s != '' else 0)\n",
    "\n",
    "combined_df['feature_count'] = combined_df['has_user_label'] + combined_df['has_dhcp'] + combined_df['has_netdisco'] + combined_df['has_domains']\n",
    "\n",
    "print(len(combined_df))\n",
    "print(combined_df['device_id'].nunique())\n",
    "\n",
    "combined_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = combined_df.value_counts('feature_count', dropna=False).to_frame('device_count')\n",
    "df['percent'] = (df['device_count'] * 100.0 / df['device_count'].sum()).round(1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "two_feature_raw_df = combined_df[combined_df['feature_count'] >= 2]\n",
    "print(len(two_feature_raw_df))\n",
    "two_feature_raw_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the length of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_length_df = two_feature_raw_df.copy()\n",
    "\n",
    "interesting_fields = ['dhcp_hostname', 'netdisco_info', 'device_vendor', 'device_name', 'device_type']\n",
    "\n",
    "for col in interesting_fields:\n",
    "    field_length_df['length_' + col] = field_length_df[col].str.len()\n",
    "    \n",
    "for col in interesting_fields:\n",
    "    print(f'Max of {col}: ' + str(field_length_df['length_' + col].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "field_length_df[['device_name', 'length_device_name']].sort_values(by='length_device_name', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = field_length_df[['netdisco_info', 'length_netdisco_info']].sort_values(by='length_netdisco_info', ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df[df['length_netdisco_info'] > 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncating long fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(two_feature_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# two_feature_df = two_feature_raw_df.copy().sample(int(len(two_feature_raw_df) / 2), random_state=0)\n",
    "two_feature_df = two_feature_raw_df.copy()\n",
    "\n",
    "for col in interesting_fields:\n",
    "    two_feature_df[col] = two_feature_df[col].apply(lambda s: str(s)[0:1200])\n",
    "    \n",
    "two_feature_df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def infer_name(pbar_inst, df_row):\n",
    "    \n",
    "    (_, row) = df_row\n",
    "    output = {}\n",
    "\n",
    "    with T_LOCK:\n",
    "        pbar_inst.update(1)    \n",
    "    \n",
    "    if row['device_name'] != '':\n",
    "        output['gpt_user_vendor'] = get_vendor(df_row)\n",
    "        output['gpt_user_type'] = get_type(df_row)\n",
    "\n",
    "    if row['dhcp_hostname'] != '':\n",
    "        output['gpt_dhcp_vendor'] = get_dhcp_vendor(row['dhcp_hostname'])\n",
    "        output['gpt_dhcp_type'] = get_dhcp_type(row['dhcp_hostname'])    \n",
    "\n",
    "    if row['netdisco_info'] != '':\n",
    "        output['gpt_netdisco_vendor'] = get_netdisco_vendor(row['netdisco_info'])\n",
    "        output['gpt_netdisco_type'] = get_netdisco_type(row['netdisco_info'])        \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tqdm.tqdm(total=len(two_feature_df), smoothing=0.1) as pbar:\n",
    "    with multiprocessing.pool.ThreadPool(processes=15) as pool:\n",
    "        gpt_list = pool.map(lambda df_row: infer_name(pbar, df_row), two_feature_df.iterrows())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove bad answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_two_feature_df = []\n",
    "for (ix, gpt_dict) in enumerate(gpt_list):\n",
    "    if gpt_dict is None:\n",
    "        continue\n",
    "    df_row_dict = two_feature_df.iloc[ix].to_dict()\n",
    "    df_row_dict.update(gpt_dict)\n",
    "    gpt_two_feature_df.append(df_row_dict)\n",
    "\n",
    "gpt_two_feature_df = pd.DataFrame(gpt_two_feature_df).fillna('')\n",
    "\n",
    "# Filter out crappy answers\n",
    "\n",
    "def remove_bad_answer(s):\n",
    "    if '___' in s:\n",
    "        return ''\n",
    "    \n",
    "    if s.endswith('.'):\n",
    "        return s[0:-1]\n",
    "    \n",
    "    if 'unknown' in s.lower() or 'espressif' in s.lower() or 'ESP' in s:\n",
    "        return ''\n",
    "    \n",
    "    if s == 'DLNA_DMR':\n",
    "        return 'Streaming Device'\n",
    "    \n",
    "    return s\n",
    "\n",
    "for col in gpt_two_feature_df.columns:\n",
    "    if col.startswith('gpt_'):\n",
    "        gpt_two_feature_df[col] = gpt_two_feature_df[col].apply(remove_bad_answer) \n",
    "\n",
    "print(len(gpt_two_feature_df))\n",
    "print(gpt_two_feature_df['device_id'].nunique())\n",
    "gpt_two_feature_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_two_feature_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove certain columns for student to build classifier\n",
    "df = gpt_two_feature_df[\n",
    "    ['device_id', 'oui_raw', 'domains'] +\n",
    "    [col for col in gpt_two_feature_df.columns if col.startswith('gpt_')]\n",
    "]\n",
    "\n",
    "df.to_parquet('../tmp-data/gpt_two_feature_df_for_ML.parquet')\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check vendor consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross product consistency checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_word_set = {\n",
    "      'electronics', 'smart', 'inc', 'technology', 'espressif', 'assistant', 'technologies', 'corporation'\n",
    "}\n",
    "\n",
    "equivalent_companies = [\n",
    "    {'lg', 'lg electronics'},\n",
    "    {'nest', 'google'},\n",
    "    {'myq', 'chamberlain'},\n",
    "    {'xiaomi', 'yeelight'}\n",
    "]\n",
    "\n",
    "def is_same_vendor(v1, v2):\n",
    "    \n",
    "    v1 = v1.lower().replace('.', '').replace('-', '')\n",
    "    v2 = v2.lower().replace('.', '').replace('-', '')\n",
    "    \n",
    "    if v1 == '' or v2 == '':\n",
    "        return ''\n",
    "    \n",
    "    # Same vendor if one is a substring of another\n",
    "    if len(v1) >= 3 and v1 in v2 and v1 not in stop_word_set:\n",
    "        return 'substring:' + v1\n",
    "    if len(v2) >= 3 and v2 in v1 and v2 not in stop_word_set:\n",
    "        return 'substring:' + v2\n",
    "    \n",
    "    # Same vendor if sharing at least one token that is not a stop word\n",
    "    v1_tokens = set(v1.split())\n",
    "    v2_tokens = set(v2.split())\n",
    "    common_tokens = (v1_tokens & v2_tokens) - stop_word_set\n",
    "    if common_tokens:\n",
    "        return 'common_tokens:' + '+'.join(common_tokens)\n",
    "    \n",
    "    # Some of these tokens are substrings of each other\n",
    "    for (t1, t2) in itertools.product(v1_tokens, v2_tokens):        \n",
    "        if len(t1) >= 3 and t1 in t2 and t1 not in stop_word_set:\n",
    "            return 'common_token_substring:' + t1\n",
    "        if len(t2) >= 3 and t2 in t1 and t2 not in stop_word_set:\n",
    "            return 'common_token_substring:' + t2\n",
    "    \n",
    "    return 'different'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_check = ['gpt_user_vendor', 'gpt_dhcp_vendor', 'gpt_netdisco_vendor', 'oui_friendly', 'domains_friendly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = {1,2,3}\n",
    "x.pop()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_two_feature_consistency_df = gpt_two_feature_df.copy()\n",
    "\n",
    "gpt_two_feature_consistency_df['domains_friendly'] = gpt_two_feature_consistency_df['domains'].apply(\n",
    "    lambda s: ' '.join(set([tldextract.extract(reg_domain).domain for reg_domain in s.split('+')]))\n",
    ")\n",
    "\n",
    "# Check across the columns for consistency\n",
    "for (col1, col2) in itertools.combinations(columns_to_check, 2):\n",
    "    new_col = f'consistency:{col1}:{col2}'\n",
    "    gpt_two_feature_consistency_df[new_col] = gpt_two_feature_consistency_df.apply(\n",
    "        lambda r: is_same_vendor(r[col1], r[col2]), axis=1\n",
    "    )\n",
    "    \n",
    "# Consolidate\n",
    "def consolidate_consistency(r):\n",
    "    r = r.to_dict()\n",
    "    \n",
    "    common_term_set = set()\n",
    "    for (k, v) in r.items():\n",
    "        if k.startswith('consistency:') and ':' in v:\n",
    "            common_term_set.add(v.split(':', 1)[1])\n",
    "    \n",
    "    # Some of the common terms are substrings of each other; merge these terms\n",
    "    terms_to_remove = set()\n",
    "    for (t1, t2) in itertools.permutations(common_term_set, 2):\n",
    "        if len(t1) >=3 and t1 in t2:\n",
    "            terms_to_remove.add(t2)\n",
    "    \n",
    "    common_term_set -= terms_to_remove\n",
    "    \n",
    "    # Merge equiv companies\n",
    "    if common_term_set in equivalent_companies:\n",
    "        common_term_set.pop()\n",
    "        \n",
    "    # If there are multiple terms and one of them is X (e.g. Spotify), remove X.\n",
    "    if len(common_term_set) > 1:\n",
    "        common_term_set -= {'spotify', 'google'}\n",
    "        \n",
    "    return '+'.join(common_term_set)\n",
    "\n",
    "gpt_two_feature_consistency_df['consolidated_vendor'] = gpt_two_feature_consistency_df.apply(lambda r: consolidate_consistency(r), axis=1)\n",
    "\n",
    "# Remove stop words and irrelevant results\n",
    "def clean_consolidated_vendor(s):\n",
    "\n",
    "    if 'hewlett' in s or 'packard' in s:\n",
    "        return 'hp'\n",
    "    \n",
    "    if 'raspberry' in s:\n",
    "        return 'raspberry-pi'\n",
    "    \n",
    "    if 'wemo' in s:\n",
    "        return 'belkin'\n",
    "    \n",
    "    if s in ('ind', 'one', 'hon', 'shenzhen', 'electric', 'media', 'hom', 'ltd', 'digital', 'things', 'the', 'night', 'security'):\n",
    "        return ''\n",
    "    \n",
    "    if s == 'free':\n",
    "        return 'freebox'\n",
    "\n",
    "    s = s.replace('electronics', '') \\\n",
    "        .replace('international inc', '') \\\n",
    "        .replace('inc', '') \\\n",
    "        .replace('corporation', '') \\\n",
    "        .replace('networks', '') \\\n",
    "        .replace('labs', '') \\\n",
    "        .replace('group', '') \\\n",
    "        .replace('llc', '') \\\n",
    "        .replace('foundation', '') \\\n",
    "        .replace('technology', '') \\\n",
    "        .replace('technologies', '') \n",
    "    \n",
    "    return s.strip()\n",
    "    \n",
    "gpt_two_feature_consistency_df['consolidated_vendor'] = gpt_two_feature_consistency_df['consolidated_vendor'].apply(clean_consolidated_vendor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_two_feature_consistency_df.iloc[4683].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_two_feature_consistency_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find all the unambiguous cases\n",
    "df = gpt_two_feature_consistency_df.groupby('consolidated_vendor')['device_id'].nunique().sort_values(ascending=False).to_frame('device_count')\n",
    "df = df[df['device_count'] >= 2].reset_index()\n",
    "df = df[~df['consolidated_vendor'].str.contains('+', regex=False)]\n",
    "df = df[df['consolidated_vendor'] != '']\n",
    "print(df.to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_two_feature_clean_df = pd.merge(\n",
    "    gpt_two_feature_consistency_df,\n",
    "    df[['consolidated_vendor']],\n",
    "    on='consolidated_vendor',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(combined_df['device_id'].nunique())\n",
    "print(two_feature_raw_df['device_id'].nunique())\n",
    "print(two_feature_df['device_id'].nunique())\n",
    "print(gpt_two_feature_clean_df['device_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_two_feature_clean_df[gpt_two_feature_clean_df['consolidated_vendor'] == 'hom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_clean_vendor(r):\n",
    "#     if r['gpt_user_vendor']:\n",
    "#         return r['gpt_user_vendor']\n",
    "#     if r['gpt_netdisco_vendor']:\n",
    "#         return r['gpt_netdisco_vendor']\n",
    "#     if r['gpt_dhcp_vendor']:\n",
    "#         return r['gpt_dhcp_vendor']\n",
    "#     return ''\n",
    "\n",
    "def get_clean_type(r):\n",
    "    if r['gpt_user_type']:\n",
    "        return r['gpt_user_type']\n",
    "    if r['gpt_netdisco_type']:\n",
    "        return r['gpt_netdisco_type']\n",
    "    if r['gpt_dhcp_type']:\n",
    "        return r['gpt_dhcp_type']\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_clean_df = gpt_two_feature_clean_df.copy()\n",
    "gpt_clean_df['gpt_clean_vendor'] = gpt_clean_df['consolidated_vendor']\n",
    "gpt_clean_df['gpt_clean_type'] = gpt_clean_df.apply(get_clean_type, axis=1)\n",
    "\n",
    "gpt_clean_df = gpt_clean_df[\n",
    "    (gpt_clean_df['gpt_clean_vendor'] != '') &\n",
    "    (gpt_clean_df['gpt_clean_type'] != '')\n",
    "]\n",
    "\n",
    "gpt_clean_output_df = gpt_clean_df[['user_key', 'device_id', 'gpt_clean_vendor', 'gpt_clean_type']]\n",
    "gpt_clean_output_df.to_parquet('../tmp-data/gpt_clean_device_ident.parquet')\n",
    "\n",
    "print(gpt_clean_output_df['device_id'].nunique())\n",
    "gpt_clean_output_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fields_to_check = ['oui_friendly', 'dhcp_hostname', 'netdisco_info', 'domains', 'device_vendor', 'device_name', 'device_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpt_clean_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_result_df = []\n",
    "\n",
    "for (_, row) in gpt_clean_df.iterrows():\n",
    "    \n",
    "    vendor = row['gpt_clean_vendor']\n",
    "    \n",
    "    vendor = vendor.replace(' home', '').replace('-pi', '')\n",
    "    \n",
    "    device_id = row['device_id']\n",
    "    \n",
    "    match_count = 0\n",
    "    output_record = {\n",
    "        'device_id': device_id\n",
    "    }\n",
    "        \n",
    "    for field in fields_to_check:\n",
    "        value = str(row[field]).lower().replace('-', '')\n",
    "        if vendor in value:\n",
    "            match_count += 1\n",
    "            output_record['matched_' + field] = 1\n",
    "        else:\n",
    "            output_record['matched_' + field] = 0\n",
    "            \n",
    "    output_record['total_matches'] = match_count\n",
    "    validation_result_df.append(output_record)\n",
    "    \n",
    "validation_result_df = pd.DataFrame(validation_result_df)\n",
    "validation_result_df = pd.merge(\n",
    "    validation_result_df, gpt_clean_df,\n",
    "    how='inner', on='device_id'\n",
    ")\n",
    "\n",
    "validation_result_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "viz_df = validation_result_df.groupby('total_matches')['device_id'].nunique().to_frame('device_count')\n",
    "viz_df['percent'] = (viz_df['device_count'] * 100.0 / viz_df['device_count'].sum()).round(1)\n",
    "viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = validation_result_df[validation_result_df['total_matches'] == 0].reset_index(drop=True)\n",
    "df.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
