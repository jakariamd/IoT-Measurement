Pre-final version Final version in: Glaenzel W., Moed H. and Schmoch U. (eds) (2004) Handbook of Quantitative Science and Technology Research: Kluwer Academic Publishers, pp. 665-694. PATENTS AND PUBLICATIONS THE LEXICAL CONNECTION Elise Bassecoulard1, Michel Zitt1,2 1 INRA, Lereco, Institut National de la Recherche Agronomique, Nantes (France) 2 OST, Observatoire des Sciences et des techniques, Paris (France) Abstract: The quantitative appraisal, partly through bibliometrics, of science-technology connections has made great progresses in the last decade. We investigate in this chapter an alternative method to the systematic exploitation of the citations of patents to science, the lexical linkage between articles and patents. We explore in particular the ability to establish correspondence tables between patent classification and scientific categories. After a recall of the methodological background (S&T linkages, lexical methods, statistical measures), we report an exploratory study based on a subset of the Chemical Abstracts database (CA) that covers both articles and patents, with a very precise indexing system. Connection measures have been established first on controlled vocabulary, secondly on some natural language fields. The comparison shows some robustness of the lexical approach, with clear limitations at the micro-level: topic-sharing between a particular article and a particular patent cannot be interpreted in the general case as the sharing of a research question. At the macro-level, for example IPC sub-classes and ISI subject categories, the lexical approach is an appealing technique, complementary to usual citation-based analysis built on very sparse matrices, as informetric performances of lexical methods can be tuned in a large scope of precision-recall features. The extension to databases specific either to articles or patents requires language processing that can be alleviated if only macro-level correspondence is sought for. Keywords: science-technology lexical methods, statistical analysis, textual data, research publications, patents nomenclatures, connection, 1 INTRODUCTION The science-technology relation and the university-industry relations are key issues to understand the knowledge society, address science policy questions and provide tools for S&T watch. In the emerging knowledge-based economy, the traditional separation between science, public good produced by a self-organized community in universities on the one hand, and technology, IPR protected in charge of industry on the other hand, with a descending flow of knowledge, has left place to more complex descriptions where the distinctions science- technology and universities-industry are neither absolute nor co- extensive. The tight fabric of relations between actors involved in STI has been described within models or metaphors such as Mode 2 and Triple Helix, with examples in biotechnology, bio-informatics and electronics. The traditional model of free science could be challenged by societal pressure and IPR extension (Dasgupta & David, 1994; David & Foray, 1995) and the view of science as a public good is even questioned (Callon, 1994). Bibliometrics is one family of methods, among many others (surveys, expert groups, etc.) able to provide measures of the linkage between science and technology at micro and macro scale. Bibliometric approach can process scientific publications on the one hand, patents on the other hand, considered as support of information amenable to statistical treatment. S&T indicators based on bibliometrics assume (a) that publication are a good representation of the contents of science (this hypothesis relies on the sociologists' and historians' works) (b) that patents (or kindred IPR forms) collect a large part of technological information, even though the patent combines several functions. These hypotheses have been largely discussed in literature: both forms are proxies, they are not fully representative, their value distribution is very skewed, etc. It remains that publication archives on the one hand, patent systems on the other hand, represent a great memory of theoretical and practical knowledge and know-how, moreover accessible with a high degree of codification that allow quantitative analysis. face-to-face tacit interactions, sharing scientific instruments etc, needs other ways if investigation (chapter by Tijssen in this volume). Transmission knowledge via assignee, (author/inventor, Publications and patents as information supports have many analogous features bibliographic institution/ referencing/ patent system referencing, bibliometric classification/ official classification, abstract, full text, references to scientific literature/ references to patent or non patent literature, etc.). Some databases store both forms of documents. Scholars have established further informetric analogies (for example statistical distributions of productivity, of citation/value) that justify to extend bibliometric techniques to patents, including the core of bibliometrics, citation analysis, with respect to fundamental differences in status and interpretation (Pavitt, 1985; Narin, 1994; Grupp, 1998). In this logic, informetric study of the linkage publication-patent is only one particular case of bibliometric data analysis and mapping, and the various entry points to investigate this linkage are the classic tetralogy: citation networks, authors/institutions network, classification schemes, lexical network - and their combination. Naturally, this particular case of bibliometrics should particularly pay attention to the discrepancies between the two entities. In this chapter, we will focus on one family of method, the study of the lexical connection. Section 2 is devoted to the general context: stakes of the measures of linkage, recall of the various bibliometric ways to address this question, especially the now classic approaches by citations and co-activity. Section 3 details the issues of lexical description of publications and patents and of methods applicable to deal with these descriptions. Section 4 reports a particular experience1 intended to assess the feasibility of a lexical approach to the issue of science-technology nomenclature concordance. Section 5 is devoted to discussion and conclusion. 2 BACKGROUND 1 Applications The measure of the linkage between patents and publications is appealing in many respects. It helps the understanding of intensity, orientation, sources of the science-technology relation. As noted above, the distinctions university/industry and publications/patents are less co-extensive than ever, and kindred bibliometric methods investigate the blurred frontier, such the study of patents from researchers (Meyer et al., 2003) and conversely the scientific publications from industrial firms (Hicks, 1995). This cross-activity generally results in observable "co-activity" publication-patents of individuals or institutions, shortly mentioned below. At micro-level, fine-grain measures able to capture the relation between a publication and a patent help to depict the scientific neighbourhood of an invention, and the possible range of technological relevance of a publication. This has an obvious interest for S&T watch and also, at earlier stages, on priority search. Commercial operators tend to offer more and more micro-level navigable links between patent and publication databases. At meso-level, a measurable relation allows to investigate knowledge transfers and potential spillovers: describing the knowledge base of particular technologies, conversely disclose the technological neighbourhood of scientific themes/research fronts, the migration of 1 results are partly based on the experiment presented at the 6th S&T indicators conference (Bassecoulard E et al., 2000) topics in the innovation process e.g. from science to technology and society, etc. At macro-level, it provides a tool for building concordance tables between scientific classifications and patent nomenclatures and investigating dependencies, a helpful instrument for S&T policy. 2 A few ways of measurement The linkage may be investigated by a variety of tools based on the informetric structure of publications and patents: citations, co- activity, classification relations, shared topics (lexical approaches). The first two are analysed in the chapter by Tijssen in this volume). a) Citation-based methods They exploit the "references" field of patents or publications. The analysis of references to non-patent literature in patents is now considered as the classic way of investigation, since pioneering works by CHI-Research (Narin, 1985) establishing basic patent indicators such as scientific intensity and immediacy of connection to the science base. The usage of patent citations in economics of S&T and economic geography is now a current practice, since the path-breaking work of Jaffe (1988) on local spillovers academy-industry, followed by many others. Extensive works conducted by Schmoch (1997) and recently by Verbeek et al. (2002) lead, at the macro-scale, to operational correspondence technological nomenclatures. scientific between tables and The symmetrical marker, citations from science to patents, has been systematically overviewed by Glaenzel & Meyer (2003). Patenting publication delays may cause some silence in citations to patents in publications, and conversely if restriction to publish results in extra-delays. Good indicator of interaction might combine the two directions of citations. At the micro-level, the patent-publication citation is relatively easy to implement as a navigation link (e.g. ISI-WPI Derwent). b) Co-activity methods In areas with a particular intensity of S-T exchanges, researchers are likely to be involved in both activities. The co-activity may take many forms. Scientists may take patents through their university, through industrial partners, or individually. They may have themselves only academic affiliations or be part time academy/ industry. Some of these configurations have been investigated by Meyer et al. (op. cit). The study of co-activity of scientists has been pioneered by Rabeharisoa (1992). Co-activity studies of academic institutions and industrial organisations are increasingly used (for example, Tijssen & Korevaar, 1997) as a part of the booming area of research on university-industry connections, especially knowledge transfer from universities in multiple forms, contractual or spillovers. c) category-sharing in common classification schemes. Classification schemes of science and technology are basically not commensurable. Whereas patent classifications belong to the strongly codified framework of patent offices, with in fact two basic official nomenclatures, USPTO and International Patent Classification (IPC) and variants, there is no commonly accepted scientific nomenclature, so that in bibliometric practice databases classifications are mostly used: ISI's journal lists and INIST Pascal classification scheme for multidisciplinary databases; classifications schemes for specialised databases. Generally speaking classifications of publications and patents are not connected to each other. However, there are some (very limited) ready-made solutions offered by thematic databases that encompass the two types of objects and propose the same classification scheme for both. The best example of a scientific database with a large coverage of patents is CA of Chemical Abstracts Service. However the classification scheme is coarse-grain, so that any in-depth correspondence should be made by other means. Any common classification scheme allowing multiple postings for an item can also be exploited by co-classification studies. d) topic-sharing: the lexical way At the micro-level, the topic-sharing principle is quite simple. The standard lexical query system implemented in most databases and web engines can be used to retrieve jointly, on a particular subject, publications and patents if present in the source(s). A publication and a patent are retrieved together if their respective sharing of terms with the query is large, with respect to the particular settings of the query engine (IR-Information Retrieval formulae/ weighting). In a more general perspective, either in bibliometrics or in cluster models of IR, direct topic-sharing between documents can be studied by various techniques. An analogy in the citation world is Kessler's bibliographic coupling (Kessler, 1963). Numerous methods or toolkits for data and text mining of S&T information have been developed with promising markets in mind. Patent offices also aim at providing efficient tools for patent information both for the final user on Internet and for their examiners (e.g; at EPO, ePatent project, 2002; bSmart presented in Sarasua, 2000). Some applications are focused on the patent- publication linkage, e.g. the non-patent prior art research, others are meant for global S&T watch (Kostoff, 2003), taxonomy development or knowledge discovery, e.g. following Swanson's (1986) approach (Weeber et al., 2000). At the meso-level, co-words maps mixing publication and patent data have been used by Engelsman & van Raan (1994) to characterise the "S&T interface" on the basis of visual analysis of multidimensional maps. The question is whether this topic-sharing approach can be operationalised in a systematic way and in a large range of scales, by calculating a direct coupling between publications (or sets of publications) and patents (or sets of patents). Section 3 addresses related issues. e) Hybrid approaches Most bibliometricians adopt a pragmatic view and are prone to combine several informetric ways. At the macro-level verbalisation of IPC has been investigated by Turner et al. (1991). Conversely, pre- classification by linguistic technologies is experimented at EPO (Krier & Zacca, 2002). Faucompré et al. (1997) extended this logic to the publication-patent linkage by using EPO catchwords to re-index automatically the scientific publications in the INIST-PASCAL database. In a different perspective, Leydesdorff (2002) linked titles of patents and titles of the scientific documents they cite. In Section 5, we will discuss some further differences between these competing and complementary approaches. 3 LEXICAL DESCRIPTION OF PUBLICATIONS AND PATENTS AND RELATED SIMILARITY MEASURES 1 Informative features in different contexts In both science and technology, examination systems assess the newness of results claimed by authors/inventors and priorities issues are met in science (Merton, 1957) as well as in technology. Naturally, intellectual property regimes deeply differ: opposition and sanction systems and legal basis are highly codified in technology, whereas informal procedures and a weak legal basis (except copyright matters) prevail on the science side, as for example Granstrand (1999) pointed out. One can expect therefore different patterns of information disclosure in scientific publications and patents. Methods and results published in academic journals must be informative enough firstly to convince referees, secondly to attract a large scope of potential users and citers as soon as possible. This is also true for self-standing article surrogates found in bibliographic data-bases: abstracts, for example, can be considered as "visit-cards". Patent applicants look for financial returns of their research legally warranted by temporary protection, in exchange for compulsory disclosure of technical features of the invention. Patent is both a legal document and a piece of technical literature. But a too precise description can be too informative for competitors and is also likely to narrow the scope of the invention (e.g. Gordon &Cookfair, 2000, Sarasua & Corremans, 2000). This shapes well-known peculiarities of patent documents, for example poorly informative original titles. With the emergence of a knowledge-based economy, more and more research teams, academic or industrial, produce results that can be both published in academic journals and patented. Disclosure issues vary among patent systems: the grace delay effective in the US system is still in discussion in the European system. Defensive publication can be chosen to prevent patenting (see for example Research Disclosure Journal archives, RDISCLOSURE@ database). In other cases a trade-off must be found in schedules and contents of scientific communication media (conferences, publications) and patents, to insure novelty at the industry/university application collaborations. As quoted in technical on-line brochures of European Patent Office (1998) "publication in a scientific periodical can sometimes give more prior-art information than a patent application, particularly where the inventor is writing about his own invention". prevent clashes date and As a result of different objectives, the respective lexical content of publication and patents, at least in the original documents, may exhibit profound differences. Further processing by databases may reduce this discrepancy. 2 Sources For patents as well as publications, usual requirements of data selection for bibliometric purposes apply: comprehensive and relevant data have to be retrieved and downloaded under time and cost constraints. These data have to be machine-readable to allow further content processing, and meet basic prerequisites for statistical treatments. This means, to be realistic, that one has to rely on bibliographic databases. The initial step of patent-publication relation, the selection of relevant documents, depends on the research question or the operational demand. There is much difference between a meso-scale study focused on ST convergence in a geographic area, a micro study of the science base of a given technology, and a macro-scale analysis aiming at a correspondence table between a scientific nomenclature and a technology nomenclature. In the first case a geographic codification of actors is sufficient. In the second case, a careful construction of the perimeter is needed (see chapter by Hinze & Schmoch in this volume). In the third case, queries have to be designed to optimize processing of large bulks of data. In that case, direct agreements with database producers are required, both for legal and practical reasons. For large scale topic-sharing studies by statistical means, prerequisites should be considered: redundancy has to be reduced; contents representations of publications and patents should as homogeneous as possible. In scientific sources, redundancy is a recurrent question that has to be dealt with in some databases or when combining sources. In patent sources, the use of basic patents or "patent families" is advisable, through the unique index in large patent system (US, EPO, PCT), or in specialized databases (Derwent, CA-Chemical Abstracts); in other cases it is necessary to process the file for reducing to patent families. Patent systems and commercial databases of publications and/or patents have various policies in terms of added value in content (e.g. for patent, reprocessed title and abstract, indexing/coding). In addition, information search and analytical tools, easy readable export formats must be carefully considered. Last but not least, commercial databases can be very expensive to access. The added value of the database partly determines the type and amount of pre-processing that will be necessary. Pre-processing (selection, downloading, extraction and representation of contents, data storage organization) will be generally reduced if data come from traditional databases and formats. Addressing full texts is another challenge. 3 Extraction and representation of contents Topic-sharing of publications and patents can be assessed by various proximity measures between individual documents and/or sets of documents of the two kinds. At first document contents have to be extracted and represented in ways amenable to quantitative processing. Various combinations of linguistic and statistical methods are used originating in Computational Linguistics, Information Retrieval and Natural Language Processing, with possible labour division between database, dynamic interfaces, and further analysis by bibliometricians/ users. Processing generally involves linguistic unification of terms, and possibly more drastic reduction of dimensions, resulting in a final indexing by broad terms or concepts. Reducing dimensions is required both for convenient handling and reducing silences that result from uncertainty of description and synonymy at large in natural language. Techniques used in data and text mining are presented in the chapter by Leopold et al. in this volume. We will focus on features related with infometric properties of S&T documents. Key terms and abstracts, the most usual representations of textual objects, are traditionally added to the source-text either by authors/inventors or by the professional indexers/ examiners. In bibliographic databases, these representations stand as surrogates of the original full texts along with document titles. However, purely human indexing tends to be assisted or substituted by computerised methods. To face information retrieval tasks and reduce manpower costs, research on automatic indexing and abstracting has been launched in the sixties (Luhn, 1957, 1958). An extensive state of the art on automatic indexing and abstracting can be found in Moens (2000). Manageable reduced global representations of publication and patent contents (abstracts in a broad sense) would be likely to open new ways of study of S&T linkages, but their automatic and reliable generation is still largely a research issue. In operational contexts of bibliometrics, pre-processing of texts to get content descriptors (indexing in a broad sense) seems more affordable. Free indexing selects significant natural language terms from the texts. The process usually involves recognition and minimal unification of words, proper name recognition, removal of stop-words, stemming and possibly phrases/multi-words recognition and normalisation. Resulting index terms can be weighted according to their importance in the texts and in the whole document collection. Statistical techniques and linguistic knowledge can be involved in term extraction and detection of .synonymies or quasi-synonymies. Potential ambiguities (homonymy, polysemy) are handled with more difficulty especially in broad subjects, and disambiguation may call for sophisticated data analysis or graph techniques. Extreme specificities may hinder further relevant document grouping. Controlled indexing establishes conceptual links between documents and items chosen in an authority list derived from a knowledge base (thesaurus - ontology) of controlled language index terms. A controlled term is the unique form assigned to terms that have similar or related meanings but unrelated surface forms (equivalence class of the controlled term, including synonyms). A thesaurus usually allows to broaden or narrow the terms that represent concepts found in texts (hierarchical relationship of generic to specific index terms). But it is rather inflexible (regular updates of knowledge bases are needed to account for changes in interests and concepts) and much less portable across different document domains. Automatic building of ontologies is an active research field. The comparison of automatic and human indexing has been studied for decades (Salton, 1969), and has been renewed by the introduction of sophisticated natural language processing techniques in information retrieval. In an experiment on machine-aided-indexing Jacquemin et al. (2002) found that a free indexing process, noisier but never silent, proved to be a valuable way of updating a thesaurus when no controlled term (or variant) were available to index a document. A major problem in S&T literature is Automatic Term Recognition, especially Acronym Recognition, be it polymers, diseases, proteins... Kostoff (2003) gives a particularly spectacular example in a test query of "IR" in the SCI source database. The problem occurs in all disciplines despite efforts of standardisation in the respective communities, and is particularly severe in chemistry (Chowdhury & Lynch, 1992). The lack of clear naming standards raises the classic issues of term ambiguity (polysemic terms) with negative effects on precision and term variation (terms that refer to the same concept) that threaten recall (e.g. Nenadic et al., 2002). Task selection, order and tuning in automatic indexing are still a matter of debate, as is the comparative efficiency of statistical and linguistic techniques (De Bruijn & Martin, 2002). Though according to Sparck Jones (1999) only high-level IR tasks - among which information extraction or automatic abstracting - could benefit from natural language processing, most practitioners seem to integrate NLP and more traditional statistical approaches in composite text processing sequences; The risk attached to complex chaining of actions may be a black-box effect. To summarize, the reduction of original texts - publications or patents - to manageable representations of contents can be achieved by statistical, morpho-syntactic and semantic treatments combined in human, automatic or machine-aided processes. The reduction can be "frozen" in permanent fields of the database or dynamically created by dedicated or general engines. These processes are not neutral vis-à-vis informetric properties of content markers (descriptors in a broad sense) used in following statistical treatments. It is up to the analyst, depending on the source/objectives, to accept the extraction or to re-process reduced forms from original texts. In the experiment reported in Section 4, we took advantage of the good quality of indexing in CA database to limit further elaboration. 4 Statistical toolbox to reduce dimensionality This objective can be achieved by statistical methods or other means, like for example semantic analysis. We focus here on the statistical toolbox. In the standard case, working datasets can be represented in a vector space model (Salton, 1968) where, in the most simple form adapted to key-word descriptions a cell value i,j is set to 1 when Lexical Unit i appears in document J and to 0 otherwise. For natural language texts, the cell value is typically set to the intra-text frequency of the term. Depending on the technical requirements and the methods, the starting point is either two rectangular matrices N1*M1 for articles and N2*M2 for patents, or a single one (N1+N2)*M where M is a common repertoire, the union of M1 and M2. The second option is more in line with further reduction of dimensions towards a common scheme. The similarity between a patent text and a publication text is a particular case of inter-text similarity. As mentioned before, the specificity of technical language and patent jargon on the one hand, and publication rhetoric on the other hand, may create false distinctions between Lexical Units (probably more than false equivalences) if "translation" problems, in the broad acceptation of translation sociology, are not addressed. False distinctions will result in silences when calculating patent-publication relations. A controlled vocabulary or the reduction to unified terms or concepts is expected to bring partial solutions. similarity Similarity between lexical units or texts has received attention from several disciplines. Firstly, a specialised area of statistics and data analysis deals with texts properties, terms distribution, allowing for similarity calculation. Correspondence analysis for example was early targeted at lexical studies (Benzecri, 1981). Secondly, a particular case of similarity between two lexical forms, a query and a document description, is central in Information Retrieval. In the standard IR "vector space model", similarity between query and documents is used to rank documents by relevance. Similarity between documents is assessed in a "lexical coupling" rationale kindred to query-document similarity, i.e. the proximity of two documents depends primarily on the number of Lexical Units they share. Among the vast choice of classic Euclidian and non Euclidian measures, some have been privileged in IR and textual statistics. Common statistical measures are often adapted either to weight lexical units as a function of frequency, or to deal with the effects of texts length. Most usual similarity indexes are cosine measures, Jaccard, Dice. Such local measures have been advocated for example by the promoters of co-word in sociology of innovation (Callon et al., 1986- 2). Probability indexes (observed linkage over expected linkage) highlight weak signals. Euclidian proximity is also used. Some weighted forms are common, for example the "best fully weighted system" where the cosine function is weighted by the TDF-IDF score, proposed by Salton & Mc Gill (1983) (see also chapter by Leopold et al. in this volume). Among Euclidian distances, a powerful form of weighting is the Chi-Square on which correspondence analysis is based. Chi-Square favours low-frequency items, and exhibits property of distributional equivalence (neutrality of aggregation of items with similar profiles). These measures can be applied to Lexical Units or documents. Specific metrics couple measures in the two universes (factor analysis, below). Further comparison of distributions of terms between two texts can be transposed from the query-document comparisons investigated by probabilistic models of IR (see below), for example in (Bookstein & Swanson, 1974). dimensionality reduction The objective is now to obtain a matrix (N1+N2)*L where L is the number of final structuring items, for example concepts, document clusters, - or directly a N1*N2 matrix of similarity after rows or columns reduction. The transformation is obtained by a reduction of dimensions involving similarity calculation and grouping of documents or, as a possible intermediary, of Lexical Units. Reduction is generally more drastic than unification of linguistic forms and keyword indexing mentioned above. As often in bibliometric applications, for example themes mapping, several ways are offered: starting with rows (grouping documents); with columns (grouping structuring items); or using dual methods dealing simultaneously with the two universes. Most dimensional reduction techniques fall into either the factor or the clustering family (see also chapter by Leopold et al. in this volume). - factor family. Factor analyses are continuous techniques that disclose latent variables as combinations of original variables. They work simultaneously in documents and items universes. Factor loading can be used to re-index documents by latent variables, for example to provide a robust re-indexing common to patents and analysis articles. particularly interesting for textual analysis, with a symmetrical treatment of individuals and variables. With a different metrics, Correspondence (Benzecri, op.cit.) the "Latent Semantic Analysis" (LSI, Deerwester et al., 1990) is increasingly used in IR and data-mining contexts. - clustering methods. Clustering methods are discontinuous methods quite common in bibliometrics and IR, for Lexical Units as well as for texts. Co-citation research fronts and co-word themes on the one hand, bibliographic coupling clusters based on citation (Kessler, op.cit. 1963) or on words on the other hand are classic examples. IR makes use of cluster models, with pre-processed clusters or interactively built clusters. Recent neuronal and other Artificial Intelligence approaches are presented in Kiang (2003). Multilevel algorithm, ascending or descending, suppose a definition of (dis)aggregation criteria and hence inter-cluster distance. Factor analysis is generally is claimed to be more powerful since it reveals latent dimensions and directly allows overlaps (as do some clustering techniques). On the other hand when high dimensionality levels are required for output (several hundreds), clustering techniques can be more effective. In fact combination of clustering and factor analysis in either sequence is common in data analysis. Some couples of techniques are consistent (Ward clustering/ correspondence analysis). Current development of data mining fosters availability and tractability of methods (neuronal clustering, fast-clustering, refined sampling, cross-checking of methods, etc.). A typical scheme is summarized on Figure 1. Figure 1. From texts to S&T aggregates linkages final state patent-article - application to similarity between ex ante aggregates Depending on the method, the reduction leads either to a new coordinate space with reduced number of columns or to a square table of similarities. For example, in the case where documents have been clustered by a hierarchic method, the similarity matrix can reflect the cut-off of the tree at a given level, with either a Boolean recoding of similarity between items (1 if the two documents belong to the same cluster, else 0), or a new similarity 0<=s<1, (1 if the two documents belong to the same cluster, else a value derived from the ultrametric distance between their clusters). In the case where terms have been clustered in macro-terms, a new similarity between documents may be calculated using this re-indexing, etc. At the end of the reduction process, or without reduction if a satisfactory controlled language is already available, the proximity between a patent and an article (and if required, proximity among patents or among articles) can be assessed on a more robust way than before reduction. This can be used for a variety of applications, as mentioned before. Let us only consider the correspondence between nomenclatures, which is a particular case of inter-group similarity assessment. Nomenclature categories are defined ex ante, say IPC categories of some level, and individual journals or set of journals. Two strategies may be used to assess inter-category proximity: Linkages between patents categoriesand article categoriesGrouping of lines (documents of thesame nomenclature category) intomacrotexts: calculating similaritiesbetween these macrotextsMatrices : A Publication* LU and B Patents*LUAggregation of document-to-document similarities by categories(Existing nomenclature categoriespub/pat considered as aggregatesof distinct documents)Extraction of content from natural language fields/text by NLP and statistical techniquesSimilarity of LUSimilarity ofdocuments( Dual methodse.g. factor an.)Reduction ofdimensionsSTAT:clustering,LSI,corresp.anal.SEM:conceptsGrouping LU into clusters,macro-terms, concepts, etc.Grouping documents intoclusters,macro-doc, latentdim.-> Reindexing documentson docs groups-> New similarity betweendocs (e.g. based onultrametric distancebetween groups )Preprocessing-> Reindexing documents onLU groups, concepts, latentvar.-> New similarities betweendocumentsFinal Matrix : documents*indexes(initial or transformed) or documents*documents a) "first group documents, then calculate similarity between groups". Articles, respectively patents, belonging to the same nomenclature category are merged into macro-texts with aggregate coordinates, typically on the (N1+N2)*L matrix. Then similarities between these macro-texts are evaluated. b) "first calculate document-to-document similarity, then aggregate similarity". The starting point is the final matrix of similarity between individual articles and individual patents. The individual linkages are then aggregated at the category level on particular rules. The first strategy has low computer requirements, since (at least at this stage) item-to-item distances are not needed. Aggregation further reduces silence. Advantages of the second strategy: simpler calculations; keeping information at the micro-level and the related optimisation means (discarding non-significant linkages); flexibility of aggregation whatever the scale. Citation linkages are Boolean: document X do or do not cite document Y. On the contrary, lexical linkages between documents used in the second strategy (b) range in a continuum: from complete discordance (nothing in common) to complete concordance (same collection of content markers/descriptors). A Boolean transformation using a threshold may be practiced. In the experiment reported below, thresholds are applied to item-to- item similarity, resulting in a Boolean measure of the publication- patent linkage. Whether Boolean or not, the same variants as for inter- cluster distances in classification algorithms are offered. A rationale of average linkage seems prudent, with optional transformations (log) limiting instability due to discrepancies in category coverage. Starting from a Boolean measure of the article-patent similarity, simple measures of connection between a science-category and an IPC category are: the gross number of connections; the ratio of observed connections to all possible connections (average linkage); the product of proportions of connected items on both sides (mutual inclusion or Equivalence Index) a few limitations Similarity-based models have been criticized for lacking theoretical anchorage and a large literature has been devoted since the seventies to alternative models of IR based on probability. The assumption that representations are uncertain (for example indexing is not an exact science) leads to key concepts of probability of relevance and probability ranking principle. Probabilistic IR has developed in many directions: model-oriented approaches, description-oriented approaches (see the review by Crestani et al., 1998). At a high level of generality, vector-space and probabilistic models are however not antagonist. The probabilistic inference model (Wong & Yao, 1995) unifies most IR models into a general approach based on a concept space where a query and a document represent a need and a content of knowledge. In this model probabilities are based on semantic relations. Users of factor analyses, though these techniques are statistical rather than semantic, would probably recognise a concept space as an acceptable framework for their quest. Besides, models based on comparison of lexical lists are not universal. Without mentioning other media challenging IR (images, videos, etc.), text-based documents may be represented by semantic structures rather than lists: set of logical rules, conceptual graphs etc. In these cases, the distance between document A and B asks for more sophisticated approaches. A general statement is that distance is proportional to the number of steps needed in a given protocol to match the representation of B by successive transformations of A. We will not expand on these models, hardly manageable up to now for large bibliometric applications, but that could gain diffusion in the future. 4 ANALYSIS ON AN EXAMPLE 1 Data In this experimentation, we intended to assess the feasibility of a lexical approach to the issue of science-technology nomenclature concordance, here a correspondence between technical fields (IPC) and scientific specialties (journal-based), namely ISI-SCI subject category codes. We have seen in section 2 that ideally, the exploration should be carried on a multidisciplinary scientific database and patent databases including the most important patent systems. In addition, to guarantee a safe statistical basis, contents of patents and articles have to be represented in a homogeneous, reliable way, which requires heavy and sophisticated language processing tasks. Not to face "all problems at once" we chose a database collecting both articles and patents in an homogeneous way, Chemical Abstracts produced by the American Chemical Society, which covers chemistry- related literature (ca. 9000 journals, conference proceedings, dissertations, technical reports, books) and patents from several granting systems, both national and international. Among the ca. 750 000 documents retrieved per year, journal articles account for 74% and patents for ca 16%, so that patent examiners use CA databases in the chemical area for much of their searching (Gordon&Cookfair, op.cit.; EPO, 1998, online information). In fact, CA coverage extends to other physical sciences, and also to life sciences through biochemistry and biophysics. Patent coverage of CA is selective but exhaustiveness is guaranteed for certain groups within a list of IPC subclasses, available on CA documentation online. Some subclasses are completely covered, especially in chemistry and metallurgy. The trial has been carried on a sub-sample of CA database, covered in a thematic CD-ROM2 "Food & Feed Chemistry". Inside this coverage, the variety of subjects has been kept: food science & technology, nutrition & cancer, toxicology, fats & oils, cosmetics. Not all groups of relevant IPC subclasses are covered in this particular source. Detailed technicalities will be published in a forthcoming article. Some results of processing steps are gathered in Table 1 below. 22969 articles and 3855 patents from the four main granting systems (USPO, JPO, EPO, WIPO-PCT) have been extracted from the CD for publication year 1997. Patents were assigned only to the main IPC class. Beside bibliographic information or patent information, indications on topics covered are found in three different types of items of CA database available both for patents and articles: - natural language: title, abstract, authors keywords - controlled terms: general subject index entries - specific terms: CA Registry numbers, unique identifiers of chemical substances Automatic inclusion of generic terms in controlled vocabulary has been kept. The advantage is to retain possible coupling that would disappear as a result of very detailed indexing. A serious shortcoming is the multiplication of forms that unduly enhance linkages by creating spurious connections from a single shared term. On both controlled and natural vocabularies, happaxes have been discarded as they cannot generate a lexical coupling, so that only 20669 articles and 3853 patents remained in the working dataset. In the following, "controlled vocabulary" refers both to controlled index terms and to CA registry numbers, the latter very valuable given the difficulty of Automatic Recognition of chemical substances. This controlled vocabulary is not totally bias-free, as CA focuses on chemistry related aspects: medical or mechanical notions, for example, may be undervalued. Nevertheless the overall quality is expected to be good and correctly emulate a reliable content extraction by sophisticated language processing techniques, some of them perhaps used as auxiliaries by the database. The experiment on natural language was limited to a basic control of natural language terms of titles and keywords3 without any attempt to process abstracts and extract concepts by reduction of dimensionality. This is meant to figure a reference of "worst case", with poor native information and extraction process, in contrast with the expected high quality of controlled vocabulary. Frequencies of the two types of descriptors among articles and patents are shown on Table 1.1. The dictionary of natural terms is 50% 2 Chemical Abstracts lend us the thematic CD ROM Food&Feed Chemistry VOLUME 1999 Issue 12, to carry on the experimentation (ca. 30 000 documents per publication year, 23000 articles and 4700 patents). Some fields available on-line, such as "roles" and 80-section classification scheme codes used for example by Morillo (2001) were not included on this version of the database. 3 extraction of words and multi-words using stop-words, standardisation of abbreviations, unification of most singular-plural forms. .Perl programs by D. Besagni, manual inspection of resulting terms. larger than the dictionary of controlled terms (after discarding the happaxes). The number of shared terms (found in both types of documents) is quite similar, resulting in a higher share for controlled descriptors (46%) than for natural terms (30%). Table 1.1 Controlled and natural descriptors in the dataset Type of Vocabulary Number of distinct descriptors 15445 22932 Controlled Natural Mean (Maximum) frequency 17 (3183) 15 (4234) Distribution Articles only 44% 64% Patents only 10% Common 46% 30% 2 measuring document similarity Principle is a document-to-document measure, the only one totally flexible for various aggregations. Usual similarity measures involve the number and/or the proportion of terms in common. We choose as set of Jaccard indexes, to allow length normalisation and term weighting. Medium frequency terms are often considered as carrying the most significant information. "When term independence and binary indexing are initially assumed, the most important terms exhibit medium frequency and the worst ones are the high frequency terms" (Salton & Wu, 1981). To favour terms of intermediate frequencies (though total independence cannot be assumed here because of automatic generation of generic terms), we used a "parabolic" weighting scheme assigning a maximum weight to words of mean frequency and a minimum weight both to words of minimum frequency and words at an upper frequency threshold. To remove poorly informative terms, the weight is set to 0 above the upper frequency threshold As very long lists of descriptors can be encountered, e.g. for chemical substances, we used a simple log transformation to minimise the length problem. For patent x and article y the plain Jaccard index is the ratio of the intersection of the two lists of descriptors to their union. We also defined a weighted index. Both can be calculated without or with log length normalisation, here in the form log(intersection)/log(union). Let nx and ny be the number of their index terms, nc the number of common descriptors, px, py, pc the corresponding sums of weights. For example, we used following Weighted Jaccard Index with Length normalization: Jacwlog = log(pc) / log(px + py -pc) Given the number of articles and patents (more than 70.106 possible pairs) we choose to keep as valid links only those involving at least 3 words and 1 common "informative" word (non-zero weighting). This first selection is all the more necessary because of automatic generation of generic terms. Selected links represent 0.25% of possible linkages on controlled vocabulary. Due to the fuzzier nature of natural vocabulary, the selection is stronger in this case, with 0.17% of possible links. Then we reduced these linkages to Boolean values with a threshold on similarity indexes (mean + one Standard Deviation on the index Jaccwlog in following examples), which discards many not significant links, and delineates a more precise neighbourhood for each article or patent. The effect of the threshold on the number of neighbours is shown on Table 1.2. Table 1.2 Effects of statistical threshold* on proximity index: Average number of linkages per article and patent Document type Article Patent Controlled vocabulary Natural vocabulary threshold 15,6 71,3 Threshold 3,2 9,5 threshold 9,9 39,7 Threshold 3,3 8,8 *Mean + 1 STD on jaccwlog 3 measuring proximities of articles and patent categories To allow flexible ways for any aggregation, we chose to aggregate pre-calculated item-to-item similarity (Boolean) rather than building a macro-text for each category and then evaluate similarity between macro-texts. We used one of the simplest measures of the linkage between i and j: the product of proportions of connected items on both sides (termed "mutual inclusion" by analogy with the Equivalence similarity index, see above). It must be kept in mind that the number of patents in IPC category i and the number of articles in science category j are counted within the dataset, i.e with respect to IPC and journal coverage of the our extract of CAS (thematic CD). A necessary improvement in further studies will concern the processing of automatic added generic terms, which proves to avoid silences in lexical coupling but tends to spuriously multiply the number of linkages. The solution to this problem should be sought at the similarity calculation stage. For convenience, results are reported only for those links that connect patents to articles in ISI-covered journals (48% of the journals and 59% of the articles). 4 results A first outcome is in terms of IR features. The number of neighbours for a given patent (Table 1-2) is about 10 for a sensible threshold corresponding to a pretty strong selection. This confirms the expected position of lexical approach compared to citation approach. Michel & Bettels (2001) record an average inferior to one non-patent reference per patent in the European system for publication year 1999, and about three times more in the USPTO. The relevance of each neighbour detected by lexical connection would be to check individually to secure interpretation in terms of recall and precision, but a first manual checking confirms the reality of a "topic-sharing" in random-examined cases. The "topic" may be of different nature and reflect some kinship in products, substances, processes, in some cases models or methods. As mentioned above, some improvements can be made to select significant linkages, but one cannot expect that the lexical linkage at the micro- level can detect article-patent couples sharing a same research question. Much higher thresholds could perhaps approach this objective at the expense of recall, but clearly better precision can be expected from citation linkages. Figure 2 compares the outcome of various settings to depict the science-base of a wide-scope IPC subclass (Miscellaneous Food preparation and preservation). Scientific categories are ordered by the rank of the expected best measure, that uses threshold on controlled vocabulary. Removing the threshold on controlled vocabulary does not alter the ranking of the three first science categories (Food Science, Applied Chemistry, Agriculture), but bring some rank shifts afterwards. The comparison between a "quick-and-dirty" processing of natural language on the one hand and controlled vocabulary on the other hand is quite interesting. Especially when using the threshold, the discrepancy between rankings on natural and controlled language is not as high as could have been expected. This is an argument in favour of a certain robustness of the lexical approach. NatVoc1: natural language, threshold ContVoc1: controlled language, threshold NatVoc0: natural language, no threshold ContVoc0: controlled language, no threshold 0,0000,0500,1000,1500,2000,2500,300FOODSCIENCEAPPL-CHEMISAGRICULTUREANAL-CHEMISBIOCH-METHODNUTRITIONHORTICULTUREBIOTECH-MICRCHEMISTRYAGRI-DAIRYBIOCHEM-MOLBIOSPECTROSCOPYCHEM-ENGScience categoriesProximity Index (jaccwlog)NatVoc1NatVoc0ContVoc1ContVoc0Miscellaneous Food Preparation & PreservationIPC A23L Fig.2. Pattern of the science-base of a IPC subclass effect of methodological options On controlled vocabulary with threshold, IPC A23L is connected first to Food Science, followed by Applied Chemistry, Agriculture and Analytical Chemistry. The scientific dependence of specialised IPC subclasses is not so scattered. For example A23C Dairy (not shown) is firstly connected to Food Science, and then four categories: Applied Chemistry, Analytical Chemistry, Agriculture-Dairy and Horticulture. Results should be carefully interpreted, if only because of the coverage arbitrarily limited to the contents of the source (thematic CD) and of perfectible methodology, especially the treatment of automatic generated terms. Table 2 establishes a tentative correspondence table in the area examined. Table 2. Tentative correspondence table (extract) based on mutual inclusion index On both sides categories are ranked by their median linkage with other side categories. Low-linkage categories are not represented (some may exhibit isolated strong connection with a few particular partner category). The CD coverage may cause important linkages to be obscured, and it should be remembered that all groups are not covered in the subclasses under examination. Concentration of linkages among privileged categories and pairs of categories is clear, but perhaps somewhat reinforced by the particular measure of proximity chosen. In spite of these reservations, two main conclusions can be put forth: FOODSCIENCEAPPL-CHEMANAL-CHEMISAGRICULTUREBIOCH-METHODAGRI-DAIRYCHEMISTRYBIOTECH-MICRBIOCHEM-MOLBIONUTRITIONPLANT-SCIREPROD-BIOMICROBIOALLERGYISI cat.codesJYDWEAAMCOADDYDBCQSADEWFQUAQIPCA61KMEDIC-TOPC12NMICR-ENZYMG01NPHYS-CHEM-ANAL**A23LMISC-FOODA23KFODDER*A23DED-OIL-FAT*C07KPEPTIDES*A01NPEST-GROWTH-REG**C12PFERMENTATIONA23BFRUIT-VEG*A23CDAIRY**A21DBAKERY**C07DHET-CYCLA23JPROT-COMPO**mutual inclusion0,000<0.01<0.02<0.03>=0.03* a) lexical connection is definitely exploitable at the macro-level to establish concordance tables b) over-interpretation should be avoided at the micro-level (one-to-one coupling). 5 DISCUSSION AND CONCLUSION 1 Complementarity of bibliometric techniques to approach the ST linkage at various scales nature of the linkage Let us summarise some technical properties of the methods above- mentioned. For memory sake, category-sharing is a very effective indication based on indexers' skill, but information is rarely available, and constrained in precision by the level of breakdown nomenclatures (for example in CA). Co-activity is relatively simple to define and capture, at least for personal co-activity, with respect to usual precautions in bibliometrics for unification of personal and institutional names. The publication-patent relation created by co-activity relation is symmetrical, and dynamic interpretation (e.g. antecedence science- technology) is possible with respect to careful examination of technical delays (patent publication cycle, grace period if applicable, publication delay, etc.). Citation linkage and lexical/linguistic linkage raise perhaps more difficult issues. Their difference is well known in bibliometrics. The citation linkage is explicit, voluntary, selective and asymmetrical. The time difference between citing and cited article is interpretable (at least in a Mertonian scheme) as a delay in the use of a former knowledge in a dependency scheme. The act of citing is strongly selective: only a few articles among citable ones are chosen, and often in a reduced repertoire, as many studies of skew distributions and Matthew effects have shown (e.g. Price, 1976). As shown by Narin (1994, op.cit.), formal properties of citations in patents are quite similar, though their nature and interpretation are deeply different. In contrast, the lexical relation is implicit, symmetrical, a- chronic. It is not established directly but disclosed by the comparison process of two texts or lists of markers. The interpretation of the linkage is in terms of topic-sharing or social relation, rather than dependency (a quite large sociological literature has been devoted to the role of texts in science, see Callon et al., 1986-1). The a- chronicity of the lexical relation does not allow to detect dependency at the micro-level. At the macro-level, dynamic interpretations of the drift or expansion of vocabulary in given fields, analysis of transfer of vocabulary from publications to patents, or from both to professional media and society are promising, but beyond the scope of this paper. Though based also on selection processes within technical and social repertoires, the lexical relation appears less scarce than citation. A topic-sharing analysis should be most of the time able to recover larger sets of "citable" objects and not only cited ones. Whether the lexical approach can operationalise this potential of topic-sharing analysis depends on many factors and settings. Statistical distributions of items and linkages help to summarise the expected conditions of recall and precision. Expected IR properties Common science-technology nomenclatures, seldom available (CA is among exception) are coarse-grain only. Co-activity at the personal level is a very high-precision and very low-recall technique, only able to measure linkages within areas showing outstanding S-T interaction (for example NTIC and biotechnology). The comparison between citations and lexical techniques is enlightened by the typical statistical properties of the two universes, which can be studied: - by usual aggregate distributions. Lexical distributions are generally described as hyperbolic Estoup-Zipf-Mandelbrot distributions (Zipf, 1949; Mandelbrot, 1953), extremely concentrated. Citations distributions are also skewed, but at a lesser degree. In this respect the "citation vocabulary" is richer i.e. more complex than the language, especially the natural language (controlled language is usually more complex after this definition). Properties of citation distribution in patents have received less attention than publication distribution properties, until Narin's works (1994, op.cit.). - by distributions of linkages and graph/ social networks properties. - by direct modelling of recall. A tool for comparing maximum expected recalls in an ideal-type of Boolean retrieval scheme is the "referencing structure" function, which unifies in a disaggregated form citation and reference distributions and can be generalised to lexical distributions for comparison (Zitt & al., 2003). The distributional properties give some evidence that the IR trade- off is different for citation analysis and lexical analysis: high- precision and low-recall for citations, high recall and low precision for lexical analysis. Lexicology can recall much more linkages, but at the expense of "false connections", due for example to high-frequency and polysemic words. In fact the signal-noise or recall-precision properties of lexical measure depend firstly on the material (database- controlled terms, natural language) and secondly on the statistical settings (type of formula, weighting, thresholds, see Section 3). Table 3 Expected properties of various measures Type of linkage Subject sharing (lexical proximity) Nature of the relation similarity of contents, evaluated: on natural language fields of notices Characteristics of linkage source: pairs of items relation: symmetrical, "a-chronic" measure: real or Boolean (depending on methods) id° IR properties Silence* Low to Med. (varies with setting) Noise** High to Med. (varies with setting) 1. natural language. controlled language. Category sharing Activity- sharing (Co-activity "scientists- inventors") Citations from patents publications on controlled language fields (high quality databases) similarity of contents, evaluated by the database classification in the same categories referencing by authors (e.g.scientific background) and indexors (priority context) source : nomenclature categories relation: symmetrical, "a-chronic" measure: Boolean (basically) source: level of individual players relation: symmetrical measure: Boolean source: individual patents relation: asymmetrical, diachronic measure: Boolean Med. (higher than above) Med. (lower than above) Low (varies with the grain of the classification High (varies with the grain of the classification Very High Very Low High Low Citations from publi. to patents * silence = "true" linkages not found (the lower the silence, the higher the recall) ** noise = linkages unduly detected or "false" linkages (the lower the noise in the results, the higher the precision) Their particular properties make this collection of methods clearly complementary. The high recall of lexical methods is particularly valuable in low-level signal areas where citation or co-activity measures are unable to capture operational linkages. Their low precision is expected to penalise interpretations at very low level (the item-to-item relation) rather than at the meso or macro-level (for example nomenclatures correspondence). It must be stressed that lexical or category-sharing proximity can be studied between any items whatever the dates of publication. This a- chronic character is an advantage in the concordance studies context. The only citation-based method with a similar immediacy, "bibliographic coupling" (reference-sharing) able to establish a linkage between two simultaneous items, is common in science bibliometrics but hardly usable for patents because of too short references list. In contrast, the classic citation studies examine diachronic relations between to- day technology and yesterday science. 2 Results and perspectives The only purpose of the experiment reported was to test the feasibility of a lexical approach to the patent-publication relationship in a favourable case where a database (CA) records both types of documents in a similar framework. Though CA coverage of scientific literature and patents is quite large, it remains focused on physical sciences and biochemistry/biophysics and matches neither the complete scope of SCI or Pascal, nor the coverage of patent offices' databases or general ones. Within the perimeter of CA, patent-publication relation analysis proved feasible using several lexical methods, either based on controlled language fields or on natural language fields. The quality of the controlled language, which includes registry numbers, is remarkable. However the analysis may be biased as a result of indexing choices made by the database providers, e.g. in CA the focus on chemical aspects and analytical methods. A serious technical problem stems from the automatic posting of generic terms, which can be solved by an adaptation of the similarity calculation algorithm. Contrary to patent citations that are readily available and only need standardisation tasks, the lexical relation should be calculated. Moreover the dimensionality reduction, desirable if natural language is used, may involve fairly heavy processing. If only macro-level relations are sought for, the process may be alleviated by the comparison of macro-texts combining documents corresponding to a publication category and a patent category respectively. However, document-to-document proximity data are more informative and allow flexible aggregation. A large scope of methodological choices is open: choices of indexes, of weighting, of thresholds, resulting in various IR properties. Results are highly sensitive to these options, and a wide range of recall-precision trade-off can be achieved. As various elaboration stages may take place in the database and in the interfaces, including not documented automatic methods, the analyst should be aware of the risk of artefact. At the micro-level, topic-sharing is rather well captured by the lexical connections, but rarely at the point where topic-sharing indicates a real dependence in terms of research questions or application path. Citation relation performs better in this respect. Two-stage perspectives are offered: - Firstly, the systematic study of patent-publication relations in databases, such as CA, which process both items with similar methods. A major limitation is of course the disciplinary coverage. - Secondly, the extension to non integrated databases, preferably multidisciplinary (ISI, more stable and INIST-Pascal, with classification scheme and indexer's descriptors) on the science side. On the patent side, the choice is between patent offices databases and Derwent, the latter with an elaborated classification scheme and lexical fields. Up to now, the multidisciplinary data-sources for patents and publications were separated. The trend today is towards an integration of knowledge information whatever the nature. A new sign perhaps is the connection between ISI publications and Derwent patents (two databases belonging to the same company) by navigation along the citation linkages. The navigation between documents along lexical connection is also likely to expand. However, facilities to aggregate linkages are not implemented so far and need particular studies. Given the stakes of a better understanding of science technology relations at all levels, benchmarking analyses of the various forms of informetric linkages (citations, topic-sharing, category-sharing, co-activity) will be of particular interest in the coming years. ACKNOWLEDGEMENTS The authors wish to thank D. Besagni, X.Polanco and B. Maudinas from the Institut de l'Information Scientifique et Technique (INIST-CNRS) for their help in the first experiments on this project, and Chemical Abstracts Service for lending us a thematic extract of their database. REFERENCES Bassecoulard, E., Polanco, X., Zitt, M. (2000). Science-Technology Relationship: the Lexical Connection. 6th S&T Indicators Conference, pp 24-27. Leiden: CWTS Benzecri, J.P. et coll. (1981). Pratique de l'analyse des données : Linguistique et lexicologie. Paris: Dunod Bookstein A., Swanson, D.R. (1974). Probabilistic models for automatic indexing. Journal of the American Society for Information Science, 25 (5) 312-318 Callon, M. (1994). Is science a public good? Science Technology and Human Values, 19, 395-424 Callon, M., Law, J., Rip, A. (1986-1). How to study the force of science. In: M. Callon, J. Law, A. Rip (Eds.), Mapping the dynamics of science and technology (pp. 3-15). London Macmillan Press Callon, M., Law, J., Rip, A. (1986-2). Qualitative scientometrics. In: M. Callon, J. Law, A. Rip (Eds.), Mapping the dynamics of science and technology (pp.107-123). London Macmillan Press Chowdhury, G.G., Lynch M.F. (1992). Automatic Interpretation of the Texts of Chemical Patent Abstracts .1. Lexical Analysis and Categorization. Journal of Chemical Information and Computer Sciences, 32, 463-467 Crestani, F., Lalmas, M., van Rijsbergen, C.J., Campbell I. (1998). 'Is This Document Relevant? ... Probably': A Survey of Probabilistic Models in Information Retrieval. ACM Computing Surveys, 30 (4) 1-30 Dasgupta P., David P.A. (1994). Toward a New Economics of Science. Research Policy, 23, 487-521, David P.A., Foray D. (1995). Accessing and expanding the science and technology knowledge base, STI Review, 16, 13-68 De Bruijn B., Martin, J. (2002). Getting to the ©ore of knowledge: mining biomedical literature. International Journal of Medical Informatics, 67, 7-18 Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., Harshman, R. (1990). Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41 (6) 391-407 Engelsman, E.C., van Raan, A.F.J. (1994) A patent-based cartography of technology. Research Policy, 23, 1-26 ePatent website : http://www.eu-projects.com/epatent/ (last visited 28/11/2003) European Patent Office (1998). Organisation of search and documentation in DG 1. http://www.european-patent-office-org/dg1/brochures/index- search-doc.htm. (last visited 28/11/2003) Faucompré, P., Quoniam, L., Dou, H. (1997). An effective link between science and technology. Scientometrics, 40 (3) 465-480 Glaenzel, W., Meyer, M. (2003). Patents cited in the scientific literature: an exploratory study of reverse citation relations. Scientometrics, 58 (2) 415-428. Gordon, T.T & Cookfair, A.S. (2000) Patent fundamentals for scientists and engineers. 2nd Edition, Boca Raton (FA): CRC Press. Granstrand, O. (1999). The economics and management of intellectual property. Cheltenham: Edward Elgar. Grupp, H. (1998). Foundations of the economics of innovation. Cheltenham: Edward Elgar. Hicks, D.M. (1995). Published Papers, Tacit Competencies and Corporate Management of the Public/Private Character of Knowledge. Industrial and Corporate Change, 4 (2) 401-424 Hinze, S., Schmoch, U. (2004) Opening the black box. Analytical approaches and their impact on the outcome of statistical patent analysis. In: W. Glaenzel, H. Moed, U. Schmoch (Eds.), Handbook of Quantitative Science and Technology Research (pp. $$). Kluwer Academic Publishers Jacquemin, C., Daille, B., Royauté, J., Polanco, X. (2002). In vitro evaluation of a program for machine-aided indexing. Information Processing and Management, 38, 765-792 Jaffe A. (1989) Real Effects of Academic Research. American Economic Review, 79, 957-970 Kessler, M.M. (1963) Bibliographic coupling between scientific papers. American Documentation, 14, 10-25 Kiang M. (2003) A comparative assessment of classification methods. Decision Support Systems, 441-454 Krier, M., Zacca, F. (2002) Automatic categorisation applications at the European patent office. World Patent Information, 24, 187-196 Kostoff, R.N. (2003). Text mining for global technology watch. In: M.A. Drake (Ed.), Encyclopedia of library and information science. (pp 2789-2799). New-York: Marcel Dekker. Leopold, E., May, M., Paass G. (2004) Data mining and text mining for science and technology research. In: W. Glaenzel, H. Moed, U. Schmoch (Eds.), Handbook of Quantitative Science and Technology research (pp. $$). Kluwer Academic Publishers Leydesdorff, L. (2002) Researching the hidden Web: patents and the technologies visited science http://www.leydesdorff.net/HiddenWeb/HiddenWeb.pdf 28/11/2003. last base Luhn, H.P. (1957). A statistical approach to mechanized encoding and searching of literary information. IBM Journal of Research and Development, 1 (4) 309-317 Luhn, H.P. (1958). The automatic creation of literature abstracts. IBM Journal of Research and Development, 2 (2) 159-165 Mandelbrot, B. (1953). An information theory of the statistical structure of language. In : W. Jackson (Ed.), Proceedings of the 2nd Symposium on Applications of Communication Theory. London : Butterworths Meyer, M., Siniläinen, T., Utecht, J.T. (2003). Towards hybrid Triple helix indicators: a study of university-related patents and a survey of academic inventors. Scientometrics, 58 (2) 321-350 Merton, R.K. (1957). Priorities in scientific discovery: a chapter in the sociology of science. American Sociological Review, 22, 635 Michel, J., Bettels, B. (2001) Patent citation analysis: a closer look at the basic input data from patent search reports. Scientometrics, 51 (1) 185-201 Moens, M.F. (2000). Automatic indexing and abstracting of document texts. Kluwer International Series on Information Retrieval. Norwell, MA: Kluwer Academic Publishers. Morillo, F., Bordons, M., Gómez, I. (2001). An approach to interdisciplinarity through bibliometric indicators. Scientometrics, 51 (1) 203-222 Narin F., Noma E. (1985). Is technology becoming science? Scientometrics, 7, 369-381. Narin F. (1994). Patent bibliometrics. Scientometrics, 30 (1) 147-155. Nenadic, G. , Mima, H., Spasic, I., Ananiadou, S., Tsujii, J. (2002). Terminology-driven literature mining and knowledge acquisistion in biomedicine. International Journal of Medical informatics, 67, 33-48 Pavitt, K. (1985). Patent statistics as indicators of innovative activities : possibilities and problems. Scientometrics, 7, 77-99. Price, D.J.d.S. (1976). A general theory of bibliometric and other cumulative advantage processes. Journal of the American Society for Information Science, 27, 292-306 Rabeharisoa, V. (1992). A Special Mediation between Science and Technology: When Inventors Publish Scientific Articles in Fuel Cells Research. Science-Based Innovation.(pp 45-72). Berlin : Springer. Dynamics H.Grupp (Ed.), In : database. www.researchdisclosure.com last visited RDISCLOSURE@ 28/11/2003 Salton, G. (1968). Automatic information organisation and retrieval. New-York : McGraw-Hill Salton, G. (1969). A comparison between manual and automatic indexing methods. American Documentation, 61-71. Salton, G., McGill, M.J. (1983). Introduction to modern information retrieval. New-York : McGraw-Hill. Salton, G., Wu, H. (1981). A term weighting model based on utility theory. In: R.N. Oddy, S.E Robertson, C.J. van Rijsbergen, R.W Williams (Eds.), Information retrieval research. (pp 9-22). Boston : Butterworths. Sarasua, L., Corremans, G. (2000). Cross-lingual issues in patent retrieval. ACM SIGIR 2000 Workshops on patent retrieval. Online last proceedings. visited 28/11/2003. http://research.nii.ac.jp/ntcir/sigir2000ws/ Schmoch, U. (1997) Indicators and the relations between science and technology. Scientometrics, 38 (1) 103-116 Sparck Jones, K. (1999). The role of NLP in text retrieval. In: T. Strzalkowski (Ed.), Natural language information retrieval (pp.1-24). Boston (MA): Kluwer Swanson, D.R. (1986). Fish oil, Raynaud's syndrome, and undiscovered public knowledge. Perspectives in biology and medicine, 30 (1) 7-18 Tijssen, R. J. W., Korevaar, J. C. (1997). Unravelling the cognitive and interorganisational structure of public/private R&D networks: A case study of catalysis research in the Netherlands. Research Policy, 25, 1277-1293 Tijssen, R. J. W (2004). Measuring science-technology interactions. In: W. Glaenzel, H. Moed, U. Schmoch (Eds.), Handbook of Quantitative Science and Technology research (pp. $$). Kluwer Academic Publishers Turner, W.A., Buffet P., Laville F. (1991). LEXITRAN for an easier public access to patent databases. World Patent Information, 13 (2) 81-90 Verbeek, A., Debackere, K.,Luwel, M., Andries, P., Zimmermann, E., Deleus, F.(2002). Linking science to technology: Using bibliographic references in patents to build linkage schemes. Scientometrics, 54 (3) 399-420. Weeber, M., Klein, H., Aronson, A.R., Mork, J.G., De Jong-Van Den Berg, L.T.W, Vos, R.. (2000). Text-based discovery in biomedicine: the architecture of the DAD-system. Journal of the American Medical Informatics Association, Suppl., 903-907 Wong, S.K.M., Yao, Y.Y (1995) On modelling information retrieval with probabilistic inference. ACM transactions in Information Systems, 13 (1) 38-68. Zipf, G. (1949). Human Behaviour and the Principle of Least Effort. Reading (MA): Addison-Wesley Zitt, M., Ramanana-Rahary, S., Bassecoulard,. E. (2003). Bridging citation and reference distribution: 1 - The referencing-structure function and its application to co-citation and co-item studies. Scientometrics, 57 (1) 93-118