# appdeveloperdevelopment # privacy policy App Development April 08, 2021 Built the Apple Ios App app as a Free app. This SERVICE is provided by at no cost and is intended for use as is. This page is used to inform visitors regarding my policies with the collection, use, and disclosure of Personal Information if anyone decided to use my Service. If you choose to use my Service, then you agree to the collection and use of information in relation to this policy. The Personal Information that I collect is used for providing and improving the Service. I will not use or share your information with anyone except as described in this Privacy Policy. The terms used in this Privacy Policy have the same meanings as in our Terms and Conditions, which is accessible at Apple Ios App unless otherwise defined in this Privacy Policy. Information Collection and Use For a better experience, while using our Service, I may require you to provide us with certain personally identifiable information, including but not limited to Free of Cost. The information that I request will be retained on your device and is not collected by me in any way. The app does use third party services that may collect information used to identify you. Link to privacy policy of third party service providers used by the app Log Data I want to inform you that whenever you use my Service, in a case of an error in the app I collect data and information (through third party products) on your phone called Log Data. This Log Data may include information such as your device Internet Protocol ("IP") address, device name, operating system version, the configuration of the app when utilizing my Service, the time and date of your use of the Service, and other statistics. Cookies Cookies are files with a small amount of data that are commonly used as anonymous unique identifiers. These are sent to your browser from the websites that you visit and are stored on your device's internal memory. This Service does not use these "cookies" explicitly. However, the app may use third party code and libraries that use "cookies" to collect information and improve their services. You have the option to either accept or refuse these cookies and know when a cookie is being sent to your device. If you choose to refuse our cookies, you may not be able to use some portions of this Service. Service Providers I may employ third-party companies and individuals due to the following reasons: To facilitate our Service; To provide the Service on our behalf; To perform Service-related services; or To assist us in analyzing how our Service is used. I want to inform users of this Service that these third parties have access to your Personal Information. The reason is to perform the tasks assigned to them on our behalf. However, they are obligated not to disclose or use the information for any other purpose. Security I value your trust in providing us your Personal Information, thus we are striving to use commercially acceptable means of protecting it. But remember that no method of transmission over the internet, or method of electronic storage is 100% secure and reliable, and I cannot guarantee its absolute security. Links to Other Sites This Service may contain links to other sites. If you click on a third-party link, you will be directed to that site. Note that these external sites are not operated by me. Therefore, I strongly advise you to review the Privacy Policy of these websites. I have no control over and assume no responsibility for the content, privacy policies, or practices of any third-party sites or services. Children's Privacy These Services do not address anyone under the age of 13. I do not knowingly collect personally identifiable information from children under 13. In the case I discover that a child under 13 has provided me with personal information, I immediately delete this from our servers. If you are a parent or guardian and you are aware that your child has provided us with personal information, please contact me so that I will be able to do necessary actions. Changes to This Privacy Policy I may update our Privacy Policy from time to time. Thus, you are advised to review this page periodically for any changes. I will notify you of any changes by posting the new Privacy Policy on this page. These changes are effective immediately after they are posted on this page. Contact Us If you have any questions or suggestions about my Privacy Policy, do not hesitate to contact me at apps@iname.com This privacy policy page was created at privacypolicytemplate.net and modified/generated by App Privacy Policy GeneratorLlama 2 Responsible Use Guide Resources and best practices for responsible development of downstream large language model (LLM)-powered products Contents Open Innovation How to use this guide Overview of responsible AI & system design Responsible AI considerations Development of the foundation model Responsible LLM product development stages Determine use case Fine-tune for product The responsible fine-tuning flow Step 1: Define content policies & mitigations Step 2: Prepare data Step 3: Train the model Reinforcement Learning from Human Feedback (RLHF) Reinforcement Learning from AI Feedback (RLAIF) Step 4: Evaluate and improve performance Red teaming best practices Privacy adversarial attacks Address input- and output-level risks Mitigating risks at the input level Mitigating risks at the output level Evaluate effectiveness Build transparency and reporting mechanisms in user interactions Feedback & reporting mechanisms Transparency & control best practices Resources for developers Combining the components of responsible generative AI Meta is committed to open science because we believe that a vibrant AI-innovation ecosystem will push the frontiers of scientific discovery and potentially revolutionize a wide array of sectors from education to agriculture, and climate management to cybersecurity. We believe that the power of AI will be harnessed to address global challenges, and unlocking that power responsibly will require democratization of access and collaboration on risk management. We want to empower developers in every industry on a global scale to drive breakthroughs, create new products and solutions, and benefit from accelerations in technological advancement and economic growth. Cover letterJULY 2023 We have already seen what open science can achieve. Meta AI's mission to advance the state of the art of AI for the benefit of all has always relied on open research and community collaboration. Meta has open sourced code and datasets for machine translation, computer vision, and fairness evaluation, while contributing to the infrastructure of the AI-developer community with tools like PyTorch, ONNX, Glow, and Detectron. We have also made our cutting-edge large language models (LLMs) Llama 1 and OPT-175B available to the scientific community through research releases. These tools have enabled exploratory research and large-scale production deployment for leading AI labs and companies, which would not have been possible without thousands of contributors across the ecosystem. Meta's open releases have spurred research in model efficiency, medicine, and conversational safety studies on evaluation methods, de-biasing techniques, and sources of hallucinations in LLMs. Making this technology more widely available for commercial use will further accelerate product integrations of LLMs, resulting in economic and competitive benefits. With these goals in mind, we are sharing new versions of Llama, the foundation LLM that Meta previously launched for research purposes. Compute costs of pretraining LLMs remain prohibitively expensive for small organizations and a proliferation of large models would increase the carbon footprint of the sector. Openly releasing these models consolidates costs and eliminates barriers to entry, allowing small businesses to leverage innovations in LLMs to explore and build text- generation use cases. Ultimately, we believe this will create a more level playing field for organizations of all sizes across the globe to benefit from the economic growth promised by the advancement of AI. Democratization of access will put these models in more people's hands, which we believe is the right path to ensure that this technology will benefit the world at large. Further, we believe this approach will facilitate efforts across the AI community to improve LLMs. Based on our experience, an open approach draws upon the collective wisdom, diversity, and ingenuity of the AI-practitioner community to realize the benefits of this technology. Collaboration will make these models better and safer. Otherwise, progress and safety work will be limited to the individual attempts of a few large companies. The commercial license for Llama prohibits certain illegal or harmful use cases, but developers who leverage foundation models will also play a critical role in ensuring that new products are built and deployed responsibly. The purpose of this guide is to support the developer community by providing resources and best practices for the responsible development of downstream LLM-powered features. We take our commitment to building responsible AI seriously, cognizant of the potential privacy- and content-related risks, as well as societal impacts. Developers using these systems should also take these risks seriously and put in place appropriate risk-management processes.As this technology becomes increasingly central to the way we work and create, all of us will play a part in collectively improving performance and safety. JULY 2023 How to use this guide This guide is a resource for developers that outlines The recommendations included in this guide reflect common approaches to building responsibly at each current research on responsible generative AI. We level of an LLM-powered product. It covers best expect these to evolve as the field advances and practices and considerations that developers should access to foundation models grows, inviting further evaluate in the context of their specific use case and innovation on AI safety. Decisions to implement market. It also highlights some mitigation strategies best practices should be evaluated based on the and resources available to developers to address risks jurisdiction where your products will be deployed and at various points in the system. These best practices should follow your company's internal legal and risk should be considered holistically because strategies management processes. adopted at one level can impact the entire system. JULY 2023 Overview of responsible AI & system design Responsible AI considerations Helping to ensure that generative AI technology does technology that have already surfaced online, such as not produce undue harm is of paramount importance. the creation or proliferation of illegal content, content Generative AI is developing rapidly and is being which may be objectionable or hateful, or content driven by research, open collaboration, and product that may result in the provision of unqualified advice. releases that are putting this technology in the hands These instances may increase as generative AI tools of people globally. Growth at this scale presents become more accessible. novel challenges for the responsible deployment of AI, yet many of the principles of responsibility remain the same as for any other AI technology. These considerations, core to Meta's approach to responsible AI, include fairness and inclusion, robustness and safety, privacy and security, and transparency and control, as well as mechanisms for governance and accountability. LLMs are one of many For our own, on-platform generative AI offerings, Meta is implementing safety measures to address context-specific risks. AI tools, and their risks should be evaluated through These mitigations are layered across different these lenses according to how they will be used. intervention points beyond those that can be Foundation models and generative AI systems represent advancements in power and accuracy compared to predecessor technologies. The increase in the performance, utility, and flexibility of these models will likely lead to their ubiquity, as the value they bring to some pre-existing use cases may outweigh operational costs of deploying the systems. The ability to generate completely new content also opens up new use cases that must be evaluated for the types of risks they may present. There are potential risks related to the misuse of this assessed and mitigated in the foundation model. As discussed in our research paper on Llama 2, some mitigations applied at early stages in the development process can be detrimental to the downstream performance and safety of the model, and some risks may be better addressed at later points in the product development cycle. Developers of generative AI-powered features that leverage open source models will similarly need to ensure that their products are safe and benefit end users, taking a holistic view of responsible AI across the entire product development cycle. JULY 2023 Mitigation points for LLM- powered products A foundation model is a general purpose AI provide opportunities to mitigate potential risks. technology whereas an LLM-powered product has It is critical that developers examine each layer of the a defined use case and performs specific tasks product to determine which potential risks may arise to enable an intended use or capability through a based on the product objectives and design, user interface, sometimes embedded in products. and implement mitigation strategies accordingly. An LLM-powered system encompasses both the foundation model and a number of product-specific layers. At various points in the product development lifecycle, developers make decisions that shape the objectives and functionality of the feature, which can introduce potential risks. These decision points also The following section presents responsible AI considerations for the different stages of LLM product development. At each of these levels, we highlight best practices for mitigating potential risks. JULY 2023 Development of the foundation model Llama 2 is a new version of the Llama 1 model, which set of diverse, publicly available online data. This was made available previously for research.The new training corpus is mostly English, which is consistent pretrained and fine-tuned versions of the model have with the current, intended use of the model. For each been updated for commercial release. In addition dataset used in training, we followed Meta's standard to performing a variety of pretraining data-level privacy review processes. And for our pretraining investigations to help understand the potential data we made an effort to remove data from capabilities and limitations of our models, we applied certain sources known to contain a high volume of considerable safety mitigations to the fine-tuned personal information about private individuals. After versions of the model through supervised fine-tuning, pretraining, the model can reproduce everything from reinforcement learning from human feedback (RLHF), simple grammatical rules to complex nuances like and iterative red teaming (these steps are covered context, sentiment, and figurative language. However, further in the section - Fine-tune for product). the model does not gain knowledge or generate Information on pretraining data, model architecture and parameters, and pretrained evaluations are contained in the Llama 2 research paper. The beliefs about the world in the way humans do. It only learns to predict the next word in a sentence based on the patterns in its training data. paper also describes in further detail the steps to If you're going to use the pretrained model, we develop the fine-tuned versions, including detailed recommend tuning it by using the techniques safety alignment efforts and evaluation results. described in the next section to reduce the likelihood Additional information is included in the model card that the model will generate unsafe outputs that are accompanying the release. The research paper and in conflict with your intended use case and tasks. If model card provide information about the capabilities you have terms of service or other relevant policies and limitations of the models, which will help that apply to how individuals may interact with your developers more safely tune, evaluate, and deploy LLM, you may wish to fine-tune your model to be Llama for new use cases. aligned with those policies. It may also be necessary During pretraining, a model builds its understanding of the statistical patterns across the sample of human language contained in its training data. The training datasets for Llama are sourced from a broad to establish new terms of service and policies specific to LLMs, or notify users about how their data or feedback provided will be used in fine-tuning. JULY 2023 Responsible LLM product development stages Developers will identify a specific product use case for the released model, and are responsible for assessing risks associated with that use case and applying best practices to ensure safety. This section is which use case(s) to focus on. Most developers outlines the considerations and mitigation strategies using this guide already have a use case in mind, available at each stage of product development such as customer support, AI assistants, internal An important decision in the development process 1Determine use case and deployment. At a high level these stages include: 1. Determine use case 2. Fine-tune for product 3. Address input- and output-level risks 4. Build transparency and reporting mechanisms in user interactions productivity tools, entertaining end-user experiences, or research applications. If you're a developer who is not certain of a particular use case for which you would want to use the model, consider focusing on use cases that improve the lives of people and society, taking into consideration different ethical principles and values. Developing or adopting an internal risk assessment process can help identify potential risks for a specific use case and should focus on how your product's end users and others could be affected. This understanding is critical for evaluating in-context safety for your product deployment, and can take forms such as surveys and interviews of potential users or market analysis of similar product applications. If you are new to considerations of values in the development and deployment of AI, refer to the principles and guidance on risk management released by academic and expert institutions, such as: • OECD's AI Principles • NIST's Trustworthy and Responsible AI Resource Center JULY 2023 2 Fine-tune for product Product-specific fine-tuning enables developers to leverage pretrained models or models with some fine- tuning for a specific task requiring only limited data and resources. Even with initial fine-tuning performed by Meta, developers can further train the model with domain-specific datasets to improve quality on their defined use case. Fine-tuning adapts the model to domain- or application- specific requirements and introduces additional layers of safety mitigations.Examples of fine-tuning for a pretrained LLM include: • Text summarization: By using a pretrained language model, the model can be fine-tuned on a dataset that includes pairs of long-form documents and corresponding summaries. This fine-tuned model can then generate concise summaries for new documents. • Question answering: Fine-tuning a language model on a Q&A dataset such as SQuAD (Stanford Question Answering Dataset) allows the model to learn how to answer questions based on a given context paragraph. The fine-tuned model can then be used to answer questions on various topics. • Sentiment analysis: A model can be fine-tuned on a dataset of labeled text reviews (positive or negative sentiment) to recognize sentiment and perform analysis to understand user satisfaction. By training the model on this task- specific dataset, it can learn to predict sentiment in text accurately. These examples showcase how fine-tuning an LLM can be used to specialize the model's capabilities for specific use cases, improving its performance and making it more suitable for specific applications. The choice of the foundation model and the task- specific dataset plays a crucial role in achieving the desired results. JULY 2023 The responsible fine-tuning flow Here are the general steps needed to responsibly fine-tune an LLM for alignment, guided at a high level by Meta's Responsible AI framework: 1. Define content policies & mitigations 2. Prepare data 3. Train the model 4. Evaluate and improve performance STEP 1: DEFINE CONTENT POLICIES & MITIGATIONS Based on the intended use and audience for your product, a content policy will define what content is allowable and may outline safety limitations on producing illegal, violent, or harmful content. These limits should be evaluated in light of the product domain, as specific sectors and regions may have different laws or standards. Additionally, the needs of specific user communities should be considered as you design content policies, such as the development of age-appropriate product experiences. Having these policies in place will dictate the data needed, annotation requirements, and goals for safety fine-tuning, including the types of mitigation steps that will be implemented. These policies will be used for labeling data in later stages when using RLHF and in additional product layers, such as making enforcement decisions for user inputs and model outputs. JULY 2023 THE RESPONSIBLE FINE-TUNING FLOW STEP 2: PREPARE DATA will depend on the specific context in which a product Developing downstream applications of LLMs begins with taking steps to consider the potential limitations, privacy implications, and representativeness of data for a specific use case. Begin by preparing and preprocessing a clean dataset that is representative is deployed. Developers should also pay attention to how human feedback and annotation of data may further polarize a fine-tuned model with respect to subjective opinions, and take steps to prevent injecting bias in annotation guidelines and to of the target domain. This involves tokenizing the text, mitigate the effect of annotators' bias. Resources handling special characters, removing unnecessary on this topic include: information, and splitting the dataset into training, • Don't Blame the Annotator: Bias Already Starts in validation, and testing sets. This step may also the Annotation Instructions involve ensuring that data are representative of the • Annotators with Attitudes: How Annotator Beliefs end users in the deployment context, for instance, by And Identities Bias Toxic Language Detection ensuring there are enough examples from relevant There are several other risks to consider, such as languages if you plan to deploy your product in a overfitting, privacy, and security. To mitigate these non-English speaking market. Representativeness risks, carefully design the fine-tuning process by of data is dependent on the use case and should be curating a high-quality dataset that is representative assessed accordingly. of your use case, conduct rigorous evaluations, and When fine-tuning for a specific use case it can be beneficial to examine training data for biases, such test your fine-tuned model's potential use via red teaming (covered in step four - Evaluate and as gender, racial, linguistic, cultural or other biases. improve performance). Understanding these patterns is important but it may not always be optimal to filter out all problematic STEP 3: TRAIN THE MODEL content in training data due to the unintended consequences this filtering may have on subsequent performance and safety mitigations, such as prompt engineering. Instead of removing data, focusing on the representativeness of the data can help prevent a fine-tuned model from perpetuating biases in its generated outputs; what is considered representative Fine-tuning involves training the model for a limited number of iterations.Once a pretrained model is loaded in the environment for fine-tuning, the training process involves setting up hyperparameters like epochs, batch size, and learning rate. The data are passed through the model, loss is computed, and weights are updated through backpropagation. The JULY 2023 THE RESPONSIBLE FINE-TUNING FLOW training progress is monitored using a validation set, Reinforcement Learning from Human and hyperparameters are adjusted as necessary. Feedback (RLHF) Fine-tuning an LLM for safety can involve a number To align the output of LLMs with user expectations of techniques, many of which the research paper on and values, one approach that developers should Llama 2 describes in greater depth. These techniques consider is implementing Reinforcement Learning can include: • Supervised Fine-Tuning (SFT): Supervised fine- tuning using data annotated across helpfulness and safety. • Reinforcement Learning from Human Feedback (RLHF) or AI Feedback (RLAIF): Training safety and helpfulness reward models to support RLHF techniques iteratively improves models and makes them more robust to jailbreaking techniques. • Targeted Safety Context Distillation: Context distillation for safety helps the model associate adversarial prompts with safe responses by prefixing a safe preprompt such as "You are a safe and responsible assistant" to the adversarial prompt, followed by fine-tuning on new outputs. from Human Feedback (RLHF) mechanisms. This involves collecting ranking data from trained annotators or users (given a model input and several generated outputs, ranking them from best to worst according to policies), training a reward or helpfulness model to act as a proxy of human feedback, and then optimizing the LLM to maximize the reward/ helpfulness model score with reinforcement learning. Reinforcement Learning from AI Feedback (RLAIF) Reward models can also be improved and tailored to specific policies by using Reinforcement Learning from AI Feedback (RLAIF). The fine-tuned LLM itself can be used to create synthetic ranking data for reward model training. Given a model input, response pairs and relevant guidelines, the LLM predicts which response would best follow the guidelines. The synthetic reward modeling data are then used to augment the reward model's training data. JULY 2023 STEP 4: EVALUATE AND IMPROVE PERFORMANCE Evaluation strategies and processes to improve The final stage is to evaluate the fine-tuned model on performance can include: a test set to measure its performance on the specific • Automatic evaluation leverages automatic task and against safety benchmarks, according to benchmarks and classifiers to judge the output the use case. This includes analyzing the model's with respect to a specific category of risk. strengths and weaknesses based on evaluation results, gathering more data to further enhance performance and safety, and iterating until satisfied with the model's performance using holdout test datasets. There are many complementary types of evaluations that are useful for measuring risks in models, • Manual evaluation leverages human annotators or subject matter experts to judge the model's output. • Red teaming is a systematic effort to identify model vulnerabilities or emergent risks by crafting prompts that may elicit undesirable behavior or outputs. This type of manipulation of the model can be used to test safeguards and attempts to including automatic benchmarks, manual annotations "jailbreak" the model. by human raters, and evaluations using an LLM itself as a rater. The Holistic Evaluation of Language Models discusses some of the commonly used automatic benchmarks. JULY 2023 Red teaming best practices • Regular testing: The model should undergo Red teams should adopt systematic approaches to testing and measurement, while estimating real-world behaviors and threat vectors to the extent possible. • Diversity: Red teams should include a diverse regular testing to determine whether or not mitigations against attacks are effective. This requires some form of automated evaluation, either with human labeling, which can be expensive, or with classifiers trained to recognize responses that fall under the set of people from a range of professional risk categories. backgrounds that are representative of a broad group of potential users and demographics. Red teams can be composed of internal employees, experts, or community members. • Subject matter expertise: Subject matter experts should judge model responses based on their familiarity with the identified risk categories and label responses that fall under each category. JULY 2023 3 Address input- and output-level risks Without proper safeguards at the input and output levels, it is hard to ensure that the model will respond properly to adversarial inputs and will be protected from efforts to circumvent content policies and safeguard measures ("jailbreaking").Mitigations at the output level can also act as a safeguard against generating high-risk or policy-violating content. Enforcement of content policies can be managed through automated systems and manual analysis of samples and reports. Automated systems may include machine learning and rule-based classifiers Privacy adversarial attacks Additional privacy protections should be considered when releasing the product, to test whether bad actors may be able to improperly extract information. A privacy adversarial attack is a method where attackers can exfiltrate data from a model. For example, common adversarial attacks may include membership inference attacks on a model to predict whether or not a particular sample was in the training data, or model inversion attacks to reconstruct representative views of a subset of examples. Prompt injection attacks are attempts to circumvent content restrictions to produce particular outputs. A red team privacy adversarial attack conducted by a for filtering prompt inputs or system outputs. Usage company may be able to demonstrate the feasibility or consequence policies may be defined for when of such attacks. In scenarios where companies users repeatedly violate those policies. fine-tune models using personal data (pursuant to applicable privacy laws), they should consider testing the outputs to see if the model memorized particular data. This approach may be especially useful for testing models that are intended to be deployed as AI assistants or agents. JULY 2023 Mitigating risks at the input level • Prompt engineering: Direct modifications of The input refers to the information provided by the user and passed to the system. The developer does not control what the user inputs. Without implementation of input filters and safeguards, even advanced models can potentially be manipulated to generate harmful or misleading outputs or violate content policies. Although safeguards to protect privacy and prevent potential harm can be developed by tuning the model, it should be expected that even after rigorous design and testing, those safeguards will not have perfect performance and may be subverted. Additional safeguards include direct filtering and engineering of the inputs. For these to be effective, model inputs must be well-formatted. These approaches include: • Prompt filters: Even when inputs may not violate content policies, the model may produce problematic engagements or outputs. In these cases, it may be appropriate to filter, block, and hard code responses for some inputs until the model can respond in the intended way. This tactic may come with tradeoffs to the user's experience and agency in engaging with the system. Thus, the safety benefits of such restrictions or modifications should be weighed against those costs, until more robust solutions are developed. the user inputs are an option for guiding the model behavior and encouraging responsible outputs, by including contextual information or constraints in the prompts to establish background knowledge and guidelines while generating the output. Modifications may be done in a variety of ways, such as with automated identification and categorization, assistance of the LLM itself, or rules engines. These can help improve the user experience by creating more diversity and expressiveness from the model. For example, prompt engineering can be leveraged to direct the model to include more diverse references or apply a certain tone or point of view. Prompt engineering rules may be hard coded or probabilistic. Alongside prompts, it might be beneficial to provide instructive sample inputs and outputs that illustrate the desired responsible behavior. JULY 2023 Mitigating risks at the output level unreasonably restrict the usage of your model. Based on the downstream use case, you can apply several approaches for detecting and filtering the generated output of models for problematic or policy- violating content. Here are some considerations and best practices for filtering outputs. Any output filter Words often have context-dependent meanings, and terms that could be sexually suggestive, for example, may also be used in medical contexts. Content policies will help articulate the specifics between permitted and prohibited topics to users. mitigation should include all languages that are used • Classifiers: The more effective, but also more in the region where your product is available. difficult, approach is to develop classifiers that • Blocklists: One of the easiest ways to prevent the generation of high-risk content is to compile a list of all the phrases that your model should not, under any circumstances, be permitted to include in a response. Many words are easily identifiable as problematic; slurs, for example, are typically offensive no matter their context. While blocklists are attractive for their simplicity, they may detect and filter outputs based on the meaning conveyed by the words chosen.Classifiers, when properly trained on known examples of a particular sentiment or type of semantic content, can become highly effective at identifying novel instances in which that sentiment or meaning is expressed. JULY 2023 Evaluate effectiveness Some best practices include: While prompt filtering and engineering are critical safety mitigations, it's important to monitor effectiveness and avoid unintended consequences. • Test for unintended outcomes. Take caution that prompt engineering doesn't inadvertently create other issues. Test end-to-end performance after any prompt engineering to ensure desired behavior. • Evaluate effectiveness of safeguards. Many publicly available datasets offer collections of prompts that are designed to benchmark against specific concerns when used as inputs. After model responses are collected, they can be evaluated by using standardized metrics. • Adjust for different languages. Prompt filtering and engineering mitigations should include all languages that are used in the region where your product is available; the effectiveness of these mitigations may be dependent on linguistic and community-level nuances. Llama was trained primarily on data in English, in accordance with its intended use, so it is critical to carefully evaluate any mitigations in other languages. JULY 2023 4 Build transparency and reporting mechanisms in user interactions performance and avoid repeating mistakes. Product developers should review feedback by monitoring the rate that users report model outputs and by manually reviewing those reports and selected samples of Releasing an LLM-powered feature for users to model outputs. interact with can reveal new use cases as well as new concerns. User interactions can provide critical feedback, which can be used for reinforcement learning (discussed in a previous section). This is also an opportunity to provide appropriate notice, transparency, and control to users, which can lead to greater satisfaction and trust in the feature. Feedback & reporting mechanisms Facilitating user interaction with appropriate feedback or reporting mechanisms is key to ensuring quality output. Feedback mechanisms can be as simple as positive or negative (thumbs up or thumbs down), and tailoring feedback to the types of issues that may be foreseeable based on a company's use case (for example, AI assistants) can enhance the quality of feedback. This feedback can be used by developers to improve the model in more targeted ways. Providing an option for freeform feedback within a reporting mechanism can also reveal new or unanticipated concerns raised by users. Furthermore, users can identify and highlight errors, unsafe behaviors, or suboptimal actions that the model might not recognize on its own. Developers can further train the model with this feedback to improve Transparency & control best practices To ensure high-quality feedback and provide end users with notice and choice about their interactions with your AI assets, developers should consider the following practices for user interactions: • Transparency: Developers should consider ways to provide transparency to end users regarding potential risks and limitations of the system prior to or at the time of user interaction. For instance, notice to users that they are interacting with an AI-powered chatbot may increasingly be required in certain markets, and is a best practice to address concerns that may be related to false or incorrect information. Developers should neither claim nor imply that an AI agent is human, especially when building and deploying anthropomorphized interfaces. Context, intent, sensitivity, and likelihood to deceive are additional critical factors in ascertaining when and how to be transparent. Work with your appropriate advisors to determine the types of transparency that should be provided to users, including whether users should be informed that their responses may be used to fine-tune a JULY 2023 model. Developers should also consider the use • Control mechanisms: Additional controls could of system cards to provide insight into their AI include giving users the option to customize the system's underlying architecture and explain how outputs generated by an LLM. For example, a a particular AI experience is produced. Further user could select or reject outputs from a list of best practices are outlined in the Partnership on multiple options. Offering editing capabilities AI's Responsible Practices for Synthetic Media. can also enhance a user's sense of agency over outputs, and developers should consider education flows that can set a user up for success, such as offering prompt suggestions or explanations of how to improve an output. JULY 2023 Resources for developers There is a value chain emerging to support the Filters and classifiers: responsible training and use of LLMs, which we believe will be advanced through more open releases and sharing of best practices, tools, and benchmarking.A growing number of researchers, platforms, companies, and developer communities are contributing to this ecosystem. We expect more tools for the responsible development of LLM to become available over time and are committed to fostering more open exchange of safety research and tools to support developers. It is critical to remain aware of the latest versions of models and use the most current version to get the best results. Our partnership to make Llama available on the Azure Model Catalog will enable developers using Microsoft Azure to leverage their cloud-native tools for content filtering and safety features. Below, we provide a few notable hubs and implementation resources for developers, but this list is not exhaustive. Microsoft also offers a repository of Responsible AI Resources. • Content-filtering systems from Azure, supporting a range of languages: https://learn. microsoft.com/en-us/azure/cognitive-services/ content-safety/overview • Filter lists for generation of problematic words: https://github.com/LDNOOBW/naughty-words-js • Recipes for safety in open-domain Chatbots, including a sensitive topics classifier: https://parl. ai/projects/safety_recipes/ Platforms for tools and evaluations: • Benchmarking of LLMs by Stanford's Center for Research on Foundation Models, HELM: https:// crfm.stanford.edu/helm/latest/ • EleutherAI LLM Evaluation Harness: https:// github.com/EleutherAI/lm-evaluation-harness • Huggingface Hub which hosts open source models, datasets, and is a space for developers to share safeguards and access benchmarking information: https://huggingface.co/docs/ hub/index • GenAI Ops Tools database curated by Credo.AI: https://www.credo.ai/gen-ai-ops-landscape JULY 2023 Reporting resources: If you have any information about issues, violations, • Reporting bugs and security concerns: facebook.com/whitehat/info or problems, please help keep our communities safe • Reporting violations of the Acceptable by using our reporting resources. • Reporting issues with the model: github.com/ facebookresearch/llama • Reporting risky content generated by the model: developers.facebook.com/ llama_output_feedback Use Policy or unlicensed uses of Llama: LlamaUseReport@meta.com JULY 2023 Combining the components of responsible generative AI Each stage of model development presents data-collection stage to user feedback, be sure to opportunities to enhance the safety of your AI keep your overall goal in mind. feature. However, it's crucial to acknowledge the interconnectedness of these stages and how the decisions made at each stage can impact others. Building a responsible AI ecosystem requires ongoing efforts to refine each component and ensure they work together effectively. Here are some key considerations for implementing these components in unison: • Holistic optimization. Although each component has a specific role and optimization goal, components are not isolated entities. Over- optimization of one component without considering its interaction with others can lead to suboptimal outcomes. For instance, over- filtering training data for safety might make later fine-tuning less effective, as the model may not recognize and handle unsafe content • Standardizing processes for learning from feedback/errors. Embracing an iterative model- development mindset is crucial. Establish a well- defined process for incorporating new learnings into subsequent model training. This process should include consistent feedback analysis, prioritization of identified issues, and systematic application of learnings in the next iteration of model training. The field of generative AI is complex, ever-evolving, and full of potential, but it's not without risks. The key to unlocking its benefits while mitigating the downsides is responsible AI practice. This practice starts with understanding the complexities of the technology, the potential impacts on users and society, and the importance of continuously striving for improvement. appropriately. This is why different layers of By embracing the principles of transparency, safety mitigations throughout the development accountability and user empowerment, as well lifecycle are critical for creating high-performing, as having a commitment to ongoing learning and responsible products. • Alignment of objectives at each stage of development. To yield a product that is optimized for your target use cases, it's essential to have a consistent set of goals and outcomes that guide each stage of the process. From the improvement, you can ensure that your AI feature is not only innovative and useful but also responsible and respectful. We hope this guide serves as a valuable tool in your journey toward responsible AI practice. JULY 2023