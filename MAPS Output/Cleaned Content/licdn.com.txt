Access Denied You don't have permission to access "http://media.licdn.com/dms/document/D561FAQGvRiPBW7TICw/feedshare-document-pdf-analyzed/0/1683746877259?" on this server. Reference #18.bf02de17.1692030731.29effe4ePrivacy and Information Security Committee RECENT PRIVACY AND INFORMATION SECURITY DEVELOPMENTS February 2022 Prepared for the PRIS Committee by Reema Moussa and Christopher Frascella MDLs Proceed against Clearview and Macy's for Use of Facial Recognition Tech The multi-district litigation (MDL) against Clearview AI survived a motion to dismiss in February, the federal judge applying intermediate scrutiny to the Biometric Information Privacy Act (BIPA) due to the mix of speech and non-speech elements in the company's facial recognition technology. Judge Coleman noted that Clearview AI captures public photos from the internet and "harvests an individual's unique biometric identifiers and information-which are not public information- without the individual's consent." She found that this presents "a grave and immediate danger to privacy, individual autonomy, and liberty", and that BIPA is narrowly tailored to address this danger through its consent provision. Clearview AI's First Amendment defense also failed last year in Illinois state court, and in 2020 in Vermont state court. Judge Coleman rejected Clearview AI's argument that BIPA applies only to the sale of biometric data. The same judge presided over the MDL against Macy's for the retailer's use of Clearview AI. Judge Coleman's determinations included finding Article III standing under BIPA for Macy's violation of plaintiffs' privacy interests but not standing under California's Unfair Competition Law due to lack of economic injury. Meta to Pay $90 Million and Delete Data Obtained by Tracking Logged-out Facebook Users, after Ninth Circuit Finds Economic Injury Facebook is alleged to have collected data on users who visited third-party websites between April 2010 and September 2011, even after those users were logged out of Facebook, in violation of the Wiretapping Act and California's Invasion of Privacy Act. The $90 million dollar settlement Facebook parent company Meta is poised to pay represents disgorgement of 100% or more of Facebook's profits connected with this data, and is supplemented by significant injunctive relief: the deletion of all user data so collected. If approved, the settlement will be among the 10 largest data privacy class settlements in U.S. history. Notably, the District Court trimmed the suit twice and then dismissed it for standing and pleading deficiencies. The Ninth Circuit revived the suit rejecting Facebook's argument that it was exempt from relevant wiretap laws as a "party" to the communications (Facebook argued the third-party websites hosted Facebook plug-ins), and accepting plaintiffs' economic harm argument based not on diminution in the value of the data possessed by the plaintiffs but instead on Facebook's profiting from the data by selling it to advertisers. The Supreme Court denied cert, paving the way for settlement negotiations. EU Telecom Regs May Inspire Digital Markets Act In an interview with MLex, the Chair of the Body of European Regulators for Electronic Communications (BEREC), Annemarie Sipkes, detailed how the telecom regulator's policies could inform the EU's Digital Markets Act (DMA). The BEREC Chair serves as an advisor to final DMA negotiations between EU governments, the European Parliament, and the European Commission. The DMA seeks, among other things, to prevent anti-competitive pricing and preferential behavior and to promote interoperability from "gatekeeper" online platforms. Chair Sipkes discussed BEREC's "ex ante" regulation in which companies with "significant market power" are required to open up markets to rivals. The Chair also addressed structuring complaint processes to distinguish legitimate access denials (e.g. apps presenting security concerns) from anti-competitive denials, and relying on complaints as grassroots intelligence. EDPS Calls to (Mostly) Ban Pegasus The European Data Protection Supervisor (EDPS) has called for an EU-wide ban to Pegasus Spyware, in all but the most exceptional circumstances. Pegasus is a cyberweapon with the potential for zero-click infiltration of a user's phone, without the ability to trace who conducted the surveillance. Once a device is compromised, Pegasus can actively record with the phone's microphone or video camera, grab personal data like calendars, contacts, and passwords, or download all the data on the device, including emails, photos, and browsing history. The European Parliament recently initiated an inquiry into Pegasus, which is significant in light of the discovery that Hungary and Poland have used Pegasus, and may have included journalists among their targets. As of 2018, Pegasus was in use by more than 40 countries. Toronto's CitizenLab first reported on Pegasus in 2016, the U.N. special rapporteur on freedom of expression called for a moratorium on the technology in 2019, and the U.S. Commerce Department added Pegasus to its Entity List in 2021.NSO Group asserts that Pegasus is a necessary encryption-circumventing tool to stop terrorists, human traffickers, drug cartels, and pedophiles, and that "there is not enough education about what a national security or intelligence organization needs to do every day [to provide] basic security to their citizens." The company released its first annual Responsibility Report in June 2021, in which it cited that in less than 0.5% of instances Pegasus was misused. New York Privacy Act passes state Senate's Consumer Protection Committee On February 8, 2022, The New York Privacy Act was approved by the New York Senate Consumer Protection Committee. Similar to the California Consumer Privacy Act (CCPA), the Massachusetts Information Privacy and Security Act, and the Colorado Privacy Act, the Act aims to "strengthen consumer privacy rights by requiring companies to disclose their methods of de- identifying personal information, placing safeguards around data sharing, and allowing consumers to obtain the names of all entities with whom their information is shared." The Act will empower individuals to bring private rights of action for violations of the Act by "legal persons that conduct business in New York State or produce products or services intentionally targeted to residents in New York State," so long as the businesses meet the following stipulations: (1) have annual gross revenues of at least 25 million dollars; (2) control or process personal data of 100,000 consumers or more; (3) capture personal data of 500,000 people nationwide, and control or process personal data of 10,000 consumers; or (4) develop 50% of their gross revenue from the sale of personal data, and control or process personal data of 25,000 thousand consumers or more. This legislation enhances New York's privacy and data protection framework, which has continued to expand, including a new Electronic Monitoring Law that was signed into law in November 2021. Privacy and Competition Policy Coordination Agreement signed between Philippine Regulatory Agencies On February 9, 2022, the Philippine Competition Commission (PCC) and the National Privacy Commission (NPC) signed a memorandum of agreement (MOA) to strengthen enforcement and policy coordination on privacy and competition issues in the digital economy. The MOA will enable investigation and enforcement support between the PCC and the NPC by forming joint task forces, cultivating two-way notification of matters of mutual concern, and enabling direct consultations between the regulators in drafting and implementing policies within the competition and privacy fields. New Google Antitrust Complaint in Europe On February 11, 2022, a new antitrust complaint was filed against Google with the European Commission for its practices in the digital advertising sector. In its complaint, the European Publishers Council (EPC) claims that Google has abused its dominant market position across different stages in the adtech chain by foreclosing competition in ad tech. EPC is the European publishers' trade body, including members such as News UK, Conde Nast, Bonnier News, and Editorial Prensa Iberica. Google has been fined more than 8 billion euros (9.2 billion dollars) in recent years for anti-competitive practices between three cases. Last year, EU Competition Commissioner Margrethe Vestager launched an investigation into Google's digital advertising business, which the EPC claims its complaint potentially strengthens. Amazon Fined 2 Million Euros for GDPR violations in Spain On February 11, 2022, the Spanish data protection authority (AEPD) fined Amazon Road Transport Spain 2 million euros for its illegal processing of personal data in violation of the EU General Data Protection Regulation (GDPR). The AEPD launched an investigation into Amazon Road Transport Spain (ARTS) following a complaint filed by the General Union of Workers alleging that ARTS had requested that a self-employed contractor provide it with a certificate of no criminal record, which would then be shared with group companies and their supplier located outside the European Economic Area. The AEPD found this conduct to be in breach of the GDRP and in addition to fining ARTS, it also ordered it to delete all the information provided in the certificates already collected, and to adapt its data processing in accordance with the requirements of Article 6(1) of the GDPR. The decision, only available in Spanish, can be viewed here. French, Austrian Data Protection Authorities Find that Google Analytics US Data Transfer Violates GDPR On February 10, 2022, the French Data Protection Authority (CNIL) held that data collected by Google Analytics that was transferred to the U.S. violates the GDPR, in line with the previously issued Google Analytics decision in Austria.One part of Google Analytics' data collection process, which was found by the CNIL to be in breach of the GDPR, entails the assignment of a unique identifier to each visitor of a webpage, which is then transferred by Google to the U.S.. The CNIL's finding is consistent with that of the Austrian Data Protection Authority in its decision on Google Analytics that IP addresses and cookie data constitute personal data under the GDPR.Lessons from GDPR for AI Policymaking GDPR & AI Policy TPRC Josephine Wolff Fletcher School Tufts University josephine.wolff@tufts.edu William Lehr CSAIL MIT wlehr@mit.edu Christopher Yoo Law School University of Pennsylvania csyoo@law.upenn.edu Abstract The ChatGPT chatbot has not just caught the public imagination; it is also amplifying concern across industry, academia, and government policymakers interested in the regulation of Artificial Intelligence (AI) about how to understand the risks and threats associated with AI applications. Following the release of ChatGPT, some EU regulators proposed changes to the EU AI Act to classify AI systems like ChatGPT that generate complex texts without any human oversight as "high-risk" AI systems that would fall under the law's requirements. That classification was a controversial one, with other regulators arguing that technologies like ChatGPT, which merely generate text, are "not risky at all." This controversy risks disrupting coherent discussion and progress toward formulating sound AI regulations for Large Language Models (LLMs), AI, or ICTs more generally. It remains unclear where ChatGPT fits within AI and where AI fits within the larger context of digital policy and the regulation of ICTs in spite of nascent efforts by OECD.AI and the EU. This paper aims to address two research questions around AI policy: (1) How are LLMs like ChatGPT shifting the policy discussions around AI regulations? (2) What lessons can regulators learn from the EU's General Data Protection Regulation (GDPR) and other data protection policymaking efforts that can be applied to AI policymaking? The first part of the paper addresses the question of how ChatGPT and other LLMs have changed the policy discourse in the EU and other regions around regulating AI and what the broader implications for these shifts may be for AI regulation more widely. This section reviews the existing proposal for an EU AI Act and its accompanying classification of high-risk AI systems, considers the changes prompted by the release of ChatGPT and examines how LLMs appear to have altered policymakers' conceptions of the risks presented by AI. Finally, we present a framework for understanding how the security and safety risks posed by LLMs fit within the larger context of risks presented by AI and current efforts to formulate a regulatory framework for AI. The second part of the paper considers the similarities and differences between the proposed AI Act and GDPR in terms of (1) organizations being regulated, or scope, (2) reliance on organizations' self-assessment of potential risks, or degree of self-regulation, (3) penalties, and (4) technical knowledge required for effective enforcement, or complexity. For each of these areas, we consider how regulators scoped or implemented GDPR to make it manageable, enforceable, meaningful, and consistent across a wide range of organizations handling many different kinds of data as well as the extent to which they were successful in doing so. We then examine different Page 1 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC ways in which those same approaches may or may not be applicable to the AI Act and the ways in which AI may prove more difficult to regulate than issues of data protection and privacy covered by GDPR. We also look at the ways in which AI may make it more difficult to enforce and comply with GDPR since the continued evolution of AI technologies may create cybersecurity tools and threats that will impact the efficacy of GDPR and privacy policies. This section argues that the extent to which the proposed AI Act relies on self-regulation and the technical complexity of enforcement are likely to pose significant challenges to enforcement based on the implementation of the most technologically and self-regulation-focused elements of GDPR. 1. Introduction In February 2023, Brando Benifei and Dragoș Tudorache, the two members of the European Parliament who serve as co-rapporteurs for the proposed EU Artificial Intelligence Act, reportedly suggested a series of amendments to the list of "high-risk" AI applications that the Act would cover.1 The initial draft released in 2021 had designated several AI systems as high risk, including those used for recruitment and hiring and those to determine eligibility for public assistance benefits and services, to provide law enforcement with help assessing the risk that someone might break the law, or to help courts research and interpret the law.The 2023 amendments suggested classifying several additional types of AI systems as high risk, including those "likely to influence democratic processes like elections," those that "may have serious effects on a child's personal development," and generative AI systems, such as ChatGPT.2 It was a stark shift in tone from mentions of generative AI in the earlier 2021 proposal of the draft law, which had largely focused on deepfakes and specified, "For some specific AI systems, only minimum transparency obligations are proposed, in particular when chatbots or 'deep fakes' are used."3 Tudorache himself had told Reuters in early 2023 that he believed generative AI was "not going to be covered" in the AI Act in depth, saying, "That's another discussion I don't think we are going to deal with in this text."4 The speed with which the release of ChatGPT in November 2022 appeared to change European regulators' perceptions of the risks associated with generative AI and large language models (LLMs) and the need to regulate those systems hints at just how malleable and undecided 1 Luca Bertuzzi, "AI Act: EU Parliament's Crunch Time on High-Risk Categorisation, Prohibited Practices," https://www.euractiv.com/section/artificial- intelligence/news/ai-act-eu-parliaments-crunch-time-on-high-risk-categorisation-prohibited-practices/. EURACTIV, February 2023, 2 Ophélie Stockhem and Asha Allen, "CDT Europe's AI Bulletin: February 2023," CDT, February 23, 2023, https://cdt.org/insights/cdt-europes-ai-bulletin-february-2023/. 3 "Proposal for a Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts," April https://eur-lex.europa.eu/resource.html?uri=cellar:e0649735-a372-11eb-9585- 01aa75ed71a1.0001.02/DOC_1&format=PDF. 2021, 21, 4 Martin Coulter and Supantha Mukherjee, "Exclusive: Behind EU Lawmakers' Challenge to Rein in ChatGPT and Generative AI," Reuters, May 1, 2023, https://www.reuters.com/technology/behind-eu- lawmakers-challenge-rein-chatgpt-generative-ai-2023-04-28/. Page 2 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC regulators' understandings of the risks associated with different types of AI are. It also highlights the challenges associated with drafting a risk-based regulation for a technology whose risks are still relatively poorly understood. Over the course of less than a year, the release of ChatGPT led to significant changes in the draft AI Act that were specifically designed to respond to concerns about a type of technology that most European regulators had previously not viewed as high risk. It is not unusual or even necessarily surprising that a popular new technology should influence regulation or interest regulators in new risks with which they had not previously been concerned. However, the regulatory impact of ChatGPT in Europe is an instructive example of how policymakers around the world struggle with designing regulations intended to counter emerging technological risks related to AI systems that can be used for a variety of different purposes, some of which may be high risk and others of which may not. This examines two questions: (1) how has the global concern over ChatGPT and LLMs shifted the discussion around Artificial Intelligence (AI) regulation, and (2) what might earlier experiences with GDPR teach us about crafting policies aimed at emerging technological risks? More broadly, this paper considers the question of how we should adapt our regulatory institutions in response to the transformative potential posed by the transition to a digital economy. This is not a new question. Information technologies have already significantly transformed how many tasks are performed by partial or full-scale automation. The process of augmenting human physical and cognitive tasks with IT and machine assistance has been ongoing for a long time, but the release of ChatGPT in November 2022 confronted the world with significant progress in AI technologies. Much of the concern and fear is that this transformation might leave humans on the sidelines as super-capable, super-intelligent AIs take over the world.5 Regardless of what one thinks of the likelihood of such an outcome, it is certainly timely that policymakers consider what role regulatory institutions should play in the digital future and with respect to AI. Unfortunately, the sudden amplification of sensationalist attention from all quarters prompted by the introduction of ChatGPT to the general public may not offer the right stimulus for sound policy progress. This essay is a collaborative effort between an economist, a lawyer, and a cybersecurity scholar who have been engaged in digital transformation-related research and policy analysis for several decades.Although we have each been engaged peripherally in matters related to AI, our principal 5 Two quotes from Weizenbaum (1976) and Bostrom (2014) are apt. "There are certain tasks which computers ought not be made to do, independent of whether computers can be made to do them" (Weizenbaum, 1976) and "If some day we build machine brains that surpass human brains in general intelligence, then this new superintelligence could become very powerful. And, as the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species would depend on the actions of the machine superintelligence" (Bostrom, 2014). Joseph Weizenbaum, an MIT computer scientist and earlier pioneer in Natural Language Processing (NLP), developed ELIZA in the mid-1960s as an early example of a chatbot program and precursor to ChatGPT. After becoming alarmed when psychiatrists touted the potential for more advanced versions of ELIZA serving as automated therapists, Professor Weizenbaum emerged as one of the leading critics of AI. His concerns were not because of the limitations in what AI might be able to automate, but because of the social-economic harms that such automation might bring if unrestricted. The British philosopher Nicholas Bostrom opens his book about the potential and implications of an AI superintelligence by suggesting that AI may one day replace humans as the arbiters of humanity's future. Page 3 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC research and professional activities have not been narrowly focused on AI technologies or its regulation. Herein we take a broader view to offer our opinions and hypothesis relating to two themes: (1) the impact of ChatGPT on regulatory efforts related to AI in the EU, 6 and (2) the legacy of GDPR, the European Global Data Protection Regulation that was passed in 2016 and became effective in 20187. The motivation for this effort was prompted by the global reaction that followed the release of ChatGPT, including calls from many leading experts across academia, industry, and governments raising concerns about the risks posed by the new technology and LLMs more generally. We agree that AI poses a significant regulatory challenge that merits investigation. However, we see in the current furor serious risk of harm to sound digital policymaking. It might induce hasty regulatory efforts resulting in misdirected yet burdensome interventions that will raise costs and slow progress without effectively addressing the perceived risks. Alternatively, the furor may damage nascent sound policymaking efforts by stoking paranoia and the attention burnout that excessively overhyped risks or benefits can lead to. We do not doubt that AI comprises a host of important technologies that are likely to be beneficial and essential for our digital future. We also recognize that AI's use may further exacerbate global challenges, such as income disparities, cybercrime, and climate change. AI is a tool, and its welfare implications depend on how it is used. It is certainly possible that our collective failure to manage how AI is used will pose an existential threat to humanity. However, ChatGPT is hardly exemplary of that threat or indicative of world domination by a super-intelligent AI any time soon. Thus, it is worth distinguishing what ChatGPT does tell us about AI and its regulatory relevance. It is also worth considering the legacy of GDPR, which, as a major piece of digital policymaking, is likely to influence the direction of AI policymaking. To extract those lessons, it is useful to compare and contrast the challenges that GDPR and AI seek to address. First, it will be important to recognize that AI's need for and use of data meant that it will necessarily be heavily dependent on GDPR, which is a keystone of the European (and hence, global) data management regulatory framework. Second, the challenges addressed by AI regulation are much more complex, broader, and more uncertain than those tackled by GDPR, and efforts to advance AI regulation are likely to have significant implications for the future of GDPR. Moreover, it is worth considering the extent to which AI regulation is being influenced by individual technologies, with ChatGPT serving as a prime example of AI technology, but AI and the regulatory challenges it poses for societies and economies go well beyond ChatGPT.It is possible that in the near term, specialized rules for LLMs 6 EC (2021), Proposal of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts, European Commission (EC), COM(2021) 206 Final, Brussels, 21.4.2021, available at https://eur-lex.europa.eu/legal- content/EN/TXT/?uri=CELEX:52021PC0206, hereafter, "AI Act." See also "Europe moves ahead on AI tech giants' power," Washington Post, June 14, 2023, available at Regulation, challenging https://www.washingtonpost.com/technology/2023/06/14/eu-parliament-approves-ai-act/. 7 GDPR (2016), Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation), available at https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=uriserv:OJ.L_.2016.119.01.0001.01.ENG, hereafter "GDPR" (and elsewhere may be referred to as Reg 2016/679). Page 4 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC and other generative AI technologies may be adopted, but it is worth considering the broader implications of crafting general AI regulations specifically to address these technologies. 2. ChatGPT's Impact on the EU AI Act On June 14, 2023, the European Parliament approved a draft version of the AI Act that included several amendments to the initial text proposed in 2021. While the proposal to classify generative AI systems as high risk did not make it into the final approved amendments, several other changes seemingly designed to target issues raised by ChatGPT were approved. This section considers those changes and the impact ChatGPT had on the AI Act overall. Negotiations over the final text of AI Act are ongoing, so the language discussed here may not be the final version passed into law in the EU. However, as the first set of changes made following the release of ChatGPT, it offers a vivid picture of the early reactions to that technology among EU regulators. 2.1 Background on ChatGPT Open.AI launched ChatGPT in November 2022. ChatGPT is a Large Language Model (LLM), which is a class of AI technologies that are based on deep-learning technology that is a further development of Machine Learning (ML) technology. It may be accessed on the web8 either as a free or fee-based service and generates text in response to user inputs. What makes ChatGPT so immediately impressive is that users with no experience with AI can often obtain quite coherent and informative responses to quite complex questions, such as a request for a simple explanation of a quantum computing or an interpretation of a poem. ChatGPT can provide answers tailored to mimic the style of an author with an emotional tone. Users can also ask ChatGPT to regenerate a response and it will come forth with a new and often equally useful but different response, and because ChatGPT keeps track of earlier inputs,9 ChatGPT can generate contextually relevant conversations. Those conversations can highlight both the capabilities and limitations of ChatGPT. For example, a New York Times reporter exploring ChatGPT's capabilities reported his conversation that led to ChatGPT declaring its love for the reporter.10 The fact that the ChatGPT could appear to express 8 See https://platform.openai.com/apps (accessed 7/7/23), which provides access to three Open.AI apps, ChatGPT (language conversational interface), Dall-E (image generating interface), and API (for integrating OpenAI into other business software applications). 9 It is also worth noting that ChatGPT uses its interactions with users to build its knowledge base and refine its answers. When ChatGPT generates or regenerates a response, it gives users an opportunity to comment on the quality of the response. This information can be collected from all users to allow the model to "learn" - in the sense that it will be able to provide answers that are perceived to be better by more users which may or may not be the relevant standard for objectively measuring whether the answers are, indeed, better. 10 See Roose, K. (2023), "A Conversation with Bing's ChatBot left me deeply unsettled," New York Times, February 16, 2023, available at https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft- chatgpt.html (visited 7/7/23). Page 5 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC emotions might seem to many like an eerily human behavior not generally thought feasible by a machine. Of course, the article (and others like it) can highlight how easily identify such behavior as a poor simulacrum of an actual human emotional interaction.Additionally, the knowledge data set that was used to train the ChatGPT model was cut off as of September 2021, so it cannot give responses that depend on knowledge or events that occurred after that time. That highlights a fundamental limitation of ML models-they are only as good as their training data sets. If the world changes and renders their training data no longer relevant for the new context, then an ML can give predictions to which it might ascribe high precision even though those predictions might be completely wrong. Additionally, AI works on patterns of text observed in its training data set, but there is no guarantee that the responses it generates will be true. For example, when asked to list papers written by a particular person, it may list papers that do not exist and were not written by the author even though they sound like papers that might have been written by the author. That is, ChatGPT is capable of creating new information based on the collection of information on which it has been trained in the past, including the history of the conversation with the user. Additionally, ChatGPT is programmed to follow "ethical guidelines," so it will not directly offer textual responses that could be used to defame, libel, or otherwise be deemed unlawful. Open.AI was launched as a non-profit in December 2015 with collaboration from many leading high-tech entrepreneurs and investors, such as Elon Musk, Sam Altman, and Peter Thiel, who collectively pledged over $1 billion to develop open software tools and platforms for AI or more specifically for Artificial General Intelligence (AGI). However, Open.AI became a for-profit company in 2019 and realized a $10 billion valuation as of January 2023 when Microsoft significantly increased its investment stake in the company.11 AGI is sometimes distinguished as being a distinct subset of AI technologies, most of which are focused on narrow, specific problems ranging from decision support systems (e.g., expert systems that seek to mimic the decisionmaking capabilities of domain experts such as interpreting diagnostic data for cardiac illness, analyzing natural resource monitoring data, etc.); robotic systems; Natural Language Processing (NLP); or Computer Vision (CV). More recently, significant progress has been made in the development of Machine Learning (ML) systems that use large data samples to develop predictive models in specific problem domains. ML systems have evolved from those based on structured data to ones capable of extracting insights from unstructured data, making use of neural net technologies and deep-learning algorithms. The nuanced differences in the underlying AI technologies have relevance for their regulatory implications. The LLM that ran ChatGPT when it was released in November 2022 was GPT-3, 11 Elon Musk left the project in 2018, citing potential conflicts arising from his AI efforts associated with his other ventures. Microsoft committed $1 billion to Open.AI in 2019, obtained an exclusive license to GPT-3 to run on Microsoft's Azure cloud computing infrastructure, which is the LLM engine on which ChatGPT runs, and then announced its plan to invest an additional $10 billion in January 2023 (see "Microsoft invested billions in ChatGPT maker OpenAI - So, who owns it?, May 15, 2023, available at https://marketrealist.com/tech-comm-services/who-owns-open-ai/, visited 7/7/2023). Page 6 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC which was the third generation of a software program that was first launched as GPT-1 in 2018.12 That program was trained on an initial data set of web pages, books, and other textual material. Successive versions of GPT were trained on significantly larger and more diverse data sets spanning a wider breadth of textual source material. ChatGPT now runs on GPT-4, which was released in March 2023 and represents a significant expansion in the range of data and enhancements in the capabilities of the deep-learning ML system that is the engine that drives ChatGPT.13 What this demonstrates is the rapid pace of innovation that has characterized the evolution of the LLM AI, illustrating what appears to be an exponential acceleration in the capabilities of the software relative to the much slower pace of improvements that characterized automated BOT conversation tools in the past (e.g., those associated with automated call-response systems and other computer-generated response systems). We say "appears" because the real improvements are harder to assess, but a major concern with AGI is that it has the capability of self-improvement.2.2 The 2023 Amendments to the EU AI Act One of the major changes made to the draft EU AI Act in June 2023 that appeared to be driven largely by the arrival of ChatGPT was the addition of language defining "foundation models" and "general purpose AI systems." The amendments adopted in 2023 define a "foundation model" as "an AI system model that is trained on broad data at scale, is designed for generality of output, and can be adapted to a wide range of distinctive tasks" and defines a "general purpose AI system" as "an AI system that can be used in and adapted to a wide range of applications for which it was not intentionally and specifically designed."14 Both of these definitions seem aimed, at least to some extent, at trying to capture the general use nature of programs like ChatGPT, which can be applied in a wide variety of different contexts. This is of particular importance for a regulation like the AI 12 GPT is an acronym coined by OpenAI which stands for Generative Pre-trained Transformer. The GPT works by feeding a massive amount of training data into a neural net that takes inputs and transforms them into outputs. The neural net is a computational algorithm that may be modeled a set of connected nodes with different weights. The weights are adjusted so that the trained data set of sampled task inputs and associated outputs are matched. Once trained, the neural net can be used to predict the output from previously unseen, new input data. For example, given a large data set of insurance applications with a large collection of data about the characteristics of the applicants and the type of coverage they selected, and the results of those policies, a neural net can be trained to predict the likely default risk for a new candidate with characteristics of the sort included in the training data set. Or, the input data could be medical imaging data and doctor diagnoses of tumors. A neural net trained on such data can analyze new images and diagnose tumors, potentially with better accuracy and faster than human doctors. 13 See Ali, F. (2023), "GPT-1 to GPT-4: Each of OpenAI's GPT Models Explained and Compared," Make Use Of Newsletter, April 11, 2023, available at https://www.makeuseof.com/gpt-models-explained-and- compared/ (visited 7/7/2023). 14 "Amendments Adopted by the European Parliament on 14 June 2023 on the Proposal for a Regulation of the European Parliament and of the Council on Laying down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts (COM(2021)0206 - C9- 0146/2021 - 2021/0106(COD))1," June 2023, https://www.europarl.europa.eu/doceo/document/TA-9- 2023-0236_EN.pdf. Page 7 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC Act, in which all of the risk tiers for AI systems are predicated on the application areas of those systems. A program like ChatGPT that can be applied in a variety of different contexts could be difficult to classify according to a specific application. One possible option for dealing with this ambiguity about ChatGPT's risk level-one that was apparently considered by EU regulators-was explicitly classifying generative AI models as high- risk systems. Instead of doing that, however, the European Parliament ultimately chose to create two new categories of AI systems-foundation models and general purpose AI systems-that are neither clearly entirely included nor entirely excluded from the high-risk designation due to the generality of their applications. Indeed, both definitions emphasize the "generality" of foundation models and general purpose AI systems. The 2023 text of the draft law also highlights the range of applications for these types of AI systems, with Amendment 99 noting that foundation models are "designed to optimize for generality and versatility of output" and are "often trained on a broad range of data sources and large amounts of data to accomplish a wide range of downstream tasks, including some for which they were not specifically developed and trained." Moreover, the amended text states, "each foundation model can be reused in countless downstream AI or general purpose AI systems." Clearly, one of the elements of ChatGPT that most confounded and complicated the regulatory issues for EU regulators was the idea that it could be used in so many different ways by so many different systems. This range of applications meant that it was difficult for regulators to classify ChatGPT and other LLMs either as high risk or not: In some applications they might, indeed, be used for high-risk purposes, but in other contexts, they might be generating text for entirely benign purposes.The balance EU regulators struck in the amended draft bill approved in 2023 was essentially to make the owners and operators of these programs responsible for risk mitigation across all of the application areas for which their services were used unless they are willing to pass on all of their source code and information about how their model was trained. More specifically, Amendment 100 of the approved draft, states: In the case of foundation models provided as a service such as through API access, the cooperation with downstream providers should extend throughout the time during which that service is provided and supported, in order to enable appropriate risk mitigation, unless the provider of the foundation model transfers the training model as well as extensive and appropriate information on the datasets and the development process of the system or restricts the service, such as the API access, in such a way that the downstream provider is able to fully comply with this Regulation without further support from the original provider of the foundation model.15 This approach essentially offers developers of LLMs and other "foundation models" the choice between giving users direct access not just to the outputs of their model but also to its algorithms and training processes or else bearing responsibility for "appropriate risk mitigation" for any 15 Ibid Page 8 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC "downstream provider" to which it grants access. The former approach is unlikely to be feasible for most companies since their entire business model often derives from having developed proprietary AI systems. Indeed, part of the point of granting access to those systems through APIs rather than selling the system code itself is often to avoid sharing the code base and inner workings of the AI system while still being able to sell its results. But the latter option-assuming responsibility for risk mitigation for all downstream providers-also places a potentially heavy burden on AI companies, requiring them to monitor everyone who uses their services and build in appropriate safeguards as needed, depending on the application areas those users are working in. Such an approach would make it difficult for an AI company to open access to their APIs freely to any interested user for fear that they might not be able to provide appropriate risk mitigation services to everyone who took advantage of their services. It is not entirely clear what it would mean for a company to try to take the third option presented by restricting their services and API access "in such a way that the downstream provider is able to fully comply with this Regulation without further support from the original provider of the foundation model," but it would presumably require users to have some access to the model's testing and auditing features in order to comply with the requirements of the AI Act around risk assessment and mitigation. Amendment 101 of the approved 2023 AI Act text offers more specific clues as to what kinds of things the owners and operators of foundation models must do to comply with the law.In particular, it states that organizations developing foundation models "should assess and mitigate possible risks and harms through appropriate design, testing and analysis, should implement data governance measures, including assessment of biases, and should comply with technical design requirements to ensure appropriate levels of performance, predictability, interpretability, corrigibility, safety and cybersecurity and should comply with environmental standards." In many ways, these obligations mirror the ones that the Act requires of the owners and operators of high- risk AI systems, including the specified obligations for foundation model operators to "prepare all necessary technical documentation for potential downstream providers to be able to comply with their obligations under this Regulation." There is also an explicit transparency requirement for generative foundation models that they include some notification or label about "the fact the content is generated by an AI system, not by humans." Documentation and transparency are central to the requirements the Act puts in place for designated high-risk AI systems, but the amendment goes to great lengths to distinguish between those requirements and the ones for foundation models, stating: "These specific requirements and obligations do not amount to considering foundation models as high risk AI systems, but should guarantee that the objectives of this Regulation to ensure a high level of protection of fundamental rights, health and safety, environment, democracy and rule of law are achieved."16 The tension in the EU's compromise over LLMs is evident here: the desire to apply many of the same safeguards to technologies like ChatGPT that are required of high-risk AI systems, while at the same time insisting that doing so does not constitute classifying those technologies as high risk. This tension arises from the generality of these models and the inability to clearly classify all of their uses and applications as either high risk or low risk. Helberger and Diakopoulos have argued that the necessity of classifying AI systems according to their level of risk makes the AI Act's framework ill-suited to regulating generative AI systems like ChatGPT both because it is 16 Ibid. Page 9 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC not feasible to sort such systems into high/no high risk categories and also because it is too difficult to predict their future risks. Writing before the June 2023 approval of the new draft law, they argued that under the 2021 draft, the regulations for high-risk systems might "only take effect once the generative AI is being used in a high-risk area." They continue: From the point of view of society and fundamental rights, this is too late. The whole point about generative AI as a general-purpose AI system is that because they can be used for so many different purposes, it is paramount to incentivise the providers of systems to think about the safety of these systems from the onset, starting with the difficult question of data quality.17 For this reason, they advocate creating a new category of general purpose AI systems in the Act and regulating those separately from the high-risk AI systems. To a large extent, Helberger and Diakopoulos seem to get what they want from the 2023 revision of the AI Act, in particular their recommendation that general purpose AI systems should be "considered a general-risk category in their own right" rather than being categorized under the existing high-risk classification. What is a little bit perplexing about this approach, however, is that it is not clear from the draft law how exactly the requirements that apply to foundation models differ from those that apply to high-risk applications beyond being somewhat vaguer and slightly more indirect, because operators of foundation models are responsible for risk mitigation of their users' applications. Interestingly, the draft law's justification for treating foundation models in this special manner is couched not just in terms of the models' general applicability but also their "unpredictability." The approved draft states: Pre-trained models developed for a narrower, less general, more limited set of applications that cannot be adapted for a wide range of tasks such as simple multi- purpose AI systems should not be considered foundation models for the purposes of this Regulation, because of their greater interpretability which makes their behaviour less unpredictable. This notion that it is not just the generality of certain AI tools but also their lack of "interpretability" that makes them difficult to regulate in the same manner as more narrowly applied systems is notable because it seems to speak to the complexity and size of the data sets used to train these models rather than the broadness of their range of applications.The implication seems to be that the large training data sets and high degree of complexity of these systems are somehow intrinsically linked to their generality and more significant than the complexity or size of more narrowly trained models. There is some justification for this perspective. Indeed, whereas earlier ML models and narrow AI systems worked with structured datasets, the current frontier of deep-learning ML models that are core to AGI can work with unstructured data. Structured data is data that is collected and organized with a particular purpose in mind. For example, a structured image database might be a collection 17 Natali Helberger and Nicholas Diakopoulos, "ChatGPT and the AI Act," Internet Policy Review 12, no. 1 (February 16, 2023), https://policyreview.info/essay/chatgpt-and-ai-act. Page 10 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC of pictures of trees with metadata labels that identify the tree and provide other salient information about the image. Then an ML can use such training data to develop its prediction capabilities, which, once trained, would allow the ML to identify a tree from an unlabeled tree picture. Algorithms that can work with unstructured data could take in a wealth of image data and learn from those images how to identify trees without be presented with the metadata that comes with structured data sets. This greatly expands the sorts of data of which the ML can make use but can also make it much more difficult to determine how particular training data impacts the performance or predictions of the ML or to understand or explain the reasoning (in human intelligible form) the reasoning that leads to the ML's forecast. Explaining the behavior of neural nets with multiple layers is difficult in all contexts, but explaining their behavior when applied to unstructured data is even more difficult. Helberger and Diakopoulos have argued that the AI Act is ill-suited to regulating generative AI models like ChatGPT because of those models' "dynamic context and scale of use."18 They link the broad applicability of generative AI models to the scale of those models' training data, writing: Generative AI systems are not built for a specific context or conditions of use, and their openness and ease of control allow for unprecedented scale of use. The output of generative AI systems can be interpreted as media (text, audio, video) by people with ordinary communication skills, lowering, therefore, significantly the threshold of who can be a user. And they can be used for such a variety of reasons to some extent because of the sheer scale of extraction of data that went into their training.19 Still, in the context of the AI Act's justification for singling out foundation models for special treatment, it is not immediately clear why broadly applicable AI systems should always necessarily be trained on more data or be more complicated or less predictable than more narrowly targeted ones. In fact, this justification seems to suggest that in carving out special requirements for foundation models, the European Parliament has actually conflated two separate characteristics of ChatGPT-its broad applicability and its complexity-that need not always be linked. A simple AI model could apply in a range of different contexts, while an extremely complicated one trained on large amounts of data might be designed for only a very narrow application but still be difficult to interpret or predict. EU regulators seem to have deliberately avoided splitting these two characteristics in its AI Act text, where foundation models are defined as being both "trained on broad data at scale" and "designed for generality of output." The definition of general purpose AI, on the other hand, makes no reference to training data or size and merely depends on a system being able to "be used in and adapted to a wide range of applications for which it was not intentionally and specifically designed." This conflation undermines the logic of the special foundation model-specific provisions in the draft law, suggesting that they are necessary not because of how general they are but because of how complex they are, when, in fact, it seems more likely that the reverse is true since the existing provisions for high-risk AI systems can apply regardless of system complexity 18 Ibid. 19 Ibid. Page 11 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC and size. Surprisingly, however, the provisions in the AI Act call out the operators of foundation models, not general purpose AI models, for special requirements and responsibilities, suggesting that regulators care more about the size and complexity of such systems than their broad range of applications.One possible explanation for this is provided in Amendment 102, which highlights concerns about the training data used for "generative AI systems" based on foundation models and the potential for such systems to violate copyright laws by exploiting publicly available, copyrighted text. These concerns about protecting copyright and reining in training data may be one reason that regulators have sought to couch oversight of foundation models in terms of the size and complexity of their training data sets rather than just their broad applicability. In practice, however, it seems likely that these concerns will be largely sidestepped. Requiring models like ChatGPT that were built on copyrighted text to be abandoned, or requiring their developers to incur the costs of acquiring consent from individuals seems largely unrealistic. 3. Reconciling General Purpose AI with GDPR Because AI systems depend on the ability to access and use lots of data for a variety of different purposes, the EU's General Data Protection Regulation (GDPR), in addition to the proposed AI Act, has significant implications for their development. Moreover, the shift in the AI Act's scope to encompass more general AI has the potential to raise even more significant GDPR-related concerns because so much of the focus of GDPR requirements is on establishing a purpose for the collection of data and notifying users of that purpose at the time of collection as well as minimizing collected data. This emphasis on identifying the purpose of collected data prior to its collection and data minimization raised concerns about GDPR's impacts on AI long before the release of the draft AI Act or ChatGPT. For instance, writing in 2017, following the passage of GDPR, Zarsky wrote that the law was "incompatible with the data environment that the availability of big data generates."20 In an analysis focused more specifically on the compatibility of GDPR and AI models, Kesa and Kerikmäe argued that "it is not possible to practice complete compliance with the requirement to ensure certain rights as they are guaranteed by GDPR, especially by data processors employing complex machine-learning systems that process vast amounts of data through complex multistep sets of instructions."21 Other analyses, such as one carried out by the Scientific Foresight Unit of the European Parliamentary Research Service, have concluded that while GDPR may not be incompatible with the development of AI, it "does not provide sufficient guidance for controllers" and it may need to be "expanded and concretised" to offer clearer explanations of how its 20 Tal Zarsky, "Incompatible: The GDPR in the Age of Big Data," Seton Hall Law Review 47, no. 4 (2017): 995-1020. 21 Alexandr Kesa and Tanel Kerikmäe, "Artificial Intelligence and the GDPR: Inevitable Nemeses?," TalTech Journal of European Studies 10, no. 3 (December 2020): 68-90. Page 12 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC provisions apply to AI systems.22 Yet another analysis posits that GDPR "may eventually help create the trust that is necessary for AI acceptance by consumers and governments."23 The recent responses of EU data protection authorities (DPAs) to ChatGPT eloquently illustrate the difficulties that general purpose AI faces in complying with GDPR. For example, on March 31, 2023, the Italian data protection authority issued a temporary emergency decision banning ChatGPT based on four potential GDPR violations. The DPAs from France, Spain, Ireland, and Germany have begun taking preliminary steps of their own, and the European Data Protection Bureau has launched a task force to foster cooperation among Member States in their potential responses to ChatGPT. Although changes made by OpenAI led the Italian DPA to permit service to resume on April 28, those changes did not appear to address all of the concerns raised in the initial ban. The purpose of this section is not to reiterate all of the ways in which GDPR's provisions may hinder (or support) the development of AI systems but rather to consider the specific ways in which general purpose AI-or the "foundation models" defined by the draft AI Act-may be especially difficult to reconcile with GDPR by virtue of its generality. Additionally, we aim to evaluate how the different structures and framings of GDPR and the AI Act influence their respective enforceability and what lessons, if any, may be drawn from GDPR in designing the AI Act. 3.1 Background on GDPR The General Data Protection Regulation (GDPR) is the EU law designed to provide a framework for protecting personal data online.24 It was enacted in 2016 and became law in 2018. It is part of the EU privacy and human rights law. Work on drafting GDPR began in 2011, and when passed, GDPR superseded the EU's 1995 data privacy protection directive.25 The basic goal of the framework remained the same, namely, to protect the privacy of individual data online.The 1995 directive in turn had been crafted from the 1980 OECD Recommended guidelines for privacy protection, which were updated in 2013 and based on 7 key principles: (1) Notice: individuals should be notified when their personal data collected; (2) Minimal & Consensual: collection should be with consent and limited; (3) Purpose: Data collection should be relevant, up to date, for specific purpose; (4) Shared: Not disclosed without individual consent unless for legal reasons; (5) Protected: security in place. (Privacy by design, by default); (6) Transparency: Individuals have 22 European Parliamentary Research Service, "The Impact of the General Data Protection Regulation (GDPR) 2020, https://www.europarl.europa.eu/RegData/etudes/STUD/2020/641530/EPRS_STU(2020)641530_EN.pdf. Intelligence," Artificial June 23 Kalliopi Spyridaki, "GDPR and AI: Friends, Foes or Something in Between?" (SAS Insights, n.d.), https://www.sas.com/en_us/insights/articles/data-management/gdpr-and-ai--friends--foes-or-something- in-between-.html. 24 See GDPR, Note 7 supra. 25 See EU Directive 95/46/EC Protection of Personal Data (European Data Protection Directive), adopted October 1995, available at https://eur-lex.europa.eu/legal-content/EN/LSU/?uri=celex:31995L0046. Page 13 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC right to know what data is collected about them; and (7) Controllers of data accountable for complying with law.26 GDPR, however, represented a significant expansion in capabilities and requirements to provide a framework for stronger enforcement of online privacy. The intent of GDPR is to enhance individuals' control and rights over their personal data and to simplify the regulatory environment for international business by replacing what previously was a mishmash of different national and sector-specific privacy frameworks with a homogeneous framework applicable across all EU member states and data contexts where individually identifiable data may be used. It applied to all firms and so its coverage and scope were broad. GDPR expanded individual privacy rights by including the right to be forgotten, the right to correct on-line data (including deleting it from on-line files), portability rights (e.g., to enable end users to move their data to another business), and expanded transparency obligations to enable individuals to learn what their personal data is being used for, potentially by third parties. While it is conceptually clear how an firm might identify and share with an individual all of the personal data associated with that individual so long as that data is structured (e.g., stored in database records that can be linked to the individual), it is unclear how these rules might be implemented for a business using unstructured data. GDPR sought to enshrine privacy-by-design and privacy-by-default in business practices. It includes a number of special features, such as requiring Data Protection Impact Assessments (DPIA), which may be requested to determine whether data is being managed consistent with GDPR and to provide guidance to regulatory decisionmaking, including the adjudication of fines. DPAs were empowered in each EU member state to monitor and enforce compliance with GDPR. The potential fines that can be levied under the GDRP are significant, running as high as the larger of 20 million Euros for a violation or up to 4% of the annual global revenue for a violator. As of June 2023, fines totaling over 4 billion Euros had been assessed against firms for violating GDPR, with 86% of those fines directed against Meta (Facebook, WhatsApp), Google, and Amazon.27 Although these numbers are sizable, the compliance costs incurred globally (by EU businesses and businesses that share data with EU businesses, which includes most multinational enterprises), is likely several multiples higher. However, it is debatable how much of this additional spending may be sound cybersecurity investment as opposed to excess investment, incurred incrementally due to GDPR. 26 See Brian Daigle and Mahnaz Khan, "The EU General Data Protection Regulation: An Analysis of Enforcement Trends by EU Data Protection Authorities," Journal of International Commerce and Economics (June 2020), https://www.usitc.gov/publications/332/journals/jice_gdpr_enforcement_0.pdf, for a history of GDPR and enforcement actions undertaken under the Act. See also "The History of the General Data Protection Regulation," provided by the European Data Protection Supervisor, available at https://edps.europa.eu/data-protection/data-protection/legislation/history-general-data-protection- regulation_en (visited 7/7/23). 27 See https://www.enforcementtracker.com/?insights, visited 6/28/23.Page 14 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC 3.2 A Risk-Based Approach Versus a Rights-Based Approach The AI Act and GDPR center on very different approaches to technological regulation. While GDPR takes a rights-based approach to regulating data protection by articulating specific rights that individuals are entitled to with regard to each transaction involving their data, the proposed AI Act frames its rules around a risk-based approach that imposes requirements on the owners and operators of AI systems according to the level of risks those systems pose. For high-risk systems, the AI Act imposes five requirements: (1) quality data and data governance; (2) transparency for users; (3) human oversight; (4) accuracy, robustness, and cybersecurity; and (5) traceability and auditability. The regulatory regime imposed by GDPR poses significant and perhaps insurmountable problems for general purpose AI. To be successful, AI models require vast quantities of training data. Although OpenAI has not fully disclosed the sources of the data used to train GPT-4, a 2023 technical report reveals that it was trained on "publicly available data (such as internet data) and data licensed from third-party providers."28 This representation suggests that GPT-4 is in serious conflict with the rights-based approach of GDPR, which envisions data subjects possessing the authority to control over specific type processing of any of their personal data. Under GDPR, what matters is whether the data is personal, not whether it is public or private. Consider Article 6's requirement that any processing of personal data falls within one of six legal justifications. Of these, three largely fall by the wayside with respect to training data: It is hard to characterize the use of data to train an AI model as necessary for compliance with one of the controller's legal obligations, necessary to protect a person's vital interests, or "necessary for the performance of a task carried out in the public interest or in the exercise official authority vested in the controller." A fourth authorizes processing that is "necessary for the legitimate interests pursued by the controller or a third party except where the interests are overridden by the interests or fundamental rights and freedom of the data subject." Recital 47 indicates that such a legitimate interest exists "where there is a relevant and appropriate relationship between the data subject and the controller . . . such as where the data subject is a client or in the service of the controller" and where "a data subject can reasonably expect at the time and in the context of the collection of the personal data that processing for that purpose may take place." Recital 69 emphasizes that data subjects whose data are processed under justification retain their right to object to the processing. The lack of a client relationship between the AI provider and the data subjects whose information is contained in the training data, the lack of expectation in most cases that the data subject's personal data would be used as training data, and the difficulties in giving people whose data was scraped off of the public Internet essentially render this fourth justification inapplicable to general purpose AI. The two remaining justifications require specific agreement of the data subject: consent or processing necessary for the performance of a contract, with GDPR further specifying that any consent by affirmative opt-in consent in writing of specific consent to each form of processing. 28 OpenAI, "GPT-4 Technical Report (Mar. 27, 2023). https://cdn.openai.com/papers/gpt-4.pdf. Page 15 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC Those collecting personal data must also disclose certain information about the data. and collect only the minimum amount necessary for the purposes of the processing. These provisions reveal the deep tension between GDPR and general purpose AI. The rights-based approach reflected in GDPR, which requires an agreement or written consent from and specific disclosures to every data subject whose personal data was included in the data used to train ChatGPT, is inconsistent with the Internet scraping used to generate the large amounts of training data on which general purpose AI depends. The fact that general purpose AI expends considerable effort to extract features from unstructured data means that people whose personal data was contained in public websites contained in the training data would have to have agreed to permit that type of feature extraction in order for it to comply with the specificity required to constitute valid consent.Nor is it likely that developers of general purpose AI provides people whose data are included in their training data with the other rights mandated by GDPR, including access, rectification, erasure, restriction of processing, data portability, withdrawal of consent, and the right to object. Should a data subject exercise one of these rights, the controller would have to expunge the impact of that data from the algorithm or delete the algorithm altogether. Moreover, the rights-based approach reflected in GDPR regulates data as an input into processing. whereas the risk-based approach reflected in the proposed AI Act focuses on the uses of the products of AI as outputs. Both of these framings make sense in the context of the motivations for these different regulations and the policy contexts in which they were developed, but neither is well-suited to the development of general purpose AI. For the AI Act, the central problem posed by general purpose AI is that it is nearly impossible to assess the risks associated with it in any meaningful way. For GDPR, the key problem is that it is nearly impossible to perform any meaningful purpose limitation (or data minimization) for data used to train general AI systems. It is hard to imagine how AI system operators could feel any real confidence that they were in compliance with either law if they wanted to develop a general AI system using European personal data. However, in terms of enforcement, GDPR appears to offer a much clearer path to imposing penalties on AI system operators than the AI Act. To show that an AI system operator is in violation of GDPR, a data protection authority need only demonstrate that personal information has been used to train an AI model without adequate lawful basis. It is much less clear what a regulator would have to demonstrate about a foundation model operator to show that they had failed to meet the bar of providing risk mitigation to their downstream providers. This reliance on self-regulation in the draft AI Act is perhaps the result of the complexity of the AI systems that the AI Act seeks to regulate-it may be that regulators have handed over responsibility for meeting their requirements to AI system owners and operators precisely because they fear they do not possess the requisite technological expertise to do so themselves. If this is in fact the case, then general AI systems and foundation models are likely to only exacerbate that problem. This speaks to a broader weakness in the AI Act, namely its reliance on AI companies to define their own risk mitigation, transparency, and explainability standards and documentation. This is true not just for foundation models but also all of the high-risk AI systems that the draft law proposes to regulate. All regulations, including GDPR, have some ambiguity and room for interpretation in their provisions, so it is perhaps unnecessary to assume that the AI Act law will suffer from a lack of clarity any more than any other piece of legislation. But from an enforcement Page 16 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698 GDPR & AI Policy TPRC standpoint, it is perhaps notable that many of the largest fines that European regulators have imposed under GDPR have been for incidents that were already punishable under the previous Data Protection Directive (namely data breaches) rather than privacy violations that only became violations under GDPR.29 That suggests that regulators may be slower to enforce new provisions in the AI Act than to rely on existing provisions, like those already enshrined in GDPR, with which they are already familiar. 4. Conclusion Regulators in the EU as well as other countries are still in the early stages of their efforts to frame policy framework for AI, but the impact of ChatGPT makes clear that these efforts can be easily and significantly influenced by the emergence of new, popular AI technologies. Interestingly, in the case of the AI Act, this impact resulted in the creation of two new regulatory categories of AI systems (foundation models and general purpose AI systems) but did not appear to significantly alter the provisions applying to those categories. Put another way, the European Parliament chose not to classify foundation models like ChatGPT as high-risk AI systems but still imposed roughly the same set of requirements on the operators of those models as it does on the operators of high- risk AI systems. This suggests that while regulators may be struggling to come to terms with the broad range of risks presented by AI and the variety of AI systems and applications they want to address, they do not have a very diverse set of regulatory mechanisms or proposals to use to address those risks.This undermines, to some extent, this risk-based framework of the AI Act, suggesting that rather than tailoring rules to different risks, regulators are instead merely designating which systems they believe are sufficiently impactful or important to merit regulation. At the same time that ChatGPT has highlighted the ambiguity and gaps in the draft AI Act's designation of high-risk AI systems, it has also underlined the challenges for AI posed by GDPR, particularly that regulation's emphasis on purpose limitation and data minimization. While the broad applicability and large training data sets used by generative AI models make them harder to regulate under the AI Act, those same characteristics also make it, in some ways, easier for regulators to go after the owners and operators of those models using GDPR since by their very nature, such models do not rely on minimized or purpose-limited data sets. It is possible that ultimately regulators seeking to impose penalties on AI companies will therefore come to rely more on GDPR than the AI Act, rendering the latter more of a symbolic piece of regulation than a law that is actively enforced. 29 Josephine Wolff and Nicole Atallah, "Early GDPR Penalties: Analysis of Implementation and Fines Through May 2020," Journal of Information Policy 11 (January 2021): 63-103. Page 17 of 17 Electronic copy available at: https://ssrn.com/abstract=4528698